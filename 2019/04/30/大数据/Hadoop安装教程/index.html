<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Hadoop安装教程 | MxRanger's Blog</title><meta name="keywords" content="Hadoop"><meta name="author" content="慕·歌"><meta name="copyright" content="慕·歌"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="马士兵hadoop第一课：虚拟机搭建和安装hadoop及启动(一) 需要用到的软件 virtualbox redhat64(centos7) hadoop-2.7.3.jar jdk8 xshell ftp(我用的是FlashFXP) 所需要的软件，最好到官网上去下载，也可以到百度云盘下载：http:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;1nvkDLbV">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop安装教程">
<meta property="og:url" content="http://example.com/2019/04/30/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/index.html">
<meta property="og:site_name" content="MxRanger&#39;s Blog">
<meta property="og:description" content="马士兵hadoop第一课：虚拟机搭建和安装hadoop及启动(一) 需要用到的软件 virtualbox redhat64(centos7) hadoop-2.7.3.jar jdk8 xshell ftp(我用的是FlashFXP) 所需要的软件，最好到官网上去下载，也可以到百度云盘下载：http:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;1nvkDLbV">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/hadoop.jpg">
<meta property="article:published_time" content="2019-04-30T11:50:40.000Z">
<meta property="article:modified_time" content="2021-06-19T03:00:59.786Z">
<meta property="article:author" content="慕·歌">
<meta property="article:tag" content="Hadoop">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/hadoop.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2019/04/30/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Hadoop安装教程',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-06-19 11:00:59'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><style type="text/css">#toggle-sidebar {bottom: 80px}</style><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="MxRanger's Blog" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/images/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">61</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">30</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">31</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/book"><i class="fa-fw fa fa-book"></i><span> 书籍</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/images/hadoop.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">MxRanger's Blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/book"><i class="fa-fw fa fa-book"></i><span> 书籍</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Hadoop安装教程</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-04-30T11:50:40.000Z" title="发表于 2019-04-30 19:50:40">2019-04-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-06-19T03:00:59.786Z" title="更新于 2021-06-19 11:00:59">2021-06-19</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/">Hadoop</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>18分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Hadoop安装教程"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="马士兵hadoop第一课：虚拟机搭建和安装hadoop及启动"><a href="#马士兵hadoop第一课：虚拟机搭建和安装hadoop及启动" class="headerlink" title="马士兵hadoop第一课：虚拟机搭建和安装hadoop及启动"></a><strong>马士兵hadoop第一课：虚拟机搭建和安装hadoop及启动</strong></h1><p><strong>(一) 需要用到的软件</strong></p>
<p>virtualbox redhat64(centos7) hadoop-2.7.3.jar jdk8 xshell ftp(我用的是FlashFXP)</p>
<p>所需要的软件，最好到官网上去下载，也可以到百度云盘下载：<a target="_blank" rel="noopener" href="http://pan.baidu.com/s/1nvkDLbV">http://pan.baidu.com/s/1nvkDLbV</a></p>
<span id="more"></span>

<p><strong>（二）安装配置虚拟机</strong></p>
<p>将virualbox安装好后，需要新建一个linux版redhat64的虚拟机，我取名叫master；</p>
<p>特别需要注意的地方：</p>
<p>将虚拟机的网络设置为host-only,我因为忘了设置成host-only，导致新建的虚拟机和宿主机怎么都ping不通，浪费了我一些时间。</p>
<p>选中虚拟机–&gt;设置–&gt;网络，设置如下：</p>
<p><img src="http://img.mxranger.cn/Hadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/761746-20170331135747492-317284414.png" alt="img"></p>
<p><strong>虚拟机网络设置</strong></p>
<p>a) 在设置虚拟机网络前，先设置宿主机的VirtualBox Host-Only Network，</p>
<p>打开网络共享中心–&gt;更改适配器设置，然后设置IP和子网掩码</p>
<p><img src="http://img.mxranger.cn/Hadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/761746-20170331143349633-851288459.png" alt="img"></p>
<p>b)  设置虚拟机GATEWAY为192.168.56.1</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# vi /etc/sysconfig/network</span><br><span class="line"></span><br><span class="line">#编辑内容如下</span><br><span class="line">NETWORKING=yes</span><br><span class="line">GATEWAY=192.168.56.1</span><br></pre></td></tr></table></figure>

<p>c） 设置虚拟机IP和子网掩码</p>
<p><a href="javascript:void(0);"><img src="index_files/copycode.gif" alt="复制代码"></a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# vim /etc/sysconfig/network-sripts/ifcfg-enp0s3</span><br><span class="line"></span><br><span class="line">#编辑内容如下</span><br><span class="line">TYPE=Ethernet</span><br><span class="line">IPADDR=192.168.56.100</span><br><span class="line">NETMASK=255.255.255.0</span><br></pre></td></tr></table></figure>

<p><a href="javascript:void(0);"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p>d) 修改master主机名</p>
<p><strong>主机名千万不能有下划线【马老师一再强调】</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# hostnamectl set-hostname master</span><br></pre></td></tr></table></figure>

<p>e） 重启master虚拟机网络</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# service network restart</span><br></pre></td></tr></table></figure>

<p> f) 在虚拟机上ping宿主机，在宿主机上ping虚拟机master</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# ping 192.168.56.1</span><br><span class="line">PING 192.168.56.1 (192.168.56.1) 56(84) bytes of data.</span><br><span class="line">64 bytes from 192.168.56.1: icmp_seq=1 ttl=128 time=0.191 ms</span><br><span class="line">64 bytes from 192.168.56.1: icmp_seq=2 ttl=128 time=0.203 ms</span><br></pre></td></tr></table></figure>

<p><a href="javascript:void(0);"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">C:\Users\Administrator&gt;ping 192.168.56.100</span><br><span class="line"></span><br><span class="line">正在 Ping 192.168.56.100 具有 32 字节的数据:</span><br><span class="line">来自 192.168.56.100 的回复: 字节=32 时间&lt;1ms TTL=64</span><br><span class="line">来自 192.168.56.100 的回复: 字节=32 时间&lt;1ms TTL=64</span><br><span class="line">来自 192.168.56.100 的回复: 字节=32 时间&lt;1ms TTL=64</span><br></pre></td></tr></table></figure>

<p><a href="javascript:void(0);"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p>互相ping，测试成功，若不成功，注意防火墙的影响，关闭windows或虚拟机防火墙。</p>
<p>systemctl stop firewalld.service</p>
<p>systemctl disable firewalld.service</p>
<p>更多防火墙操作：<a target="_blank" rel="noopener" href='https://www.cnblogs.com/yucongblog/p/9722414.html'>Centos7下防火墙操作</a></p>
<p><strong>（3）安装jdk</strong></p>
<p>将已下载好的jdk-8u91-linux-x64.rpm和hadoop-2.7.3.tar.gz，</p>
<p>通过FlashFXP工具（也可以是其他的ftp工具）上传上去，</p>
<p>用xshell连接master虚拟机。</p>
<p>使用rpm进行安装jdk：</p>
<p><img src="http://img.mxranger.cn/Hadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/761746-20170331144734461-1434564237.png" alt="img"></p>
<p>默认安装在 /usr/java下面，执行java看到如下输入，即表示java安装成功：</p>
<p><img src="http://img.mxranger.cn/Hadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/761746-20170331145021727-280621814.png" alt="img"></p>
<p><strong>（4）安装hadoop</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -xvf hadoop-2.7.3.tar.gz</span><br></pre></td></tr></table></figure>

<p>并将解压后的文件hadoop-2.7.3修改成hadoop，执行mv hadoop-2.7.3 hadoop</p>
<p><strong>(5) 配置hadoop的JAVA_HOME</strong></p>
<p>vim /usr/hadoop/etc/hadoop/hadoop-env.sh</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/java/default</span><br></pre></td></tr></table></figure>

<p><strong>(6) 配置hadoop的环境变量</strong></p>
<p>vim /etc/profile</p>
<p>在profile文件尾部添加内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH=$PATH:/usr/hadoop/bin:/usr/hadoop/sbin</span><br></pre></td></tr></table></figure>

<p>要想使profile文件生效，还要执行指令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# source /etc/profile</span><br></pre></td></tr></table></figure>

<p> <strong>（7）修改master的/usr/local/hadoop/etc/hadoop/core-site.xml，指明namenode的信息</strong></p>
<p><a href="javascript:void(0);"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://master:9000&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p><a href="javascript:void(0);"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p><strong>这里需要指明一下，core-site.xml里面的配置需要复制到slave虚拟机上，由于采用的是步骤（9）虚拟机复制，这个信息也已经复制过去了。</strong></p>
<p><strong>（8） 测试hadoop命令是否可以直接执行</strong></p>
<p>任意目录下敲 hadoop,打印如下，表示hadoop的环境变量配置成功</p>
<p><img src="http://img.mxranger.cn/Hadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/761746-20170331150347445-1806775875.png" alt="img"></p>
<p><strong>（9） 复制3台虚拟机</strong></p>
<p>关闭master，选中master–&gt;右键–&gt;复制，分别复制出取名为slave1，slave2，slave3的3台虚拟机。</p>
<p><img src="http://img.mxranger.cn/Hadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/761746-20170331150745383-1650173178.png" alt="img"></p>
<p>使用无界面启动方式启动4台虚拟机</p>
<p><img src="http://img.mxranger.cn/Hadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/761746-20170331142027180-735483812.png" alt="img"></p>
<p> 然后，使用以上步骤（2）中的<strong>虚拟机网络配置（b）（c）（d）（e）（f）</strong>操作slave1，slave2，slave3，</p>
<p>slave1 设置为IP：192.168.56.101，hostname：slave1</p>
<p>slave1 设置为IP：192.168.56.102，hostname：slave2</p>
<p>slave1 设置为IP：192.168.56.103，hostname：slave3</p>
<p>使用xshell依次登陆上maser，slave1，slave2，slave3四台虚拟机。</p>
<p><img src="http://img.mxranger.cn/Hadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/761746-20170331190948227-543137763.png" alt="img"></p>
<p>要想达到以上截图中的效果，操作：工具–&gt;发送键输入到所有会话；选项卡–&gt;排列–&gt;瓷砖排序。</p>
<p> <strong>（10）搭建集群</strong></p>
<p>在hadoop中，</p>
<p>跑在master机器上的组件/模块/进程有：</p>
<p>namenode，secondarynamenode,resource manager(job tracker),history sever,</p>
<p>跑在slave机器上的有：</p>
<p>datanode,node manager(task tracker)</p>
<p><img src="http://img.mxranger.cn/Hadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/761746-20170331191423492-967252413.png" alt="img"></p>
<p>a) 修改4台机器的/etc/hosts,让他们通过名字认识对方，测试一下互相用名字可以ping通。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">192.168.56.100 master</span><br><span class="line">192.168.56.101 slave1</span><br><span class="line">192.168.56.102 slave2</span><br><span class="line">192.168.56.103 slave3</span><br></pre></td></tr></table></figure>

<p>b) 修改master下的/usr/local/hadoop/etc/hadoop/slaves</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">slave1</span><br><span class="line">slave2</span><br><span class="line">slave3</span><br></pre></td></tr></table></figure>

<p>这样，master就可以知道slave1,2,3对应的IP了。</p>
<p>c） 启动namenode和datanode</p>
<p>master上需要格式化namenode，执行指令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop namenode -format</span><br></pre></td></tr></table></figure>

<p>启动master上的namenode，在master上执行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>

<p>启动slave上的datanode，在每个slave上执行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop-daemon.sh start datanode</span><br></pre></td></tr></table></figure>

<p>使用jps查看namenode和datanode的启动情况。</p>
<p><img src="http://img.mxranger.cn/Hadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/761746-20170401091831649-1205735648.png" alt="img"></p>
<p>至此，一个master，三个slave的hadoop集群搭建完成并启动成功。</p>
<p>感谢马士兵老师的无私奉献，讲解视频百度云盘地址：<a target="_blank" rel="noopener" href="http://pan.baidu.com/s/1slU6QrN">http://pan.baidu.com/s/1slU6QrN</a></p>
<h1 id="马士兵hadoop第二课：hdfs集群集中管理和hadoop文件操作"><a href="#马士兵hadoop第二课：hdfs集群集中管理和hadoop文件操作" class="headerlink" title="马士兵hadoop第二课：hdfs集群集中管理和hadoop文件操作"></a>马士兵hadoop第二课：hdfs集群集中管理和hadoop文件操作</h1><p>（1）观察集群配置情况</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# hdfs dfsadmin -report</span><br></pre></td></tr></table></figure>

<p>（2）web界面观察集群运行情况</p>
<p>使用netstat命令查看端口监听</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# netstat -ntlp</span><br></pre></td></tr></table></figure>

<p><img src="http://img.mxranger.cn/Hadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/761746-20170401094236414-976025229.png" alt="img"></p>
<p>浏览器地址栏输入：<a target="_blank" rel="noopener" href="http://192.168.56.100:50070/">http://192.168.56.100:50070</a></p>
<p><img src="http://img.mxranger.cn/Hadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/761746-20170401093928758-1861932371.png" alt="img"></p>
<p> （3）对集群进行集中管理</p>
<p>a) 修改master上的/usr/local/hadoop/etc/hadoop/slaves文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@master hadoop]# vim slaves </span><br><span class="line">#编辑内容如下</span><br><span class="line">slave1</span><br><span class="line">slave2</span><br><span class="line">slave3</span><br></pre></td></tr></table></figure>

<p>先使用hadoop-daemon.sh stop namenode（datanode）手工关闭集群。</p>
<p>b) 使用start-dfs.sh启动集群</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master hadoop]# start-dfs.sh</span><br></pre></td></tr></table></figure>

<p>发现需要输入每个节点的密码，太过于繁琐，于是需要配置免密ssh远程登陆。</p>
<p>在master上用ssh连接一台slave，需要输入密码slave的密码，</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master hadoop]# ssh slave1</span><br></pre></td></tr></table></figure>

<p>需要输入密码，输入密码登陆成功后，使用exit指令退回到master。</p>
<p>c) 免密ssh远程登陆</p>
<p>生成rsa算法的公钥和私钥</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master hadoop]# ssh-keygen -t rsa （然后四个回车）</span><br></pre></td></tr></table></figure>

<p>进入到/root/.ssh文件夹，可看到生成了id_rsa和id_rsa.pub两个文件。</p>
<p>使用以下指令完成免密ssh登陆</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master hadoop]# ssh-copy-id slaveX</span><br></pre></td></tr></table></figure>

<p>更多细节讲解，请查看马士兵hadoop第二课视频讲解：<a target="_blank" rel="noopener" href="http://pan.baidu.com/s/1qYNNrxa">http://pan.baidu.com/s/1qYNNrxa</a></p>
<p>使用stop-dfs.sh停止集群，然后使用start-dfs.sh启动集群。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# stop-dfs.sh </span><br><span class="line">[root@master ~]# stop-dfs.sh </span><br></pre></td></tr></table></figure>

<p>（3）修改windows上的hosts文件，通过名字来访问集群web界面</p>
<p>编辑C:\Windows\System32\drivers\etc\hosts</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">192.168.56.100 master</span><br></pre></td></tr></table></figure>

<p>然后就可以使用<code>http://master:50070</code>代替<code>http://192.168.56.100:50070</code></p>
<p>（4） 使用hdfs dfs 或者 hadoop fs命令对文件进行增删改查的操作</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1 hadoop fs -ls /</span><br><span class="line">2 hadoop fs -put file /</span><br><span class="line">3 hadoop fs -mkdir /dirname</span><br><span class="line">4 hadoop fs -text /filename</span><br><span class="line">5 hadoop fs -rm /filename</span><br></pre></td></tr></table></figure>

<p>将hadoop的安装文件put到了hadoop上操作如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master local]# hadoop -fs put ./hadoop-2.7.3.tar.gz /</span><br></pre></td></tr></table></figure>

<p><img src="http://img.mxranger.cn/Hadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/761746-20170401103311742-1681568432.png" alt="img"></p>
<p>通过网页观察文件情况</p>
<p><img src="http://img.mxranger.cn/Hadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/761746-20170401103518961-910282417.png" alt="img"></p>
<p>（5）将dfs-site.xml的replication值设为2</p>
<p>replication参数是分块拷贝份数，hadoop默认为3。</p>
<p>也就是说，一块数据会至少在3台slave上都存在，假如slave节点超过3台了。</p>
<p>vim hdfs-site.xml</p>
<p><a href="javascript:void(0);"><img src="index_files/copycode-1553217715661.gif" alt="复制代码"></a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"> 1 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line"> 2 &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"> 3 &lt;configuration&gt;</span><br><span class="line"> 4   &lt;property&gt;</span><br><span class="line"> 5     &lt;name&gt;dfs.replication&lt;/name&gt;  </span><br><span class="line"> 6     &lt;value&gt;2&lt;/value&gt;</span><br><span class="line"> 7   &lt;/property&gt;</span><br><span class="line"> 8   &lt;property&gt;</span><br><span class="line"> 9     &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt;</span><br><span class="line">10     &lt;value&gt;10000&lt;/value&gt;</span><br><span class="line">11   &lt;/property&gt;</span><br><span class="line">12 &lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p><a href="javascript:void(0);"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p> 为了方便测试，同时需要修改另外一个参数dfs.namenode.heartbeat.recheck-interval，这个值默认为300s，</p>
<p>将其修改成10000，单位是ms，这个参数是定期间隔时间后检查slave的运行情况并更新slave的状态。</p>
<p>可以通过 hadoop-2.7.3\share\doc\hadoop\index.html里面查找这些默认的属性</p>
<p> <img src="http://img.mxranger.cn/Hadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/761746-20170401105644680-1459493321.png" alt="img"></p>
<p><img src="http://img.mxranger.cn/Hadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/761746-20170401105613070-1143864292.png" alt="img"></p>
<p> 修改完hdf-size.xml文件后，重启hadoop集群，</p>
<p>stop-dfs.sh  #停止hadoop集群</p>
<p>start-dfs.sh #启动hadoop集权</p>
<p>hadoop -fs put ./jdk-8u91-linux-x64.rpm / #将jdk安装包上传到hadoop的根目录</p>
<p>到web页面上去观察jdk安装包文件分块在slave1，slave2，slave3的存储情况</p>
<p>hadoop-daemon.sh stop datanode #在slave3上停掉datanode</p>
<p>等一会时间后（大概10s，前面修改了扫描slave运行情况的间隔时间为10s），刷新web页面</p>
<p>观察到slave3节点挂掉</p>
<p>hadoop-daemon.sh start datanode #在slave3上启动datanode</p>
<p>然后再去观察jdk安装包文件分块在slave1，slave2，slave3的存储情况</p>
<h1 id="马士兵hadoop第三课：java开发hdfs"><a href="#马士兵hadoop第三课：java开发hdfs" class="headerlink" title="马士兵hadoop第三课：java开发hdfs"></a>马士兵hadoop第三课：java开发hdfs</h1><p><strong>(1)关于hdfs小结</strong></p>
<p>hadoop由hdfs + yarn + map/reduce组成，</p>
<p>hdfs是数据库存储模块，主要由1台namenode和n台datanode组成的一个集群系统，</p>
<p>datanode可以动态扩展，文件根据固定大小分块（默认为128M），</p>
<p>每一块数据默认存储到3台datanode，故意冗余存储，防止某一台datanode挂掉，数据不会丢失。</p>
<p>HDFS = NameNode + SecondaryNameNode + journalNode + DataNode</p>
<p>hdfs的典型应用就是：百度云盘</p>
<p><strong>（2）修改hadoop.tmp.dir默认值</strong></p>
<p>hadoop.tmp.dir默认值为/tmp/hadoop-${user.name}，由于/tmp目录是系统重启时候会被删除，所以应该修改目录位置。<br>修改core-site.xml（在所有节点上都修改）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]#  vim core-site.xml</span><br></pre></td></tr></table></figure>

<p><img src="http://img.mxranger.cn/Hadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/761746-20170403160004519-1985282122.png" alt="img"></p>
<p>修改完namenode和datanode上的hadoop.tmp.dir参数后，需要格式化namenode，在master上执行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# hdfs namenode -format</span><br></pre></td></tr></table></figure>

<p><strong>（4）测试期间关闭权限检查</strong></p>
<p>为了简单起见，需要关闭权限检查，需要在namenode的hdfs-site.xml上，添加配置：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.permissions.enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>重新启动namenode:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# hadoop-daemon.sh stop namenode</span><br><span class="line">[root@master ~]# hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>

<p><strong>(5) 使用FileSyste类来读写hdfs</strong></p>
<p><a href="javascript:void(0);"><img src="index_files/copycode-1553217773048.gif" alt="复制代码"></a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">package com.hadoop.hdfs;</span><br><span class="line"></span><br><span class="line">import java.io.FileInputStream;</span><br><span class="line">import org.apache.commons.logging.Log;</span><br><span class="line">import org.apache.commons.logging.LogFactory;</span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line">import org.apache.hadoop.fs.FileStatus;</span><br><span class="line">import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line"></span><br><span class="line">public class HelloHDFS &#123;</span><br><span class="line"></span><br><span class="line">    public static Log log =  LogFactory.getLog(HelloHDFS.class);</span><br><span class="line">    </span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">        Configuration conf = new Configuration();</span><br><span class="line">        conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://192.168.56.100:9000&quot;);</span><br><span class="line">        conf.set(&quot;dfs.replication&quot;, &quot;2&quot;);//默认为3</span><br><span class="line">        FileSystem fileSystem = FileSystem.get(conf);</span><br><span class="line">        </span><br><span class="line">        boolean success = fileSystem.mkdirs(new Path(&quot;/yucong&quot;));</span><br><span class="line">        log.info(&quot;创建文件是否成功:&quot; + success);</span><br><span class="line">        </span><br><span class="line">        success = fileSystem.exists(new Path(&quot;/yucong&quot;));</span><br><span class="line">        log.info(&quot;文件是否存在:&quot; + success);</span><br><span class="line">        </span><br><span class="line">        success = fileSystem.delete(new Path(&quot;/yucong&quot;), true);</span><br><span class="line">        log.info(&quot;删除文件是否成功：&quot; + success);</span><br><span class="line">        </span><br><span class="line">        /*FSDataOutputStream out = fileSystem.create(new Path(&quot;/test.data&quot;), true);</span><br><span class="line">        FileInputStream fis = new FileInputStream(&quot;c:/test.txt&quot;);</span><br><span class="line">        IOUtils.copyBytes(fis, out, 4096, true);*/</span><br><span class="line">        </span><br><span class="line">        FSDataOutputStream out = fileSystem.create(new Path(&quot;/test2.data&quot;));</span><br><span class="line">        FileInputStream in = new FileInputStream(&quot;c:/test.txt&quot;);</span><br><span class="line">        byte[] buf = new byte[4096];</span><br><span class="line">        int len = in.read(buf);</span><br><span class="line">        while(len != -1) &#123;</span><br><span class="line">            out.write(buf,0,len);</span><br><span class="line">            len = in.read(buf);</span><br><span class="line">        &#125;</span><br><span class="line">        in.close();</span><br><span class="line">        out.close();</span><br><span class="line">        </span><br><span class="line">        FileStatus[] statuses = fileSystem.listStatus(new Path(&quot;/&quot;));</span><br><span class="line">        log.info(statuses.length);</span><br><span class="line">        for(FileStatus status : statuses) &#123;</span><br><span class="line">            log.info(status.getPath());</span><br><span class="line">            log.info(status.getPermission());</span><br><span class="line">            log.info(status.getReplication());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><a href="javascript:void(0);"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p> 这是一个maven项目，pom.xml文件为：</p>
<p><a href="javascript:void(0);"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line"></span><br><span class="line">  &lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.7.3&lt;/version&gt;</span><br><span class="line">  &lt;/dependency&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;</span><br><span class="line">     &lt;version&gt;2.7.3&lt;/version&gt;</span><br><span class="line">  &lt;/dependency&gt;</span><br><span class="line">  </span><br><span class="line">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure>

<p><a href="javascript:void(0);"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p>马士兵视频课程百度云盘下载：<a target="_blank" rel="noopener" href="http://pan.baidu.com/s/1kVSbxS7">http://pan.baidu.com/s/1kVSbxS7</a></p>
<h1 id="马士兵hadoop第四课：Yarn和Map-Reduce配置启动和原理讲解"><a href="#马士兵hadoop第四课：Yarn和Map-Reduce配置启动和原理讲解" class="headerlink" title="马士兵hadoop第四课：Yarn和Map/Reduce配置启动和原理讲解"></a>马士兵hadoop第四课：Yarn和Map/Reduce配置启动和原理讲解</h1><p>前三节课主要讲了hdfs，hdfs就是一个分鱼展的大硬盘</p>
<p>分：分块</p>
<p>鱼：冗余</p>
<p>展：动态扩展</p>
<p>接下来讲云计算，也可以理解为分布式计算，其设计原则：</p>
<p>移动计算，而不是移动数据</p>
<p>前面说过，hadoop由hdfs，yarn，map/reduce组成，</p>
<p>而yarn（Yet Another Resource Negotiator）是资源调度系统，yarn调配的是内存和cpu，不参入计算。</p>
<p>map/reduce是计算引擎。</p>
<p><strong>（1）配置yarn</strong></p>
<p>yarn由一台resourceManager和n台dataManager组成，resourceManager管理着n台dataManager，</p>
<p>resourceManager原则上应该和namenode分开，单独在一个节点上，现在是在做实验，为了演示方便，</p>
<p>才放在一起的，而dataManager可以和datanode放在一起，这样dataManager和数据离的近一点，</p>
<p>当然也可以不放在一起。</p>
<p>要启动yarn系统，需要先配置一些参数：</p>
<p>a）配置yarn-size.xml</p>
<p>resourceManager和dataManager每一个节点都需要配置yarn-size.xml，配置如下：</p>
<p><a href="javascript:void(0);"><img src="index_files/copycode-1553217833392.gif" alt="复制代码"></a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;master&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> </span><br><span class="line"> &lt;property&gt;  </span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;  </span><br><span class="line">    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;  </span><br><span class="line"> &lt;/property&gt;  </span><br><span class="line"> </span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.auxservices.mapreduce.shuffle.class&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;  </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p><a href="javascript:void(0);"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p>b) 配置mapred-site.xml</p>
<p>只需要在master的/usr/local/hadoop/etc/hadoop目录下，</p>
<p>复制mapred-site.xml.template，即执行命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master hadoop]# cp mapred-site.xml.template mapred-site.xml</span><br></pre></td></tr></table></figure>

<p>编辑mapred-site.xml,vim mapred-site.xml:</p>
<p><a href="javascript:void(0);"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p><a href="javascript:void(0);"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p>这是配置map/reduce在哪个系统上运行，这里配置的yarn，也可以配置其他的。</p>
<p><strong>（2）启动yarn</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master hadoop]# start-yarn.sh</span><br></pre></td></tr></table></figure>

<p>使用jps查看启动情况</p>
<p><img src="http://img.mxranger.cn/Hadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/761746-20170403205351191-326558688.png" alt="img"></p>
<p>启动成功后，可在浏览器上查看web界面</p>
<p><strong><img src="http://img.mxranger.cn/Hadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/761746-20170403205124316-221519799.png" alt="img"></strong></p>
<p><strong>（3）运行一个map/reduce示例程序</strong></p>
<p>要先把hdfs也启动起来：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master hadoop]# start-dfs.sh</span><br></pre></td></tr></table></figure>

<p>上传一个文件到hdfs的/input目录上</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#在namenode的根目录上创建input目录</span><br><span class="line">[root@master hadoop]# hadoop fs -mkdir /input</span><br><span class="line">#上传一个测试文件到hadoop的/input目录上</span><br><span class="line">[root@master hadoop]# hadoop fs -put /root/input.txt  /input</span><br></pre></td></tr></table></figure>

<p>input.txt的内容如下：</p>
<p><img src="http://img.mxranger.cn/Hadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/761746-20170403203623191-669556828.png" alt="img"></p>
<p>find /usr/local/hadoop -name <em>example</em>.jar 查找示例程序文件</p>
<p>通过hadoop jar xxx.jar wordcount /input /output来运行示例程序</p>
<p> 执行结果为：</p>
<p><img src="http://img.mxranger.cn/Hadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/761746-20170403203502519-154518548.png" alt="img"></p>
<h1 id="马士兵hadoop第五课：java开发Map-Reduce"><a href="#马士兵hadoop第五课：java开发Map-Reduce" class="headerlink" title="马士兵hadoop第五课：java开发Map/Reduce"></a>马士兵hadoop第五课：java开发Map/Reduce</h1><p>配置系统环境变量HADOOP_HOME，指向hadoop安装目录（如果你不想招惹不必要的麻烦，不要在目录中包含空格或者中文字符）<br>把HADOOP_HOME/bin加到PATH环境变量（非必要，只是为了方便）<br>如果是在windows下开发，需要添加windows的库文件<br>把盘中共享的bin目录<a href="index_files/bin">bin</a>覆盖HADOOP_HOME/bin<br>如果还是不行，把其中的hadoop.dll复制到c:\windows\system32目录下，可能需要重启机器<br>建立新项目，引入hadoop需要的jar文件</p>
<p>代码WordMapper：</p>
<p><a href="javascript:void(0);"><img src="index_files/copycode-1553217912346.gif" alt="复制代码"></a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import java.io.IOException;</span><br><span class="line">import org.apache.hadoop.io.IntWritable;</span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"> </span><br><span class="line">public class WordMapper extends Mapper&lt;LongWritable,Text, Text, IntWritable&gt; &#123;</span><br><span class="line"> </span><br><span class="line">    @Override</span><br><span class="line">    protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, IntWritable&gt;.Context context)</span><br><span class="line">            throws IOException, InterruptedException &#123;</span><br><span class="line">        String line = value.toString();</span><br><span class="line">        String[] words = line.split(&quot; &quot;);</span><br><span class="line">        for(String word : words) &#123;</span><br><span class="line">            context.write(new Text(word), new IntWritable(1));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">     </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><a href="javascript:void(0);"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p>代码WordReducer:</p>
<p><a href="javascript:void(0);"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import java.io.IOException;</span><br><span class="line">import org.apache.hadoop.io.IntWritable;</span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"> </span><br><span class="line">public class WordReducer extends Reducer&lt;Text, IntWritable, Text, LongWritable&gt; &#123;</span><br><span class="line"> </span><br><span class="line">    @Override</span><br><span class="line">    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,</span><br><span class="line">            Reducer&lt;Text, IntWritable, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">        long count = 0;</span><br><span class="line">        for(IntWritable v : values) &#123;</span><br><span class="line">            count += v.get();</span><br><span class="line">        &#125;</span><br><span class="line">        context.write(key, new LongWritable(count));</span><br><span class="line">    &#125;</span><br><span class="line">     </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><a href="javascript:void(0);"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p>代码Test：</p>
<p><a href="javascript:void(0);"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.IntWritable;</span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">public class Test &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        Configuration conf = new Configuration();</span><br><span class="line">                         </span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line">         </span><br><span class="line">        job.setMapperClass(WordMapper.class);</span><br><span class="line">        job.setReducerClass(WordReducer.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(LongWritable.class);</span><br><span class="line">         </span><br><span class="line">        FileInputFormat.setInputPaths(job, &quot;c:/bigdata/hadoop/test/test.txt&quot;);</span><br><span class="line">        FileOutputFormat.setOutputPath(job, new Path(&quot;c:/bigdata/hadoop/test/out/&quot;));</span><br><span class="line">         </span><br><span class="line">        job.waitForCompletion(true);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><a href="javascript:void(0);"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p>把hdfs中的文件拉到本地来运行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FileInputFormat.setInputPaths(job, &quot;hdfs://master:9000/wcinput/&quot;);</span><br><span class="line">FileOutputFormat.setOutputPath(job, new Path(&quot;hdfs://master:9000/wcoutput2/&quot;));</span><br></pre></td></tr></table></figure>

<p>注意这里是把hdfs文件拉到本地来运行，如果观察输出的话会观察到jobID带有local字样<br>同时这样的运行方式是不需要yarn的(自己停掉yarn服务做实验)<br>在远程服务器执行</p>
<p><a href="javascript:void(0);"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://master:9000/&quot;);</span><br><span class="line"> </span><br><span class="line">conf.set(&quot;mapreduce.job.jar&quot;, &quot;target/wc.jar&quot;);</span><br><span class="line">conf.set(&quot;mapreduce.framework.name&quot;, &quot;yarn&quot;);</span><br><span class="line">conf.set(&quot;yarn.resourcemanager.hostname&quot;, &quot;master&quot;);</span><br><span class="line">conf.set(&quot;mapreduce.app-submission.cross-platform&quot;, &quot;true&quot;);</span><br><span class="line"></span><br><span class="line">FileInputFormat.setInputPaths(job, &quot;/wcinput/&quot;);</span><br><span class="line">FileOutputFormat.setOutputPath(job, new Path(&quot;/wcoutput3/&quot;));</span><br></pre></td></tr></table></figure>

<p><a href="javascript:void(0);"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p>如果遇到权限问题，配置执行时的虚拟机参数-DHADOOP_USER_NAME=root<br>也可以将hadoop的四个配置文件拿下来放到src根目录下，就不需要进行手工配置了，默认到classpath目录寻找<br>或者将配置文件放到别的地方，使用conf.addResource(.class.getClassLoader.getResourceAsStream)方式添加，不推荐使用绝对路径的方式</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">慕·歌</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2019/04/30/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/">http://example.com/2019/04/30/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">MxRanger's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Hadoop/">Hadoop</a></div><div class="post_share"><div class="social-share" data-image="/images/hadoop.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2019/04/30/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/"><img class="prev-cover" src="/images/hadoop.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">周阳七天Hadoop</div></div></a></div><div class="next-post pull-right"><a href="/2019/04/01/%E5%90%8E%E7%AB%AF/SpringBoot%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/"><img class="next-cover" src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1580736024802&amp;di=a53178dace5fdaf4f3da4da5d95fb085&amp;imgtype=0&amp;src=http%3A%2F%2Fpic.51yuansu.com%2Fpic3%2Fcover%2F02%2F43%2F06%2F59e4ba7460114_610.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">SpringBoot发送邮件</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/images/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">慕·歌</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">61</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">30</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">31</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/GavinGrayer"><i class="fas fa-bookmark"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="mailto:mxranger@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">守的云开见明月, 做时间的朋友☕^_^</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%A9%AC%E5%A3%AB%E5%85%B5hadoop%E7%AC%AC%E4%B8%80%E8%AF%BE%EF%BC%9A%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%90%AD%E5%BB%BA%E5%92%8C%E5%AE%89%E8%A3%85hadoop%E5%8F%8A%E5%90%AF%E5%8A%A8"><span class="toc-text">马士兵hadoop第一课：虚拟机搭建和安装hadoop及启动</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%A9%AC%E5%A3%AB%E5%85%B5hadoop%E7%AC%AC%E4%BA%8C%E8%AF%BE%EF%BC%9Ahdfs%E9%9B%86%E7%BE%A4%E9%9B%86%E4%B8%AD%E7%AE%A1%E7%90%86%E5%92%8Chadoop%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C"><span class="toc-text">马士兵hadoop第二课：hdfs集群集中管理和hadoop文件操作</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%A9%AC%E5%A3%AB%E5%85%B5hadoop%E7%AC%AC%E4%B8%89%E8%AF%BE%EF%BC%9Ajava%E5%BC%80%E5%8F%91hdfs"><span class="toc-text">马士兵hadoop第三课：java开发hdfs</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%A9%AC%E5%A3%AB%E5%85%B5hadoop%E7%AC%AC%E5%9B%9B%E8%AF%BE%EF%BC%9AYarn%E5%92%8CMap-Reduce%E9%85%8D%E7%BD%AE%E5%90%AF%E5%8A%A8%E5%92%8C%E5%8E%9F%E7%90%86%E8%AE%B2%E8%A7%A3"><span class="toc-text">马士兵hadoop第四课：Yarn和Map&#x2F;Reduce配置启动和原理讲解</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%A9%AC%E5%A3%AB%E5%85%B5hadoop%E7%AC%AC%E4%BA%94%E8%AF%BE%EF%BC%9Ajava%E5%BC%80%E5%8F%91Map-Reduce"><span class="toc-text">马士兵hadoop第五课：java开发Map&#x2F;Reduce</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/11/13/%E5%88%A9%E7%94%A8MINIO%E7%BB%99%E5%8D%9A%E5%AE%A2%E5%81%9A%E5%9B%BE%E5%BA%8A/" title="利用MINIO给博客做图床"><img src="http://mxranger.tpddns.cn:9000/blog/9cdce029-f42b-40f3-9064-7134b91368b1.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="利用MINIO给博客做图床"/></a><div class="content"><a class="title" href="/2021/11/13/%E5%88%A9%E7%94%A8MINIO%E7%BB%99%E5%8D%9A%E5%AE%A2%E5%81%9A%E5%9B%BE%E5%BA%8A/" title="利用MINIO给博客做图床">利用MINIO给博客做图床</a><time datetime="2021-11-13T09:17:25.000Z" title="发表于 2021-11-13 17:17:25">2021-11-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/06/26/%E5%90%8E%E7%AB%AF/SpirngBoot%E6%95%B4%E5%90%88Mybatis%20Plus/" title="SpirngBoot整合Mybatis Plus"><img src="/images/mp.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="SpirngBoot整合Mybatis Plus"/></a><div class="content"><a class="title" href="/2021/06/26/%E5%90%8E%E7%AB%AF/SpirngBoot%E6%95%B4%E5%90%88Mybatis%20Plus/" title="SpirngBoot整合Mybatis Plus">SpirngBoot整合Mybatis Plus</a><time datetime="2021-06-26T09:45:56.000Z" title="发表于 2021-06-26 17:45:56">2021-06-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/06/26/%E5%B0%8F%E6%8A%80%E5%B7%A7/%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E7%B1%BB/" title="常用工具类"><img src="/images/%E5%B7%A5%E5%85%B7%E7%B1%BB.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="常用工具类"/></a><div class="content"><a class="title" href="/2021/06/26/%E5%B0%8F%E6%8A%80%E5%B7%A7/%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E7%B1%BB/" title="常用工具类">常用工具类</a><time datetime="2021-06-26T09:27:56.000Z" title="发表于 2021-06-26 17:27:56">2021-06-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/06/19/hello-world/" title="Hello World"><img src="http://mxranger.tpddns.cn:9000/blog/fd126075-92be-4933-b213-e1a259dfa92c.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Hello World"/></a><div class="content"><a class="title" href="/2021/06/19/hello-world/" title="Hello World">Hello World</a><time datetime="2021-06-19T00:44:25.616Z" title="发表于 2021-06-19 08:44:25">2021-06-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/06/04/%E5%B0%8F%E6%8A%80%E5%B7%A7/%E5%8F%8C%E6%BB%9A%E5%8A%A8%E9%A1%B5%E9%9D%A2/" title="双滚动页面"><img src="/images/%E5%8F%8C%E6%BB%9A%E5%8A%A8%E9%A1%B5%E9%9D%A2/image-20210604111252516.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="双滚动页面"/></a><div class="content"><a class="title" href="/2021/06/04/%E5%B0%8F%E6%8A%80%E5%B7%A7/%E5%8F%8C%E6%BB%9A%E5%8A%A8%E9%A1%B5%E9%9D%A2/" title="双滚动页面">双滚动页面</a><time datetime="2021-06-04T03:12:56.000Z" title="发表于 2021-06-04 11:12:56">2021-06-04</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/images/hadoop.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2021 By 慕·歌</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a target="_blank" rel="noopener" href="http://blog.mxranger.cn/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'default',
      })
      true && mermaid.init()
    })
  }
}</script></div><div class="aplayer no-destroy" data-id="6589190051" data-server="netease" data-type="playlist" data-fixed="true" data-mini="true" data-listFolded="false" data-order="random" data-preload="none" data-autoplay="true" muted></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/gh/metowolf/MetingJS@1.2/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = [
  'title',
  '#config-diff',
  '#body-wrap',
  '#rightside-config-hide',
  '#rightside-config-show',
  '.js-pjax'
]

if (false) {
  pjaxSelectors.unshift('meta[property="og:image"]', 'meta[property="og:title"]', 'meta[property="og:url"]')
}

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener toc scroll 
  window.removeEventListener('scroll', window.tocScrollFn)

  typeof preloader === 'object' && preloader.initLoading()
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // Analytics
  if (false) {
    MtaH5.pgv()
  }

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>