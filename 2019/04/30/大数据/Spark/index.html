<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Spark使用 | MxRanger's Blog</title><meta name="keywords" content="spark"><meta name="author" content="慕·歌"><meta name="copyright" content="慕·歌"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="一、Spark1、大数据介绍123456789101112131415161718192021222324252627大数据特征    v:volumn			&#x2F;&#x2F;体量大    v:variety			&#x2F;&#x2F;样式多    v:velocity			&#x2F;&#x2F;速度快    v:valueless			&#x2F;&#x2F;价值密度低去IOE（使用下面的软件成本太高）    IBM					&#x2F;&#x2F;    Oracle    EMC">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark使用">
<meta property="og:url" content="http://example.com/2019/04/30/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/index.html">
<meta property="og:site_name" content="MxRanger&#39;s Blog">
<meta property="og:description" content="一、Spark1、大数据介绍123456789101112131415161718192021222324252627大数据特征    v:volumn			&#x2F;&#x2F;体量大    v:variety			&#x2F;&#x2F;样式多    v:velocity			&#x2F;&#x2F;速度快    v:valueless			&#x2F;&#x2F;价值密度低去IOE（使用下面的软件成本太高）    IBM					&#x2F;&#x2F;    Oracle    EMC">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://img.mxranger.cn/Spark/spark-stack.png">
<meta property="article:published_time" content="2019-04-30T11:50:40.000Z">
<meta property="article:modified_time" content="2020-03-21T09:22:08.624Z">
<meta property="article:author" content="慕·歌">
<meta property="article:tag" content="spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://img.mxranger.cn/Spark/spark-stack.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2019/04/30/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Spark使用',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2020-03-21 17:22:08'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><style type="text/css">#toggle-sidebar {bottom: 80px}</style><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="MxRanger's Blog" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/images/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">55</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">27</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">27</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/book"><i class="fa-fw fa fa-book"></i><span> 书籍</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('http://img.mxranger.cn/Spark/spark-stack.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">MxRanger's Blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/book"><i class="fa-fw fa fa-book"></i><span> 书籍</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Spark使用</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-04-30T11:50:40.000Z" title="发表于 2019-04-30 19:50:40">2019-04-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2020-03-21T09:22:08.624Z" title="更新于 2020-03-21 17:22:08">2020-03-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/spark/">spark</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">17k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>83分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Spark使用"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="一、Spark"><a href="#一、Spark" class="headerlink" title="一、Spark"></a>一、Spark</h1><h2 id="1、大数据介绍"><a href="#1、大数据介绍" class="headerlink" title="1、大数据介绍"></a>1、大数据介绍</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">大数据特征</span><br><span class="line">    v:volumn			//体量大</span><br><span class="line">    v:variety			//样式多</span><br><span class="line">    v:velocity			//速度快</span><br><span class="line">    v:valueless			//价值密度低</span><br><span class="line"></span><br><span class="line">去IOE（使用下面的软件成本太高）</span><br><span class="line">    IBM					//</span><br><span class="line">    Oracle</span><br><span class="line">    EMC</span><br><span class="line"></span><br><span class="line">事务特点</span><br><span class="line">    OLTP</span><br><span class="line">    ----------</span><br><span class="line">        online transaction proceed</span><br><span class="line">        atomic			//原子性（全成功，全失败）</span><br><span class="line">        consistent		//一致性</span><br><span class="line">        isolate			//隔离性	</span><br><span class="line">        durable			//持久性</span><br><span class="line">        </span><br><span class="line">hadoop</span><br><span class="line">-------------</span><br><span class="line">	common</span><br><span class="line">	yarn			//资源调度框架,ResourceManager , NodeManager</span><br><span class="line">	mapreduce		//高度简化的编程模型。Map + Reduce</span><br><span class="line">	hdfs			//物理切割.512</span><br><span class="line">					//namenode : 元数据</span><br></pre></td></tr></table></figure>

<h2 id="2、spark介绍"><a href="#2、spark介绍" class="headerlink" title="2、spark介绍"></a>2、spark介绍</h2><p>快如闪电的分析引擎。<br>1.速度快<br>    流计算和批处理都具有很高的性能，使用DAG（direct acycle graph,有向无环图）<br>    查询优化器、物理执行引擎。</p>
<p><img src="http://img.mxranger.cn/Spark/logistic-regression-1553396094472.png" alt="speed"></p>
<p>2.易于使用<br>    java scala R python sql<br>3.通用性<br>    sql、流计算以及复杂分析联合使用。<br>    seamlessly i无缝集成.<br>4.四大组件<br>    spark SQL        是Spark用来操作结构化数据的组件<br>    Spark Streaming  是Spark平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API<br>    MLlib            是Spark提供的一个机器学习算法库<br>    GraphX           Spark面向图计算提供的框架与算法库</p>
<p><img src="http://img.mxranger.cn/Spark/spark-stack.png" alt="组件"></p>
 <span id="more"></span> 

<h2 id="3、安装"><a href="#3、安装" class="headerlink" title="3、安装"></a>3、安装</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">安装spark</span><br><span class="line">----------------</span><br><span class="line">	1.下载spark软件包</span><br><span class="line">		http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz</span><br><span class="line">	2.tar</span><br><span class="line">		$&gt;tar -xzvf spark-2.4.0-bin-hadoop2.7.tgz -C /soft</span><br><span class="line">	3.创建软连接</span><br><span class="line">		$&gt;cd /soft</span><br><span class="line">		$&gt;ln -sfT spark-2.4.0-bin-hadoop2.7 spark</span><br><span class="line">	4.配置环境变量</span><br><span class="line">		$&gt;sudo nano /etc/profile</span><br><span class="line">		...</span><br><span class="line">		#spark</span><br><span class="line">		export SPARK_HOME=/soft/spark</span><br><span class="line">		export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin</span><br><span class="line">	5.生效环境变量</span><br><span class="line">		$&gt;source /etc/profile</span><br><span class="line"></span><br><span class="line">	6.验证spark</span><br><span class="line">		$&gt;spark-shell</span><br></pre></td></tr></table></figure>

<h2 id="4、练习"><a href="#4、练习" class="headerlink" title="4、练习"></a>4、练习</h2><p>1、Spark核心类</p>
<pre><code>1.RDD
    resilient distributed dataset , 弹性分布式数据集。 功能上类似于java中的集合。 
2.
3. 
4.
5.
</code></pre>
<p>2、wordcount计算</p>
<p><img src="http://img.mxranger.cn/Spark/1553408064970.png" alt="1553408064970"></p>
<p>输入命令</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; sc.textFile(<span class="string">&quot;/root/spark/tutorial/hello.txt&quot;</span>) <span class="comment">//读取文件</span></span><br><span class="line">	.flatMap(_.split(<span class="string">&quot; &quot;</span>))						<span class="comment">//每行“ ”拆分，一行一个单词</span></span><br><span class="line">	.map((_,<span class="number">1</span>))									<span class="comment">//每行转成元组</span></span><br><span class="line">	.reduceByKey(_ + _)							<span class="comment">//相同key的value相加</span></span><br><span class="line">	.collect()									<span class="comment">//打印信息</span></span><br></pre></td></tr></table></figure>
<p>结果如下：<br><img src="http://img.mxranger.cn/Spark/1553408104296.png" alt="1553408104296"></p>
<h2 id="5、wordcount实现"><a href="#5、wordcount实现" class="headerlink" title="5、wordcount实现"></a>5、wordcount实现</h2><p><img src="http://img.mxranger.cn/Spark/1553516979384.png" alt="1553516979384"></p>
<h3 id="1、scala版"><a href="#1、scala版" class="headerlink" title="1、scala版"></a>1、scala版</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * wordcount实现</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCountScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建配置对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">&quot;WordCountScala&quot;</span>)</span><br><span class="line">    conf.setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建sparkContext对象</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="comment">//加载文件</span></span><br><span class="line">    <span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">&quot;file:///f:/sparktest/hello.txt&quot;</span>)</span><br><span class="line">    <span class="comment">//压扁</span></span><br><span class="line">    <span class="keyword">val</span> rdd2 = rdd1.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">    <span class="comment">//标一成对</span></span><br><span class="line">    <span class="keyword">val</span> rdd3 = rdd2.map((_,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">//聚合</span></span><br><span class="line">    <span class="keyword">val</span> rdd4 = rdd3.reduceByKey(_+_)</span><br><span class="line">    <span class="keyword">val</span> arr = rdd4.collect()</span><br><span class="line">    arr.foreach(println(_))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<h3 id="2、java版"><a href="#2、java版" class="headerlink" title="2、java版"></a>2、java版</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * ClassName WordCountJava</span></span><br><span class="line"><span class="comment"> * Author    MxRanger</span></span><br><span class="line"><span class="comment"> * Date      2019/3/24</span></span><br><span class="line"><span class="comment"> * Time      20:11</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * wordcount实现  Java版   scala所有的函数式编程在java中都是匿名内部类（接口实现）</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line"></span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">        conf.setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">                .setAppName(<span class="string">&quot;WordCountScala&quot;</span>);</span><br><span class="line">        <span class="comment">//java上下文对象</span></span><br><span class="line">        JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">        <span class="comment">//加载文件</span></span><br><span class="line">        JavaRDD&lt;String&gt; rdd1 = sc.textFile(<span class="string">&quot;file:///f:/sparktest/hello.txt&quot;</span>);</span><br><span class="line">        <span class="comment">//压扁</span></span><br><span class="line">        JavaRDD&lt;String&gt; rdd2 = rdd1.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Iterator&lt;String&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Arrays.asList(s.split(<span class="string">&quot; &quot;</span>)).iterator();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//标一成对  (key , 1)</span></span><br><span class="line">        JavaPairRDD&lt;String, Integer&gt; rdd3 = rdd2.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(s, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">//化简</span></span><br><span class="line">        JavaPairRDD&lt;String, Integer&gt; rdd4 = rdd3.reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer v1, Integer v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//收集数据，打印输出</span></span><br><span class="line">        List&lt;Tuple2&lt;String, Integer&gt;&gt; list = rdd4.collect();</span><br><span class="line">        <span class="keyword">for</span> (Tuple2&lt;String,Integer&gt; t : list)&#123;</span><br><span class="line">            System.out.println(t._1 + <span class="string">&quot; : &quot;</span> + t._2);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="6、案例"><a href="#6、案例" class="headerlink" title="6、案例"></a>6、案例</h2><h3 id="1、最高气温案例"><a href="#1、最高气温案例" class="headerlink" title="1、最高气温案例"></a>1、最高气温案例</h3><p>数据如下：</p>
<p><img src="http://img.mxranger.cn/Spark/1553524435284.png" alt="1553524435284"></p>
<h4 id="Scala版"><a href="#Scala版" class="headerlink" title="Scala版"></a>Scala版</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * ClassName MaxTempScala</span></span><br><span class="line"><span class="comment">  * Author    MxRanger</span></span><br><span class="line"><span class="comment">  * Date      2019/3/25</span></span><br><span class="line"><span class="comment">  * Time      20:53</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  **/</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MaxTempScala</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建配置对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">&quot;WordCountScala&quot;</span>)</span><br><span class="line">    conf.setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建sparkContext对象</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="comment">//加载文件</span></span><br><span class="line">    <span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">&quot;file:///f:/sparktest/temp3.dat&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd2 = rdd1.map(line=&gt;&#123;</span><br><span class="line">      <span class="keyword">val</span> arr = line.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">      (arr(<span class="number">0</span>).toInt,arr(<span class="number">1</span>).toInt)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd3 = rdd2.reduceByKey((a,b)=&gt;&#123;</span><br><span class="line">      <span class="type">Math</span>.max(a,b)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> arr = rdd3.collect()</span><br><span class="line">    <span class="keyword">val</span> arr2 = arr.sortBy(t =&gt; t._1)</span><br><span class="line">    arr2.foreach(println)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h4 id="java版"><a href="#java版" class="headerlink" title="java版"></a>java版</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * ClassName WordCountJava</span></span><br><span class="line"><span class="comment"> * Author    MxRanger</span></span><br><span class="line"><span class="comment"> * Date      2019/3/24</span></span><br><span class="line"><span class="comment"> * Time      20:11</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Comparator;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * MaxTemp最高气温  Java版   scala所有的函数式编程在java中都是匿名内部类（接口实现）</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MaxTempJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line"></span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">        conf.setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">                .setAppName(<span class="string">&quot;WordCountScala&quot;</span>);</span><br><span class="line">        <span class="comment">//java上下文对象</span></span><br><span class="line">        JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">        <span class="comment">//加载文件</span></span><br><span class="line">        JavaRDD&lt;String&gt; rdd1 = sc.textFile(<span class="string">&quot;file:///f:/sparktest/temp3.dat&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//mapToPair::每行标成元组  (key , temp)</span></span><br><span class="line">        JavaPairRDD&lt;Integer, Integer&gt; rdd2 = rdd1.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, Integer, Integer&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;Integer, Integer&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] line = s.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;Integer, Integer&gt;(Integer.valueOf(line[<span class="number">0</span>]), Integer.valueOf(line[<span class="number">1</span>]));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//化简    reduceByKey::相同key的不同temp比较</span></span><br><span class="line">        JavaPairRDD&lt;Integer, Integer&gt; rdd3 = rdd2.reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer v1, Integer v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Math.max(v1,v2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// [1] 收集数据，打印输出 默认升序  false降序</span></span><br><span class="line">        <span class="comment">//List&lt;Tuple2&lt;Integer, Integer&gt;&gt; list = rdd3.sortByKey(false).collect();</span></span><br><span class="line">        List&lt;Tuple2&lt;Integer, Integer&gt;&gt; collect = rdd3.collect();</span><br><span class="line">        <span class="keyword">for</span> (Tuple2&lt;Integer, Integer&gt; cc : collect)&#123;</span><br><span class="line">            System.out.println(cc);</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(<span class="string">&quot;======================================&quot;</span>);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">            [2] 按照气温降排</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; rdd4 = rdd3.map(<span class="keyword">new</span> Function&lt;Tuple2&lt;Integer, Integer&gt;, Tuple2&lt;Integer, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;Integer, Integer&gt; <span class="title">call</span><span class="params">(Tuple2&lt;Integer, Integer&gt; v1)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> v1;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; rdd5 = rdd4.sortBy(<span class="keyword">new</span> Function&lt;Tuple2&lt;Integer, Integer&gt;, Integer&gt;() &#123;</span><br><span class="line"></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Tuple2&lt;Integer, Integer&gt; v1)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> v1._2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;, <span class="keyword">true</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        List&lt;Tuple2&lt;Integer, Integer&gt;&gt; list = rdd5.collect();</span><br><span class="line">        <span class="keyword">for</span> (Tuple2&lt;Integer, Integer&gt; t : list)&#123;</span><br><span class="line">            System.out.println(t._1 + <span class="string">&quot; : &quot;</span> + t._2);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<p><img src="http://img.mxranger.cn/Spark/1553524522092.png" alt="1553524522092"></p>
<p>聚合原理如下：</p>
<p><img src="http://img.mxranger.cn/Spark/1553564420612.png" alt="1553564420612"></p>
<h3 id="2、气温数据多指标聚合"><a href="#2、气温数据多指标聚合" class="headerlink" title="2、气温数据多指标聚合"></a>2、气温数据多指标聚合</h3><p>聚合出(年份，最高气温，最低气温，气温总和，总个数，平均气温)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">聚合是两两聚合，产生一个新的值，然后再聚合再产生新的值......</span><br><span class="line"></span><br><span class="line">需要重新对每行重造元组，</span><br><span class="line">由(年份，气温)----mapToPair---&gt;(年份，最高气温，最低气温，气温总和，总个数，平均气温)</span><br><span class="line">------&gt; reduceByKey（聚合）-----&gt;遍历</span><br></pre></td></tr></table></figure>



<h4 id="Scala版-1"><a href="#Scala版-1" class="headerlink" title="Scala版"></a>Scala版</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * ClassName MaxTempScala</span></span><br><span class="line"><span class="comment">  * Author    MxRanger</span></span><br><span class="line"><span class="comment">  * Date      2019/3/25</span></span><br><span class="line"><span class="comment">  * Time      20:53</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 气温聚合统计</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  **/</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TempAggScala</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建配置对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">&quot;WordCountScala&quot;</span>)</span><br><span class="line">    conf.setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建sparkContext对象</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="comment">//加载文件</span></span><br><span class="line">    <span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">&quot;file:///f:/sparktest/temp3.dat&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd2 = rdd1.map(line=&gt;&#123;</span><br><span class="line">      <span class="keyword">val</span> arr = line.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">      <span class="keyword">val</span> year = arr(<span class="number">0</span>).toInt</span><br><span class="line">      <span class="keyword">val</span> temp = arr(<span class="number">1</span>).toInt</span><br><span class="line">      (year,(temp,temp,temp,<span class="number">1</span>,temp.toDouble))</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    rdd2.reduceByKey((t1,t2)=&gt;&#123;</span><br><span class="line">      <span class="keyword">val</span> max = <span class="type">Math</span>.max(t1._1,t2._1)</span><br><span class="line">      <span class="keyword">val</span> min = <span class="type">Math</span>.min(t1._2,t2._2)</span><br><span class="line">      <span class="keyword">val</span> sum = t1._3 + t2._3</span><br><span class="line">      <span class="keyword">val</span> cnt = t1._4 + t2._4 <span class="comment">//个数</span></span><br><span class="line">      <span class="keyword">val</span> avg = sum.toDouble / cnt</span><br><span class="line">      (max,min,sum,cnt,avg)</span><br><span class="line">    &#125;).sortByKey().collect().foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<p><img src="http://img.mxranger.cn/Spark/1553568074968.png" alt="1553568074968"></p>
<h4 id="java版-作业"><a href="#java版-作业" class="headerlink" title="java版(作业)"></a>java版(作业)</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> scala.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.lang.Double;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * ClassName TempAggJava</span></span><br><span class="line"><span class="comment"> * Author    MxRanger</span></span><br><span class="line"><span class="comment"> * Date      2019/3/26</span></span><br><span class="line"><span class="comment"> * Time      9:50</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TempAggJava</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">        conf.setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">                .setAppName(<span class="string">&quot;WordCountScala&quot;</span>);</span><br><span class="line">        <span class="comment">//java上下文对象</span></span><br><span class="line">        JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">        <span class="comment">//加载文件</span></span><br><span class="line">        JavaRDD&lt;String&gt; rdd1 = sc.textFile(<span class="string">&quot;file:///f:/sparktest/temp3.dat&quot;</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//元组(年份，最高气温，最低气温，气温总和，总个数，平均气温)</span></span><br><span class="line">        JavaPairRDD&lt;Integer, Tuple5&lt;Integer, Integer, Integer, Integer, Double&gt;&gt; rdd2 = rdd1.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, Integer, Tuple5&lt;Integer, Integer, Integer, Integer, Double&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;Integer, Tuple5&lt;Integer, Integer, Integer, Integer, Double&gt;&gt; call(String s) <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                String[] line = s.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">                <span class="keyword">int</span> year = Integer.valueOf(line[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">int</span> max = Integer.valueOf(line[<span class="number">1</span>]);</span><br><span class="line">                <span class="keyword">int</span> min = Integer.valueOf(line[<span class="number">1</span>]);</span><br><span class="line">                <span class="keyword">int</span> sum = Integer.valueOf(line[<span class="number">1</span>]);</span><br><span class="line">                <span class="keyword">double</span> avg = Integer.valueOf(line[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;Integer, Tuple5&lt;Integer, Integer, Integer, Integer, Double&gt;&gt;(year,</span><br><span class="line">                        <span class="keyword">new</span> Tuple5&lt;&gt;(max, min, sum, <span class="number">1</span>, avg));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//化简 聚合</span></span><br><span class="line">        JavaPairRDD&lt;Integer, Tuple5&lt;Integer, Integer, Integer, Integer, Double&gt;&gt; rdd3 = rdd2.reduceByKey(<span class="keyword">new</span> Function2&lt;Tuple5&lt;Integer, Integer, Integer, Integer, Double&gt;, Tuple5&lt;Integer, Integer, Integer, Integer, Double&gt;, Tuple5&lt;Integer, Integer, Integer, Integer, Double&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple5&lt;Integer, Integer, Integer, Integer, Double&gt; <span class="title">call</span><span class="params">(Tuple5&lt;Integer, Integer, Integer, Integer, Double&gt; v1, Tuple5&lt;Integer, Integer, Integer, Integer, Double&gt; v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">int</span> max = Math.max(v1._1(), v2._1());</span><br><span class="line">                <span class="keyword">int</span> min = Math.min(v1._2(), v2._2());</span><br><span class="line">                <span class="keyword">int</span> sum = v1._3() + v2._3();</span><br><span class="line">                <span class="keyword">int</span> num = v1._4() + v2._4();</span><br><span class="line">                <span class="keyword">double</span> avg = Double.valueOf(sum) / num;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple5&lt;Integer, Integer, Integer, Integer, Double&gt;(max, min, sum, num, avg);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        List&lt;Tuple2&lt;Integer, Tuple5&lt;Integer, Integer, Integer, Integer, Double&gt;&gt;&gt; list = rdd3.sortByKey().collect();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Tuple2&lt;Integer, Tuple5&lt;Integer, Integer, Integer, Integer, Double&gt;&gt; data : list) &#123;</span><br><span class="line">            System.out.println(data._1 + <span class="string">&quot;::&quot;</span> + data._2._1() + <span class="string">&quot;:&quot;</span> + data._2._2()+ <span class="string">&quot;:&quot;</span> + data._2._3()</span><br><span class="line">                    + <span class="string">&quot;:&quot;</span> + data._2._4()+ <span class="string">&quot;:&quot;</span> + data._2._5());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结果如下：<br><img src="http://img.mxranger.cn/Spark/1553568053620.png" alt="1553568053620"></p>
<h2 id="7、搭建集群模式"><a href="#7、搭建集群模式" class="headerlink" title="7、搭建集群模式"></a>7、搭建集群模式</h2><h3 id="1、集群搭建"><a href="#1、集群搭建" class="headerlink" title="1、集群搭建"></a>1、集群搭建</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">hadoop集群模式</span><br><span class="line"></span><br><span class="line">1、local</span><br><span class="line">2、standalone</span><br><span class="line">3、full</span><br><span class="line">4、ha</span><br><span class="line">	持续提供服务的能力99.999%，解决单点故障的问题</span><br><span class="line">5、federation</span><br><span class="line"></span><br><span class="line">spark集群模式</span><br><span class="line">1、local</span><br><span class="line">	本地模式</span><br><span class="line">	不需要启动任何线程，统一jvm中使用多线程模拟</span><br><span class="line">2、standalone</span><br><span class="line">	独立模式</span><br><span class="line">	启动进程</span><br><span class="line">	master</span><br><span class="line">	worker</span><br><span class="line">3、mesos</span><br><span class="line">	</span><br><span class="line">4、yarn</span><br></pre></td></tr></table></figure>

<p>1、集群规划<br>    s200    //master<br>    s201    //worker<br>    s202    //worker<br>    s203    //worker</p>
<p>2、每台安装spark，配置环境变量 + ssh</p>
<p>3、配置spark，两处地方修改</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[master]</span><br><span class="line">	1、</span><br><span class="line">	/usr/local/spark/conf/slaves</span><br><span class="line">	slave1,slave2,slave3</span><br><span class="line">	2、</span><br><span class="line">	/usr/local/spark/conf/spark-config.sh</span><br><span class="line">	.....文件末尾</span><br><span class="line">	JAVA_HOME=/usr/local/jdk1.8.0_172</span><br></pre></td></tr></table></figure>

<p>4、分发配置文件</p>
<p>5、启动集群</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[master]</span><br><span class="line">&gt; 指定spark的bin目录下的start-all.sh，否则和hadoop冲突</span><br></pre></td></tr></table></figure>

<p>6、查看进程jps</p>
<p><img src="http://img.mxranger.cn/Spark/1553570465592.png" alt="1553570465592"></p>
<p>查看web UI  master:8080<br><img src="http://img.mxranger.cn/Spark/1553570505993.png" alt="1553570505993"></p>
<h3 id="2、spark-shell-启动集群模式"><a href="#2、spark-shell-启动集群模式" class="headerlink" title="2、spark-shell 启动集群模式"></a>2、spark-shell 启动集群模式</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell --master spark://master:7077</span><br></pre></td></tr></table></figure>

<p><img src="http://img.mxranger.cn/Spark/1553666125250.png" alt="1553666125250"></p>
<h4 id="1、使用静态数据"><a href="#1、使用静态数据" class="headerlink" title="1、使用静态数据"></a>1、使用静态数据</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = sc.makeRDD(Array(&quot;hello world1&quot;,&quot;hello world2&quot;,&quot;hello world3&quot;))</span><br></pre></td></tr></table></figure>

<p><img src="http://img.mxranger.cn/Spark/1553666635616.png" alt="1553666635616"></p>
<p>webui显示：</p>
<p><img src="http://img.mxranger.cn/Spark/1553666652696.png" alt="1553666652696"></p>
<h4 id="2、spark集群与hdfs集成"><a href="#2、spark集群与hdfs集成" class="headerlink" title="2、spark集群与hdfs集成"></a>2、spark集群与hdfs集成</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">1、停止spark集群</span><br><span class="line">	&gt; stop-all.sh</span><br><span class="line"></span><br><span class="line">2、配置</span><br><span class="line">	在spark的conf目录创建到hadoop的core-site.xml和hdfs-site.xml的软连接</span><br><span class="line">	&gt;ln -s /usr/local/hadoop/etc/hadoop/hdfs-site.xml /usr/local/spark/conf/hdfs-site.xml</span><br><span class="line">	&gt;ln -s /usr/local/hadoop/etc/hadoop/core-site.xml /usr/local/spark/conf/core-site.xml</span><br><span class="line"></span><br><span class="line">3、启动hdfs</span><br><span class="line">	3.1）启动zk</span><br><span class="line">	</span><br><span class="line">	3.2）启动hdfs</span><br><span class="line">	</span><br><span class="line">4、启动spark</span><br><span class="line">	spark-all.sh</span><br></pre></td></tr></table></figure>

<p>【测试】</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1、启动spark-shell集群</span><br><span class="line">&gt; spark-shell --master spark://master:7077</span><br><span class="line">2、默认读取hdfs中的文件</span><br><span class="line">&gt;sc.textFile(&quot;/input/count.txt&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_ + _).collect()</span><br></pre></td></tr></table></figure>

<p><img src="http://img.mxranger.cn/Spark/1553668730947.png" alt="1553668730947"></p>
<p><img src="http://img.mxranger.cn/Spark/1553668744683.png" alt="1553668744683"></p>
<h3 id="3、将程序部署到spark集群执行"><a href="#3、将程序部署到spark集群执行" class="headerlink" title="3、将程序部署到spark集群执行"></a>3、将程序部署到spark集群执行</h3><h4 id="1、代码-（没有local）"><a href="#1、代码-（没有local）" class="headerlink" title="1、代码   （没有local）"></a>1、代码   （没有local）</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cluster</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * ClassName MaxTempScala</span></span><br><span class="line"><span class="comment">  * Author    MxRanger</span></span><br><span class="line"><span class="comment">  * Date      2019/3/25</span></span><br><span class="line"><span class="comment">  * Time      20:53</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  **/</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MaxTempScala</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建配置对象 </span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">&quot;WordCountScala&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建sparkContext对象</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="comment">//加载文件</span></span><br><span class="line">    <span class="keyword">val</span> rdd1 = sc.textFile(args(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd2 = rdd1.map(line=&gt;&#123;</span><br><span class="line">      <span class="keyword">val</span> arr = line.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">      (arr(<span class="number">0</span>).toInt,arr(<span class="number">1</span>).toInt)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd3 = rdd2.reduceByKey((a,b)=&gt;&#123;</span><br><span class="line">      <span class="type">Math</span>.max(a,b)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> arr = rdd3.collect()</span><br><span class="line">    <span class="keyword">val</span> arr2 = arr.sortBy(t =&gt; t._1)</span><br><span class="line">    arr2.foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="2、模块打包"><a href="#2、模块打包" class="headerlink" title="2、模块打包"></a>2、模块打包</h4><p>点击<img src="http://img.mxranger.cn/Spark/1553670414804.png" alt="1553670414804"></p>
<p><img src="http://img.mxranger.cn/Spark/1553670432947.png" alt="1553670432947"></p>
<p><img src="http://img.mxranger.cn/Spark/1553670445482.png" alt="1553670445482"><br>选择相应的模块<br><img src="http://img.mxranger.cn/Spark/1553670458946.png" alt="1553670458946"><br>将不需要的jar包删除，只留自己<br><img src="http://img.mxranger.cn/Spark/1553670471418.png" alt="1553670471418"><br>最后一步, Build Artifacts… –&gt; XXX.jar –&gt; Build<br><img src="http://img.mxranger.cn/Spark/998529-20160929170405078-2125381157.png" alt="img"></p>
<p><img src="http://img.mxranger.cn/Spark/998529-20160929170504891-1525363776.png" alt="img"></p>
<p>将气温文件上传hdfs，执行以下命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; spark-submit --master spark://master:7077 --class cluster.MaxTempScala myspark.jar /spark/data/temp3.dat</span><br></pre></td></tr></table></figure>

<p>得出结果：</p>
<p><img src="http://img.mxranger.cn/Spark/1553670585068.png" alt="1553670585068"></p>
<p><img src="http://img.mxranger.cn/Spark/1553670571817.png" alt="1553670571817"></p>
<h1 id="二、标签生成案例"><a href="#二、标签生成案例" class="headerlink" title="二、标签生成案例"></a>二、标签生成案例</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">生成如下结果：</span><br><span class="line">    087765	分量足（29）环境好（20）</span><br><span class="line">    087766	高大上（15）味道赞（10）</span><br><span class="line">    087760	分量足（14）环境好（10）</span><br><span class="line">    087754	分量足（29）环境好（20）</span><br></pre></td></tr></table></figure>



<h2 id="1、解析json字符串"><a href="#1、解析json字符串" class="headerlink" title="1、解析json字符串"></a>1、解析json字符串</h2><p>诸如以下字符串(每行格式)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">77287793	&#123;&quot;reviewPics&quot;:null,&quot;extInfoList&quot;:[&#123;&quot;title&quot;:&quot;contentTags&quot;,&quot;values&quot;:[&quot;环境优雅&quot;,&quot;性价比高&quot;,&quot;干净卫生&quot;,&quot;停车方便&quot;,&quot;音响效果好&quot;],&quot;desc&quot;:&quot;&quot;,&quot;defineType&quot;:0&#125;,&#123;&quot;title&quot;:&quot;tagIds&quot;,&quot;values&quot;:[&quot;24&quot;,&quot;300&quot;,&quot;852&quot;,&quot;506&quot;,&quot;173&quot;],&quot;desc&quot;:&quot;&quot;,&quot;defineType&quot;:0&#125;],&quot;expenseList&quot;:null,&quot;reviewIndexes&quot;:[2],&quot;scoreList&quot;:null&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>我们要做的就是提取extInfoList中values内的的评价信息，那么就需要进行相应的提取</p>
<h2 id="2、spark-API（rdd）的sacla版"><a href="#2、spark-API（rdd）的sacla版" class="headerlink" title="2、spark API（rdd）的sacla版"></a>2、spark API（rdd）的sacla版</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> taggen</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.<span class="type">JSON</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * ClassName TaggenScala</span></span><br><span class="line"><span class="comment">  * Author    MxRanger</span></span><br><span class="line"><span class="comment">  * Date      2019/3/28</span></span><br><span class="line"><span class="comment">  * Time      9:16</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 标签生成  spark版scala</span></span><br><span class="line"><span class="comment">  **/</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TaggenScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">&quot;tagGenDemo&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="comment">//加载文件</span></span><br><span class="line">    <span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">&quot;F:\\sparktest\\temptags.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2、变换</span></span><br><span class="line">    <span class="keyword">val</span> rdd2 = rdd1.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> arr = line.split(<span class="string">&quot;\t&quot;</span>)</span><br><span class="line">      <span class="keyword">val</span> busid = arr(<span class="number">0</span>)</span><br><span class="line">      <span class="keyword">val</span> json = arr(<span class="number">1</span>)</span><br><span class="line">      <span class="keyword">val</span> tagBuff = parseJson(json)</span><br><span class="line">      (busid, tagBuff)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3、过滤空集合</span></span><br><span class="line">    <span class="keyword">val</span> rdd3 = rdd2.filter(e =&gt;&#123;e._2 != <span class="literal">null</span> &amp;&amp; e._2.length &gt; <span class="number">0</span>&#125; )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4、压扁  把list根据值压扁</span></span><br><span class="line">    <span class="keyword">val</span> rdd4 = rdd3.flatMapValues(list =&gt; list)</span><br><span class="line">    <span class="comment">//5、标一成对</span></span><br><span class="line">    <span class="keyword">val</span> rdd5 = rdd4.map(e =&gt;(e,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">//6、聚合算值</span></span><br><span class="line">    <span class="keyword">val</span> rdd6 = rdd5.reduceByKey(_+_)</span><br><span class="line">    <span class="comment">//7、再变换，重组元组</span></span><br><span class="line">    <span class="keyword">val</span> rdd7 = rdd6.map(t=&gt;( t._1._1 , (t._1._2,t._2) ) )</span><br><span class="line">    <span class="comment">//8、分组，将key相同的val放在一起</span></span><br><span class="line">    <span class="keyword">val</span> rdd8 = rdd7.groupByKey()</span><br><span class="line">    <span class="comment">//9、对商家内排序 对不同评价数进行降排</span></span><br><span class="line">    <span class="keyword">val</span> rdd9 = rdd8.mapValues(it=&gt;it.toList.sortBy(-_._2))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//10、商家间排序 根据每家第一个最多评价数进行降排</span></span><br><span class="line">    <span class="keyword">val</span> rdd10 = rdd9.sortBy(t=&gt; -t._2(<span class="number">0</span>)._2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    rdd10.collect().foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 解析json串，抽取评论，形成集合</span></span><br><span class="line"><span class="comment">    * @param str</span></span><br><span class="line"><span class="comment">    * @return</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">parseJson</span></span>(str:<span class="type">String</span>):<span class="type">ArrayBuffer</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> result : <span class="type">ArrayBuffer</span>[<span class="type">String</span>] = <span class="type">ArrayBuffer</span>()</span><br><span class="line">    <span class="keyword">val</span> jo1 = <span class="type">JSON</span>.parseObject(str)</span><br><span class="line">    <span class="comment">//得到extInfoList数组 extInfoList:[&#123;&#125;,&#123;&#125;,....]  &#123;&#125;&lt;----&gt;jarr</span></span><br><span class="line">    <span class="keyword">val</span> jarr = jo1.getJSONArray(<span class="string">&quot;extInfoList&quot;</span>)<span class="comment">//[] 数组</span></span><br><span class="line">    <span class="keyword">if</span>(jarr != <span class="literal">null</span> &amp;&amp; jarr.size() &gt; <span class="number">0</span>)&#123;</span><br><span class="line">      <span class="comment">//得到第一个对象</span></span><br><span class="line">      <span class="comment">//&#123;&quot;title&quot;:&quot;contentTags&quot;,&quot;values&quot;:[&quot;干净卫生&quot;,&quot;服务热情&quot;],&quot;desc&quot;:&quot;&quot;,&quot;defineType&quot;:0&#125;</span></span><br><span class="line">      <span class="keyword">val</span> firstobj = jarr.getJSONObject(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> tagArr = firstobj.getJSONArray(<span class="string">&quot;values&quot;</span>)<span class="comment">//数组</span></span><br><span class="line">      <span class="keyword">if</span>(tagArr != <span class="literal">null</span> &amp;&amp; tagArr.size() &gt; <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">val</span> arr = tagArr.toArray()</span><br><span class="line">        <span class="comment">//return arr.mkString(&quot;,&quot;)</span></span><br><span class="line">        <span class="keyword">for</span> (e&lt;-arr)&#123;</span><br><span class="line">          result.+=(e.asInstanceOf[<span class="type">String</span>])</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="3、Java版"><a href="#3、Java版" class="headerlink" title="3、Java版"></a>3、Java版</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> taggen;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.ArrayBuffer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Comparator;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * ClassName TaggenJava</span></span><br><span class="line"><span class="comment"> * Author    MxRanger</span></span><br><span class="line"><span class="comment"> * Date      2019/3/28</span></span><br><span class="line"><span class="comment"> * Time      9:53</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 标签生成 Java版</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TaggenJava</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">        conf.setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">                .setAppName(<span class="string">&quot;WordCountScala&quot;</span>);</span><br><span class="line">        <span class="comment">//java上下文对象</span></span><br><span class="line">        JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">        <span class="comment">//1、加载文件</span></span><br><span class="line">        JavaRDD&lt;String&gt; rdd1 = sc.textFile(<span class="string">&quot;F:\\sparktest\\temptags.txt&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2、变换,抽取出所有的标签</span></span><br><span class="line">        JavaPairRDD&lt;String, List&lt;String&gt;&gt; rdd2 = rdd1.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, List&lt;String&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;String, List&lt;String&gt;&gt; call(String s) <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">                String[] arr = s.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">                String busid = arr[<span class="number">0</span>];</span><br><span class="line">                String json = arr[<span class="number">1</span>];</span><br><span class="line">                List&lt;String&gt; tags = TagUtil.parseJson(json);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, List&lt;String&gt;&gt;(busid, tags);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3、过滤</span></span><br><span class="line">        JavaPairRDD&lt;String, List&lt;String&gt;&gt; rdd3 = rdd2.filter(<span class="keyword">new</span> Function&lt;Tuple2&lt;String, List&lt;String&gt;&gt;, Boolean&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Boolean <span class="title">call</span><span class="params">(Tuple2&lt;String, List&lt;String&gt;&gt; v1)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> (v1._2 != <span class="keyword">null</span> &amp;&amp; !v1._2.isEmpty());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4、压扁</span></span><br><span class="line">        JavaPairRDD&lt;String, String&gt; rdd4 = rdd3.flatMapValues(<span class="keyword">new</span> Function&lt;List&lt;String&gt;, Iterable&lt;String&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">call</span><span class="params">(List&lt;String&gt; v1)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> v1;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//5、标一成对</span></span><br><span class="line">        JavaPairRDD&lt;Tuple2&lt;String, String&gt;, Integer&gt; rdd5 = rdd4.mapToPair(<span class="keyword">new</span> PairFunction&lt;Tuple2&lt;String, String&gt;, Tuple2&lt;String, String&gt;, Integer&gt;() &#123;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;Tuple2&lt;String, String&gt;, Integer&gt; call(Tuple2&lt;String, String&gt; s) <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;Tuple2&lt;String, String&gt;, Integer&gt;(s, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">//6、聚合，用数字相加来聚合</span></span><br><span class="line">        JavaPairRDD&lt;Tuple2&lt;String, String&gt;, Integer&gt; rdd6 = rdd5.reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer v1, Integer v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//7、重组</span></span><br><span class="line">        JavaPairRDD&lt;String, Tuple2&lt;String, Integer&gt;&gt; rdd7 = rdd6.mapToPair(<span class="keyword">new</span> PairFunction&lt;Tuple2&lt;Tuple2&lt;String, String&gt;, Integer&gt;, String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;String, Tuple2&lt;String, Integer&gt;&gt; call(Tuple2&lt;Tuple2&lt;String, String&gt;, Integer&gt; t) <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, Tuple2&lt;String, Integer&gt;&gt;(t._1._1, <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(t._1._2, t._2));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">//8、分组</span></span><br><span class="line">        JavaPairRDD&lt;String, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt;&gt; rdd8 = rdd7.groupByKey();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//9、商家内排序</span></span><br><span class="line">        JavaPairRDD&lt;String, List&lt;Tuple2&lt;String, Integer&gt;&gt;&gt; rdd9 = rdd8.mapValues(<span class="keyword">new</span> Function&lt;Iterable&lt;Tuple2&lt;String, Integer&gt;&gt;, List&lt;Tuple2&lt;String, Integer&gt;&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> List&lt;Tuple2&lt;String, Integer&gt;&gt; call(Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; it) <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                List&lt;Tuple2&lt;String, Integer&gt;&gt; newlist = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> (Tuple2&lt;String, Integer&gt; t : it) &#123;</span><br><span class="line">                    newlist.add(t);</span><br><span class="line">                &#125;</span><br><span class="line">                newlist.sort(<span class="keyword">new</span> Comparator&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Tuple2&lt;String, Integer&gt; o1, Tuple2&lt;String, Integer&gt; o2)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> -(o1._2 - o2._2);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">                <span class="keyword">return</span> newlist;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//10、将pair变换成rdd</span></span><br><span class="line">        <span class="comment">//(77705462,[(服务热情,3), (价格实惠,2), (羊肉,2), (环境优雅,2), (羊蝎子,1), (干净卫生,1), (味道赞,1), (肉类好,1), (回头客,1)])</span></span><br><span class="line">        JavaRDD&lt;Tuple2&lt;String, List&lt;Tuple2&lt;String, Integer&gt;&gt;&gt;&gt; rdd10 = rdd9.map(<span class="keyword">new</span> Function&lt;Tuple2&lt;String, List&lt;Tuple2&lt;String, Integer&gt;&gt;&gt;, Tuple2&lt;String, List&lt;Tuple2&lt;String, Integer&gt;&gt;&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;String, List&lt;Tuple2&lt;String, Integer&gt;&gt;&gt; call(Tuple2&lt;String, List&lt;Tuple2&lt;String, Integer&gt;&gt;&gt; v1) <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> v1;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        JavaRDD&lt;Tuple2&lt;String, List&lt;Tuple2&lt;String, Integer&gt;&gt;&gt;&gt; rdd11 = rdd10.sortBy(<span class="keyword">new</span> Function&lt;Tuple2&lt;String, List&lt;Tuple2&lt;String, Integer&gt;&gt;&gt;, Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Tuple2&lt;String, List&lt;Tuple2&lt;String, Integer&gt;&gt;&gt; v1)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> v1._2.get(<span class="number">0</span>)._2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;, <span class="keyword">false</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        List list = rdd11.collect();</span><br><span class="line">        <span class="keyword">for</span> (Object o : list)&#123;</span><br><span class="line">            System.out.println(o);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<p><img src="http://img.mxranger.cn/Spark/1553909018110.png" alt="1553909018110"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">并行</span><br><span class="line">-------------</span><br><span class="line">	分布式，集群上多个节点同时执行计算任务</span><br><span class="line">并发</span><br><span class="line">-------------</span><br><span class="line">	通常指单个节点处理同时发起的请求的能力</span><br><span class="line">三高</span><br><span class="line">------------</span><br><span class="line">	高并发、高负载、高吞吐量</span><br></pre></td></tr></table></figure>

<h1 id="三、RDD常见操作"><a href="#三、RDD常见操作" class="headerlink" title="三、RDD常见操作"></a>三、RDD常见操作</h1><p><strong>rdd都是延迟计算的，只有调用action方法时，才会触发job的提交。</strong><br><img src="http://img.mxranger.cn/Spark/1553930073520.png" alt="1553930073520"><br>1.【transform】变换<br>    只要返回新的RDD就是transform。<br>    map、filter、flatMap<br>    mapPartitons                //对每个分区进行变换处理<br>    sample、union、distinct、intersection<br>    groupByKey                    //没有combine过程，可以改变v类型<br>    reduceByKey                    //有combine过程，不能改变v类型  PairRDDFunction<br>    join<br>2.【action】<br>    只有调用action方法，rdd才会进行运算，调用作业执行<br>    2.1)collect<br>    2.2)foreachPartition        //迭代每个分区<br>    2.3)reduce<br>    2.4)count<br>    2.5)take<br>    2.6)first</p>
<h2 id="1、transform方法"><a href="#1、transform方法" class="headerlink" title="1、transform方法"></a>1、transform方法</h2><h3 id="1、mapPartitions和mapPartitionsWithIndex"><a href="#1、mapPartitions和mapPartitionsWithIndex" class="headerlink" title="1、mapPartitions和mapPartitionsWithIndex"></a>1、<strong>mapPartitions</strong>和<strong>mapPartitionsWithIndex</strong></h3><h3 id="mapPartitions查看所有分区里的信息"><a href="#mapPartitions查看所有分区里的信息" class="headerlink" title="mapPartitions查看所有分区里的信息"></a>mapPartitions查看所有分区里的信息</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建配置对象</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">conf.setAppName(<span class="string">&quot;WordCountScala&quot;</span>)</span><br><span class="line">conf.setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"><span class="comment">//创建sparkContext对象</span></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="comment">//加载文件</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">&quot;file:///f:/sparktest/hello.txt&quot;</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment">//压扁</span></span><br><span class="line"><span class="keyword">val</span> rdd2 = rdd1.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd2.map((_,<span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> rdd4 = rdd3.reduceByKey(_+_)</span><br><span class="line">rdd4.mapPartitions(it =&gt;&#123;</span><br><span class="line">    <span class="keyword">for</span> (e &lt;- it)&#123;</span><br><span class="line">        <span class="type">System</span>.out.println(it + <span class="string">&quot; : &quot;</span> + e)</span><br><span class="line">    &#125;</span><br><span class="line">    it</span><br><span class="line">&#125;).collect()</span><br></pre></td></tr></table></figure>

<p><img src="http://img.mxranger.cn/Spark/1554085595717.png" alt="1554085595717"></p>
<h3 id="mapPartitionsWithIndex可以查看某个分区里的信息"><a href="#mapPartitionsWithIndex可以查看某个分区里的信息" class="headerlink" title="mapPartitionsWithIndex可以查看某个分区里的信息"></a>mapPartitionsWithIndex可以查看某个分区里的信息</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">rdd4.mapPartitionsWithIndex((idx,it) =&gt; &#123;</span><br><span class="line">    <span class="keyword">for</span> (e &lt;- it)&#123;</span><br><span class="line">        <span class="type">System</span>.out.println(idx + <span class="string">&quot; : &quot;</span> + e)</span><br><span class="line">    &#125;</span><br><span class="line">    it</span><br><span class="line">&#125;).collect()</span><br></pre></td></tr></table></figure>



<p><img src="http://img.mxranger.cn/Spark/1554085672391.png" alt="1554085672391"></p>
<h3 id="2、sample采样"><a href="#2、sample采样" class="headerlink" title="2、sample采样"></a>2、sample采样</h3><p><img src="http://img.mxranger.cn/Spark/1554085801327.png" alt="1554085801327"></p>
<p>Sample是对rdd中的数据集进行采样,并生成一个新的RDD,这个新的RDD只有原来RDD的部分数据,这个保留的数据集大小由fraction来进行控制</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">参数说明:</span><br><span class="line"></span><br><span class="line">withReplacement=&gt;,这个值如果是true时,采用PoissonSampler取样器(Poisson分布),</span><br><span class="line">                  否则使用BernoulliSampler的取样器.</span><br><span class="line"></span><br><span class="line">Fraction=&gt;,一个大于0,小于或等于1的小数值,用于控制要读取的数据所占整个数据集的概率.</span><br><span class="line"></span><br><span class="line">Seed=&gt;,这个值如果没有传入,默认值是一个0~Long.maxvalue之间的整数.</span><br></pre></td></tr></table></figure>

<h3 id="3、union"><a href="#3、union" class="headerlink" title="3、union"></a>3、union</h3><p>等价于sql中的union操作，不去重</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RDDUnionScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建配置对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">&quot;WordCountScala&quot;</span>)</span><br><span class="line">    conf.setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建sparkContext对象</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line">    <span class="keyword">val</span> rdd2 = sc.makeRDD(<span class="number">3</span> to <span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd3 = rdd1.union(rdd2)</span><br><span class="line">    rdd3.collect().foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="http://img.mxranger.cn/Spark/1554086043708.png" alt="1554086043708"></p>
<h3 id="4、intersection"><a href="#4、intersection" class="headerlink" title="4、intersection"></a>4、intersection</h3><p>取交集</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RDDIntersectionScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建配置对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">&quot;WordCountScala&quot;</span>)</span><br><span class="line">    conf.setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建sparkContext对象</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line">    <span class="keyword">val</span> rdd2 = sc.makeRDD(<span class="number">3</span> to <span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd3 = rdd1.intersection(rdd2)</span><br><span class="line">    rdd3.collect().foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="http://img.mxranger.cn/Spark/1554086179887.png" alt="1554086179887"></p>
<h3 id="5、distinct"><a href="#5、distinct" class="headerlink" title="5、distinct"></a>5、distinct</h3><p>去重</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RDDistinctScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建配置对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">&quot;WordCountScala&quot;</span>)</span><br><span class="line">    conf.setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建sparkContext对象</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line">    <span class="keyword">val</span> rdd2 = sc.makeRDD(<span class="number">3</span> to <span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd3 = rdd1.union(rdd2)</span><br><span class="line">    rdd3.distinct().foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="http://img.mxranger.cn/Spark/1554086505184.png" alt="1554086505184"></p>
<h3 id="6、groupByKey和reduceByKey"><a href="#6、groupByKey和reduceByKey" class="headerlink" title="6、groupByKey和reduceByKey"></a>6、groupByKey和reduceByKey</h3><p>groupByKey：没有map聚合 可以改变value的类型<br>reduceByKey：有map聚合 不能改变value的类型</p>
<h3 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RDDGroupByKeyScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建配置对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">&quot;WordCountScala&quot;</span>)</span><br><span class="line">    conf.setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建sparkContext对象</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd2 = rdd1.map(e =&gt;&#123;</span><br><span class="line">      (<span class="keyword">if</span>(e%<span class="number">2</span> == <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="string">&quot;even&quot;</span></span><br><span class="line">      &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="string">&quot;odd&quot;</span></span><br><span class="line">      &#125;,e)</span><br><span class="line">    &#125;)</span><br><span class="line">    rdd2.groupByKey().foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="http://img.mxranger.cn/Spark/1554087213647.png" alt="1554087213647"></p>
<h3 id="7、join"><a href="#7、join" class="headerlink" title="7、join"></a>7、join</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RDDGroupByKeyScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建配置对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">&quot;WordCountScala&quot;</span>)</span><br><span class="line">    conf.setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建sparkContext对象</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">Seq</span>((<span class="number">1</span>,<span class="string">&quot;tom1&quot;</span>),(<span class="number">2</span>,<span class="string">&quot;tom2&quot;</span>),(<span class="number">3</span>,<span class="string">&quot;tom3&quot;</span>)))</span><br><span class="line">    <span class="keyword">val</span> rdd2 = sc.makeRDD(<span class="type">Seq</span>((<span class="number">1</span>,<span class="number">600</span>),(<span class="number">2</span>,<span class="number">550</span>),(<span class="number">3</span>,<span class="number">450</span>)))</span><br><span class="line">    <span class="comment">//spark中必须要(k,v)对（pairrdd）才能join相连</span></span><br><span class="line">    rdd1.join(rdd2).foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="http://img.mxranger.cn/Spark/1554087994248.png" alt="1554087994248"></p>
<h3 id="8、fullOuterJoin全连接和leftOuterJoin左外连接"><a href="#8、fullOuterJoin全连接和leftOuterJoin左外连接" class="headerlink" title="8、fullOuterJoin全连接和leftOuterJoin左外连接"></a>8、fullOuterJoin全连接和leftOuterJoin左外连接</h3><p>和sql语句中类似</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * RDD join</span></span><br><span class="line"><span class="comment">  * select a.* , b.* from a left outer join tableb b on a.id = b.id</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RDDGroupByKeyScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建配置对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">&quot;WordCountScala&quot;</span>)</span><br><span class="line">    conf.setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建sparkContext对象</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">Seq</span>((<span class="number">1</span>,<span class="string">&quot;tom1&quot;</span>),(<span class="number">2</span>,<span class="string">&quot;tom2&quot;</span>),(<span class="number">3</span>,<span class="string">&quot;tom3&quot;</span>)))</span><br><span class="line">    <span class="keyword">val</span> rdd2 = sc.makeRDD(<span class="type">Seq</span>((<span class="number">4</span>,<span class="number">600</span>),(<span class="number">2</span>,<span class="number">550</span>),(<span class="number">3</span>,<span class="number">450</span>)))</span><br><span class="line">    <span class="comment">//spark中必须要(k,v)对（PairRDD）才能相连,相同key将value放在一起</span></span><br><span class="line">    <span class="comment">//rdd1.join(rdd2).foreach(println)</span></span><br><span class="line">    rdd1.fullOuterJoin(rdd2).foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>fullOuterJoin结果：</p>
<p><img src="http://img.mxranger.cn/Spark/1554105936807.png" alt="1554105936807"></p>
<p>leftOuterJoin结果：</p>
<p><img src="http://img.mxranger.cn/Spark/1554106084623.png" alt="1554106084623"></p>
<h3 id="9、cogroup"><a href="#9、cogroup" class="headerlink" title="9、cogroup"></a>9、<strong>cogroup</strong></h3><p>多个value都按key分组，编程多个对</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> rdd</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.<span class="type">DriverManager</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * RDD cogroup 协分组</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RDDCoGroupScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建配置对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">&quot;rdddemo&quot;</span>)</span><br><span class="line">    conf.setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">Seq</span>((<span class="number">1</span>,<span class="string">&quot;1_1&quot;</span>),(<span class="number">1</span>,<span class="string">&quot;1_2&quot;</span>),(<span class="number">2</span>,<span class="string">&quot;2_1&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;2_3&quot;</span>)))</span><br><span class="line">    <span class="keyword">val</span> rdd2 = sc.makeRDD(<span class="type">Seq</span>((<span class="number">1</span>,<span class="string">&quot;beijing&quot;</span>),(<span class="number">1</span>,<span class="string">&quot;上海&quot;</span>),(<span class="number">2</span>,<span class="string">&quot;广州&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;深圳&quot;</span>)))</span><br><span class="line">    rdd1.cogroup(rdd2).foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="http://img.mxranger.cn/Spark/1554107020642.png" alt="1554107020642"></p>
<h3 id="10、coalesce和repartition"><a href="#10、coalesce和repartition" class="headerlink" title="10、coalesce和repartition"></a>10、<strong>coalesce</strong>和<strong>repartition</strong></h3><p>repartition(numPartitions)    //再分区，底层调用的coalesce(,shuffle=true)<br>                                //始终执行shuffle，存在性能问题<br>                                //改变并行度.</p>
<p>coalesce(numPartitions)        //可以控制是否shuffle，默认false<br>                                //减少分区时推荐该方法，不进行shuffle<br>                                //增加分区，如果不进行shuffle，分区数不变。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * RDD cogroup 协分组</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RDDRepartitionScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建配置对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">&quot;rdddemo&quot;</span>)</span><br><span class="line">    conf.setMaster(<span class="string">&quot;local[3]&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> rdd1 = sc.makeRDD((<span class="number">1</span> to <span class="number">10</span>) , <span class="number">5</span>)</span><br><span class="line">    println(<span class="string">&quot;rdd1&#x27;pars : &quot;</span> + rdd1.partitions.length)</span><br><span class="line">    <span class="keyword">val</span> rdd2 = rdd1.map(e=&gt;&#123;</span><br><span class="line">        <span class="keyword">val</span> tid = <span class="type">Thread</span>.currentThread().getId</span><br><span class="line">        <span class="keyword">val</span> tname = <span class="type">Thread</span>.currentThread().getName</span><br><span class="line">        printf(<span class="string">&quot;分区前%d/%s : %d\r\n&quot;</span> , tid,tname , e)</span><br><span class="line">        e</span><br><span class="line">    &#125;)</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">val</span> rdd3 = rdd2.repartition(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">val</span> rdd4 = rdd3.map(e=&gt;&#123;</span><br><span class="line">      <span class="keyword">val</span> tid = <span class="type">Thread</span>.currentThread().getId</span><br><span class="line">      <span class="keyword">val</span> tname = <span class="type">Thread</span>.currentThread().getName</span><br><span class="line">      printf(<span class="string">&quot;分区后%d/%s : %d\r\n&quot;</span> , tid,tname , e)</span><br><span class="line">      e</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    println(<span class="string">&quot;rdd2&#x27;pars : &quot;</span> + rdd2.partitions.length)</span><br><span class="line">    rdd2.collect()</span><br><span class="line">    <span class="keyword">while</span>(<span class="literal">true</span>)&#123;</span><br><span class="line">      <span class="type">Thread</span>.sleep(<span class="number">1000</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>执行结果：</p>
<p>分区前46/Executor task launch worker for task 0 : 1<br>分区前48/Executor task launch worker for task 2 : 5<br>分区前47/Executor task launch worker for task 1 : 3<br>分区前46/Executor task launch worker for task 0 : 2<br>分区前48/Executor task launch worker for task 2 : 6<br>分区前47/Executor task launch worker for task 1 : 4<br>分区前48/Executor task launch worker for task 3 : 7<br>分区前48/Executor task launch worker for task 3 : 8<br>分区前47/Executor task launch worker for task 4 : 9<br>分区前47/Executor task launch worker for task 4 : 10</p>
<p>分区后48/Executor task launch worker for task 5 : 1<br>分区后46/Executor task launch worker for task 6 : 2<br>分区后48/Executor task launch worker for task 5 : 4<br>分区后46/Executor task launch worker for task 6 : 3<br>分区后48/Executor task launch worker for task 5 : 5<br>分区后46/Executor task launch worker for task 6 : 6<br>分区后46/Executor task launch worker for task 6 : 8<br>分区后48/Executor task launch worker for task 5 : 7<br>分区后48/Executor task launch worker for task 5 : 10<br>分区后46/Executor task launch worker for task 6 : 9</p>
<h3 id="11、aggregateByKey"><a href="#11、aggregateByKey" class="headerlink" title="11、aggregateByKey"></a>11、aggregateByKey</h3><p><code>aggregateByKey[U: ClassTag](zeroValue: U)(seqOp: (U, V) =&gt; U,combOp: (U, U) =&gt; U): RDD[(K, U)]</code></p>
<p>按照key分区内聚合(seqOp) + 分区间聚合(combOp) + 可以改变值的类型。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * RDD cogroup 协分组</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">object AggByKeyDemoScala &#123;</span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    <span class="comment">//创建配置对象</span></span><br><span class="line">    val conf = <span class="keyword">new</span> SparkConf()</span><br><span class="line">    conf.setAppName(<span class="string">&quot;rdddemo&quot;</span>)</span><br><span class="line">    conf.setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建sparkContext对象</span></span><br><span class="line">    val sc = <span class="keyword">new</span> SparkContext(conf)</span><br><span class="line">    <span class="comment">//加载文件</span></span><br><span class="line">    val rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>,<span class="number">3</span>)</span><br><span class="line">    <span class="comment">//重组</span></span><br><span class="line">    val rdd2 = rdd1.map((<span class="string">&quot;hello&quot;</span>,_))</span><br><span class="line">    <span class="comment">//zeroU</span></span><br><span class="line">    val u = <span class="string">&quot;S&quot;</span></span><br><span class="line">    <span class="comment">//seqOp</span></span><br><span class="line">    <span class="function">def <span class="title">seqOp</span><span class="params">(u:String,v:Int)</span>:String </span>= &#123;</span><br><span class="line">      u + <span class="string">&quot;,&quot;</span> + v</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//compOp</span></span><br><span class="line">    <span class="function">def <span class="title">combOp</span><span class="params">(u1:String,u2:String)</span>:String </span>= &#123;</span><br><span class="line">      u1 + <span class="string">&quot;||&quot;</span> + u2</span><br><span class="line">    &#125;</span><br><span class="line">    val rdd3 = rdd2.aggregateByKey(u)(seqOp,combOp)</span><br><span class="line">  rdd3.collect().foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>执行结果：</p>
<p>48/Executor task launch worker for task 2 : 5<br>46/Executor task launch worker for task 0 : 1<br>47/Executor task launch worker for task 1 : 3<br>48/Executor task launch worker for task 2 : 6<br>46/Executor task launch worker for task 0 : 2<br>47/Executor task launch worker for task 1 : 4<br>48/Executor task launch worker for task 3 : 7<br>48/Executor task launch worker for task 3 : 8<br>47/Executor task launch worker for task 4 : 9<br>47/Executor task launch worker for task 4 : 10</p>
<h3 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h3><p>1.使用aggregteByKey计算wordcount例子中分区内的value的总和，<br>      再将每个分区的总和用-连接起来输出<br>      例如: hello : 3-5-8</p>
<h2 id="2、action方法"><a href="#2、action方法" class="headerlink" title="2、action方法"></a>2、action方法</h2><p>rdd.count<br>rdd.take(n)                 //取前n<br>rdd.first()                //take1() 取前一<br>rdd.count<br>rdd.foreachPartition()</p>
<h3 id="saveAsTextFile"><a href="#saveAsTextFile" class="headerlink" title="saveAsTextFile"></a>saveAsTextFile</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * wordcount实现</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RDDActionScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建配置对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">&quot;rdddemo&quot;</span>)</span><br><span class="line">    conf.setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建sparkContext对象</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="comment">//加载文件</span></span><br><span class="line">    <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">5</span>).map((<span class="string">&quot;hello&quot;</span>,_))</span><br><span class="line"></span><br><span class="line">    rdd1.saveAsTextFile(<span class="string">&quot;file:///f:/sparktest/action&quot;</span>)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="http://img.mxranger.cn/Spark/1554296653208.png" alt="1554296653208"></p>
<p><img src="http://img.mxranger.cn/Spark/1554296662728.png" alt="1554296662728"></p>
<h1 id="四、Spark核心API"><a href="#四、Spark核心API" class="headerlink" title="四、Spark核心API"></a>四、Spark核心API</h1><h2 id="1-SparkConf"><a href="#1-SparkConf" class="headerlink" title="1.SparkConf"></a>1.SparkConf</h2><p>Spark配置对象，设置各种参数，使用kv类型。</p>
<h2 id="2-SparkContext"><a href="#2-SparkContext" class="headerlink" title="2.SparkContext"></a>2.SparkContext</h2><p>​    spark主要入口点，代表到spark集群的连接，可以创建rdd、累加器和广播变量。<br>​    每个JVM中只能有一个SparkContext，启动新的SparkContext必须stop的原来的。<br>​    val rdd1 = sc.textFile()<br>​    <strong>textFile方法会创建HadoopRDD和MapPartitionsRDD</strong></p>
<h2 id="3-RDD"><a href="#3-RDD" class="headerlink" title="3.RDD"></a>3.RDD</h2><p>轻量级的，没有数据。内置5个属性<br>    分区列表            //<br>    计算函数            //func<br>    依赖列表            //deps<br>    首选位置<br>    分区类</p>
<p>​    HadoopRDD<br>​    MapPartitionRDD<br>​    ShuffleRDD</p>
<h2 id="4-Dependency"><a href="#4-Dependency" class="headerlink" title="4.Dependency"></a>4.Dependency</h2><p>​    依赖，<br>​    指的是子RDD的每个分区和父RDD的分区之间数量的对应关系。<br>​    Dependency<br>​        |<br>​        |—NarrowDependency(窄依赖)子RDD的每个分区依赖于父RDD的少量分区.<br>​            |—-OneToOne依赖(一对一依赖)<br>​            |—-Range依赖(范围依赖)<br>​            |—-Prune依赖(修剪依赖)<br>​        |—ShuffleDependency(宽依赖)</p>
<h2 id="5-Stage"><a href="#5-Stage" class="headerlink" title="5.Stage"></a>5.Stage</h2><p>​    阶段是并行任务的集合，由调度器运行DAG图根据shuffle进行划分成若干stage。<br>​    阶段分两种类型：ShuffleMapStage和ResultStage<br>​    1.ShuffleMapStage<br>​        该阶段的输出是下一个阶段的输入，<strong>跟踪每个节点的输出情况</strong>。<br>​        一个阶段会重试执行多次处于容错考虑。<br>​        由多个ShuffleMapTask构成。</p>
<p>​        中间环节，产生输出，作为下一个stage的输入。</p>
<p>​    2.ResultStage<br>​        在某些分区上应用计算函数，有些操作例如take(n)/first()没必要在所有分区上执行的。<br>​        结果阶段的输出结果回传给driver.<br>​        由多个ResultTask构成。</p>
<p>​        计算函数，有可能在部分分区上执行，比如first，take等，回传结果给driver。</p>
<p><img src="http://img.mxranger.cn/Spark/1554194199377.png" alt="1554194199377"></p>
<h2 id="6-Task"><a href="#6-Task" class="headerlink" title="6.Task"></a>6.Task</h2><p>​    Spark执行的最小单位，有两种类型，和Stage相对。</p>
<p>任务运行在standalone模式下，运行在worker上，worker启动Executor上。</p>
<p>​    1.ShuffleMapTask</p>
<p>​    2.ResultTask<br>​        执行任务，并将结果回传给driver。</p>
<h2 id="7-job"><a href="#7-job" class="headerlink" title="7.job"></a>7.job</h2><p>​    每个action是一个job。</p>
<h2 id="8-Application"><a href="#8-Application" class="headerlink" title="8.Application"></a>8.Application</h2><p>​    一个应用有多个job，对应一个SparkContext。<br>​    </p>
<h2 id="9-DAGScheduler"><a href="#9-DAGScheduler" class="headerlink" title="9.DAGScheduler"></a>9.DAGScheduler</h2><p>DAGScheduler提交job，也可以提交Stage，<strong>面向stage的一个高级调度层面</strong>，负责为每个job计算Stage的DAG图。跟踪RDD和输出，以Tasket方式提交给下层的task</p>
<p><img src="http://img.mxranger.cn/Spark/1554194166537.png" alt="1554194166537"></p>
<p>Spark以shuffle为边界将RDD划分为多个Stage，Stage存储前后关系</p>
<p>DAG调度器会因为输出文件</p>
<h2 id="10-内核"><a href="#10-内核" class="headerlink" title="10. 内核"></a>10. 内核</h2><p>待续 TODO</p>
<p>stage   task</p>
<h2 id="11-资源分配——启动执行器"><a href="#11-资源分配——启动执行器" class="headerlink" title="11.资源分配——启动执行器"></a>11.资源分配——启动执行器</h2><p><img src="http://img.mxranger.cn/Spark/1554272478760.png" alt="1554272478760"></p>
<p>spark job 运行机制</p>
<p>job运行分为两步走</p>
<ul>
<li>1）注册应用，分配资源</li>
</ul>
<p>new SparkContext()发生如下的行为:<br>        client发送消息给master注册应用，master收到消息进行应用的注册，然后master向worker发送LaunchExecutor消息，<br>        worker收到消息开始启动executor，executor启动时再向driver发送registerExecutor消息，driver收到消息，维护<br>        executor地址信息。</p>
<ul>
<li>2）提交job</li>
</ul>
<p>提交是三级调度框架，<br>        2.1)DagScheduler<br>            计算Stage的Dag图 (通过ShuffleDep),提交阶段，最终提交的任务集合给Task调度器,任务调度器转交给底层后台调度器。<br>        2.2)TaskScheduler<br>        2.3)SchedulerBackend<br>            Local<br>            Coarsegrain…<br>            Standalone</p>
<h2 id="作业："><a href="#作业：" class="headerlink" title="作业："></a>作业：</h2><p>1、画出下列代码stage图并说明理由</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>,<span class="number">5</span>)</span><br><span class="line">.map(e=&gt;(e%<span class="number">2</span>,e))</span><br><span class="line">.repartition(<span class="number">3</span>)</span><br><span class="line">.coalesce(<span class="number">2</span>,shuffle = <span class="literal">true</span>)</span><br><span class="line">.repartition(<span class="number">6</span>)</span><br><span class="line">.reduceByKey(_+_)</span><br><span class="line">.map(t=&gt;(t._1,t._2 + <span class="number">1</span>))</span><br><span class="line">.collect()</span><br></pre></td></tr></table></figure>

<p><img src="http://img.mxranger.cn/Spark/1554277451384.png" alt="1554277451384"></p>
<p>2、使用aggregteByKey计算wordcount例子中分区内的value的总和，<br>      再将每个分区的总和用-连接起来输出[跳转](# 练习)<br>      例如: hello : 1:3-3:5-2:8<br>    (0,1)<br>    (0,1)<br>    (0,1)<br>    (0,1)</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> wordcount</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * wordcount实现</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCountWithIndexScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建配置对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">&quot;WordCountScala&quot;</span>)</span><br><span class="line">    conf.setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建sparkContext对象</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="comment">//加载文件</span></span><br><span class="line">    <span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">&quot;file:///f:/sparktest/hello.txt&quot;</span>,<span class="number">4</span>)</span><br><span class="line">    <span class="comment">//压扁</span></span><br><span class="line">    <span class="keyword">val</span> rdd2 = rdd1.flatMap(line =&gt;&#123;</span><br><span class="line">      <span class="comment">//println(&quot;x&quot;)</span></span><br><span class="line">      line.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//标一成对</span></span><br><span class="line">    <span class="keyword">val</span> rdd3 = rdd2.map(e=&gt;&#123;</span><br><span class="line">      <span class="comment">//println(&quot;y&quot;)</span></span><br><span class="line">      (e,<span class="number">1</span>)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd4 = rdd3.mapPartitionsWithIndex((index,it)=&gt;&#123;</span><br><span class="line">      <span class="keyword">val</span> buf = <span class="type">ArrayBuffer</span>[(<span class="type">String</span>,(<span class="type">Int</span>,<span class="type">Int</span>))]()</span><br><span class="line">      <span class="keyword">for</span> (x &lt;- it)&#123;</span><br><span class="line">        buf.append((x._1,(index,x._2)))</span><br><span class="line">      &#125;</span><br><span class="line">      buf.iterator</span><br><span class="line">    &#125;)</span><br><span class="line">    rdd4.collect().foreach(println)</span><br><span class="line">    <span class="comment">//零值</span></span><br><span class="line">    <span class="keyword">val</span> zeroU = <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="comment">//分区内聚合</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">seqOp</span></span>(a:<span class="type">String</span>,b:(<span class="type">Int</span>,<span class="type">Int</span>)):<span class="type">String</span> = &#123;</span><br><span class="line">      <span class="type">System</span>.out.println(<span class="string">&quot;a::&quot;</span>+a)</span><br><span class="line">      <span class="keyword">if</span>(a.equals(<span class="string">&quot;&quot;</span>))&#123;</span><br><span class="line">        b._1 + <span class="string">&quot;:&quot;</span> + b._2</span><br><span class="line">      &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="keyword">val</span> arr = a.split(<span class="string">&quot;:&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> cnt = arr(<span class="number">1</span>).toInt</span><br><span class="line">        <span class="type">System</span>.out.println(<span class="string">&quot;arr::&quot;</span>+arr(<span class="number">0</span>)+<span class="string">&quot; cnt::&quot;</span>+cnt)</span><br><span class="line">        arr(<span class="number">0</span>) + <span class="string">&quot;:&quot;</span> + (cnt + b._2)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">combOp</span></span>(a:<span class="type">String</span>,b:<span class="type">String</span>) :<span class="type">String</span> = &#123;</span><br><span class="line">      a + <span class="string">&quot;-&quot;</span> + b</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd5 = rdd4.aggregateByKey(zeroU)(seqOp,combOp)</span><br><span class="line"></span><br><span class="line">    rdd5.collect().foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="http://img.mxranger.cn/Spark/1554284948217.png" alt="1554284948217"></p>
<h1 id="五、数据倾斜"><a href="#五、数据倾斜" class="headerlink" title="五、数据倾斜"></a>五、数据倾斜</h1><p>重新设计key | 自定义分区类</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> rdd</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">HashPartitioner</span>, <span class="type">Partitioner</span>, <span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Random</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * ClassName RDDDataskew</span></span><br><span class="line"><span class="comment">  * Author    MxRanger</span></span><br><span class="line"><span class="comment">  * Date      2019/4/3</span></span><br><span class="line"><span class="comment">  * Time      20:19</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 数据倾斜:自定义分区，rdd中如果定义了分区对象，shuffle产生rdd时，应用的就是原来的分区对象</span></span><br><span class="line"><span class="comment">  * 需要的话，重新指定特定的分区对象</span></span><br><span class="line"><span class="comment">  **/</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RDDDataskew2</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建配置对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">&quot;WordCountScala&quot;</span>)</span><br><span class="line">    conf.setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建sparkContext对象</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="comment">//加载文件</span></span><br><span class="line">    <span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">&quot;file:///f:/sparktest/hello.txt&quot;</span>,<span class="number">3</span>)</span><br><span class="line">    <span class="comment">//压扁</span></span><br><span class="line">    <span class="comment">//val rdd2 = rdd1.flatMap(_.split(&quot; &quot;))</span></span><br><span class="line">    <span class="comment">//等同于上面</span></span><br><span class="line">    <span class="keyword">val</span> rdd2 = rdd1.flatMap(line=&gt;&#123;</span><br><span class="line">      line.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//标一成对</span></span><br><span class="line">    <span class="comment">//val rdd3 = rdd2.map((_,1))</span></span><br><span class="line">    <span class="keyword">val</span> rdd3 = rdd2.map(e=&gt;&#123;</span><br><span class="line">      (e,<span class="number">1</span>)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//自定义随机分区</span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">RandomPartitioner</span>(<span class="params">val n:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Partitioner</span></span>&#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = n</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        <span class="type">Random</span>.nextInt(n)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//随机分区，按照key聚合</span></span><br><span class="line">    <span class="keyword">val</span> rdd4 = rdd3.reduceByKey(<span class="keyword">new</span> <span class="type">RandomPartitioner</span>(<span class="number">4</span>),_+_)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//二次shuffle</span></span><br><span class="line">    <span class="keyword">val</span> rdd5 = rdd4.reduceByKey(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">3</span>),_+_)</span><br><span class="line"></span><br><span class="line">    rdd5.collect().foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="六、job调度"><a href="#六、job调度" class="headerlink" title="六、job调度"></a>六、job调度</h1><h2 id="1-集群模式"><a href="#1-集群模式" class="headerlink" title="1.集群模式"></a><strong>1.集群模式</strong></h2><p>​    1.standalone<br>​    2.yarn</p>
<h2 id="2-job的部署模式"><a href="#2-job的部署模式" class="headerlink" title="2.job的部署模式"></a>2.job的部署模式</h2><h3 id="1-client"><a href="#1-client" class="headerlink" title="1.client"></a>1.client</h3><p>​        <span style="color:red">默认模式</span><br>​        driver程序运行在client节点</p>
<p><img src="http://img.mxranger.cn/Spark/1554298408495.png" alt="1554298408495">    </p>
<h3 id="2-cluster-离线"><a href="#2-cluster-离线" class="headerlink" title="2.cluster(离线)"></a>2.cluster(离线)</h3><p><strong>driver运行在某个worker上,进程名称DriverWrapper。</strong><br>spark-shell不能以cluster模式运行。</p>
<p><img src="http://img.mxranger.cn/Spark/1554298419903.png" alt="1554298419903"></p>
<h2 id="3-job执行模式"><a href="#3-job执行模式" class="headerlink" title="3.job执行模式"></a><strong>3.job执行模式</strong></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[0] spark-submit --master spark://s101:7077 --deploy-mode client</span><br><span class="line"></span><br><span class="line">//spark-shell不能以cluster方式运行</span><br><span class="line">[1] spark-submit --master spark://s101:7077 --deploy-mode cluster</span><br></pre></td></tr></table></figure>

<p>查看jar包：<code>jar -tf myspark.jar</code></p>
<h3 id="1-以cluster方式运行job"><a href="#1-以cluster方式运行job" class="headerlink" title="1.以cluster方式运行job"></a>1.以cluster方式运行job</h3><p>​    a)上传jar到hdfs</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -put myspark.jar /spark/tutorial</span><br></pre></td></tr></table></figure>


<p>​    b)执行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --master spark://master:7077 --class wordcount.WordCountScala --deploy-mode cluster hdfs:///spark/tutorial/myspark.jar hello.txt</span><br></pre></td></tr></table></figure>

<h2 id="4、yarn模式"><a href="#4、yarn模式" class="headerlink" title="4、yarn模式"></a>4、yarn模式</h2><p>yarn模式下根本不存在spark集群,也需要每个节点都去安装spark软件包。只在客户端安装spark即可，提交作业时，spark本质上作为had oop的一个job来执行的，<strong>执行流程和hadoop的是一致的，只不过NM启动YarnChild的时候，启动的是spark的executor</strong>.<br>[yarn + client]<br>    driver运行在client节点上,Appmaster只负责请求资源。<br>[yarn + cluster]<br>    appmaster不但负责请求资源，还负责运行driver.</p>
<pre><code>[操作yarn-client]
        1.停止spark集群
            $&gt;/soft/spark/sbin/stop-allsh
        2.启动yarn
            $&gt;start-yarn.sh
        3.考察webui
            http://s101:8088
        
        4.在spark-env.sh中配置hadoop环境变量
            ...
            export HADOOP_CONF_DIR=/soft/hadoop/etc/hadoop
            ...
        5.提交job
            $&gt;spark-submit --master yarn --deploy-mode client --class WordCountScala myspark.jar /user/centos/data/hello10.txt

        6.为避免每次都上传200m的spark包，性能过低
            1)将spark job导出的zip包上传到hdfs.
                $&gt;hdfs dfs -put spark_libs.zip data
            2)配置spark,指定上传的文件
                [spark/conf/spark-default.conf]
                spark.yarn.archive  hdfs://mycluster/user/centos/data/spark_libs.zip
            3)执行job提交
                $&gt;spark-submit --master yarn --deploy-mode client --class WordCountScala myspark.jar /user/centos/data/hello10.txt
</code></pre>
<p>​<br>​    [操作yarn-cluster]<br>​            1.上传job的jar到hdfs<br>​                $&gt;hdfs dfs -put myspark.jar data/myspark.jar<br>​            2.提交作业<br>​                $&gt;spark-submit –master yarn –deploy-mode cluster –class WordCountScala hdfs://mycluster/user/centos/data/myspark.jar /user/centos/data/hello10.txt</p>
<p>第四季09 </p>
<p>第五季 概念………………….</p>
<p>第六季  spark内存管理………………….</p>
<p>内存管理<br>    1.StaticMemoryManager<br>        执行和存储相互独立，不能借用。<br>    2.UnifiedMemoryManager<br>        执行和存储相互可以相互借用。<br>    3.内存<br>        spark内存的管理–executor-memory只是堆内存。</p>
<p>​    <img src="http://img.mxranger.cn/Spark/1554431556754.png" alt="1554431556754"></p>
<h1 id="七、Spark-SQL"><a href="#七、Spark-SQL" class="headerlink" title="七、Spark SQL"></a>七、Spark SQL</h1><h2 id="1、介绍"><a href="#1、介绍" class="headerlink" title="1、介绍"></a>1、介绍</h2><p>​    Spark SQL是构建在Spark core模块之上的四大模块之一，提供DataFrame等丰富API，可以采用传统的SQL语句 进行数学计算。运行期间，会通过Spark查询优化器翻译成物理执行计划，并行计算后输出结果。底层计算原理仍 然采用RDD计算实现。</p>
<h2 id="2、spark和hive集成"><a href="#2、spark和hive集成" class="headerlink" title="2、spark和hive集成"></a>2、spark和hive集成</h2><h3 id="1、安装"><a href="#1、安装" class="headerlink" title="1、安装"></a>1、安装</h3><p>【0】hive的配置安装以及配置mysql作为元数据 <a href="E:\md\Hadoop.md">跳转</a></p>
<p>【1】需要在<strong>每台</strong>虚拟机上都安装hive，<strong>每台</strong>spark上都需要在spark/conf下创建到hive配置文件hive-site.xml软连接</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; ln -s /usr/local/hive/conf/hive-site.xml /usr/local/spark/conf/hive-site.xml</span><br></pre></td></tr></table></figure>

<p>【3】启动spark-shell</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; spark-shell</span><br></pre></td></tr></table></figure>
<p>常见的几个命令<br><img src="http://img.mxranger.cn/Spark/1554427396127.png" alt="1554427396127"></p>
<p><img src="http://img.mxranger.cn/Spark/1554427441992.png" alt="1554427441992"></p>
<p><img src="http://img.mxranger.cn/Spark/1554427641081.png" alt="1554427641081"></p>
<h2 id="3、idea编写spark-sql开发"><a href="#3、idea编写spark-sql开发" class="headerlink" title="3、idea编写spark sql开发"></a>3、idea编写spark sql开发</h2><h3 id="1、使用scala"><a href="#1、使用scala" class="headerlink" title="1、使用scala"></a>1、使用scala</h3><p> 1、创建模块，添加maven支持</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-sql --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">        <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-hive --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.scala-lang/scala-library --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-lang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-library<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.11.8<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>fastjson<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.15<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.17<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p> 2、复制core-site.xml，hdfs-site.xml，hive-site.xml文件到resource下</p>
<p> 3、编写scala程序</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkSQLDemo1</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">&quot;sparkSQL&quot;</span>)</span><br><span class="line">                                      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">                                      .enableHiveSupport()</span><br><span class="line">                                      .getOrCreate()</span><br><span class="line">    spark.sql(<span class="string">&quot;show databases&quot;</span>).show()</span><br><span class="line">    spark.sql(<span class="string">&quot;use test&quot;</span>).show()</span><br><span class="line">    spark.sql(<span class="string">&quot;show tables&quot;</span>).show()</span><br><span class="line">    spark.sql(<span class="string">&quot;select * from phone&quot;</span>).show()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>结果如下：<br>show databases结果：<br><img src="http://img.mxranger.cn/Spark/1554429109866.png" alt="1554429109866"><br>use test结果：<br><img src="http://img.mxranger.cn/Spark/1554429117007.png" alt="1554429117007"><br>show tables结果：<br><img src="http://img.mxranger.cn/Spark/1554429124474.png" alt="1554429124474"><br>select * from phone结果：<br><img src="http://img.mxranger.cn/Spark/1554429133528.png" alt="1554429133528"></p>
<h3 id="2、使用java"><a href="#2、使用java" class="headerlink" title="2、使用java"></a>2、使用java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkSQLJavaDemo1</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        SparkSession spark = SparkSession.builder().appName(<span class="string">&quot;sparkSQL&quot;</span>)</span><br><span class="line">                .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">                .enableHiveSupport()</span><br><span class="line">                .getOrCreate();</span><br><span class="line">        Dataset&lt;Row&gt; sql1 = spark.sql(<span class="string">&quot;show databases&quot;</span>);</span><br><span class="line">        sql1.show();</span><br><span class="line">        spark.sql(<span class="string">&quot;use test&quot;</span>);</span><br><span class="line">        spark.sql(<span class="string">&quot;show tables&quot;</span>);</span><br><span class="line">        spark.sql(<span class="string">&quot;select * from phone&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="CRUD"><a href="#CRUD" class="headerlink" title="CRUD"></a>CRUD</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * ClassName SparkSQLDemo1</span></span><br><span class="line"><span class="comment"> * Author    MxRanger</span></span><br><span class="line"><span class="comment"> * Date      2019/4/5</span></span><br><span class="line"><span class="comment"> * Time      9:54</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 使用java实现spark sql访问</span></span><br><span class="line"><span class="comment"> * SparkSQL:DataFrame，相当于表，存在于内存当中，等同于RDD</span></span><br><span class="line"><span class="comment"> * DataFrame ==== DataSet&lt;Row&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkSQLJavaDemo1</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        SparkSession spark = SparkSession.builder().appName(<span class="string">&quot;sparkSQL&quot;</span>)</span><br><span class="line">                .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">                .enableHiveSupport()</span><br><span class="line">                .getOrCreate();</span><br><span class="line">        spark.sql(<span class="string">&quot;use test&quot;</span>);</span><br><span class="line">        spark.sql(<span class="string">&quot;show tables&quot;</span>);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        查询</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        Dataset&lt;Row&gt; sql = spark.sql(<span class="string">&quot;select * from phone where price &gt; 6000&quot;</span>);</span><br><span class="line">        sql.createOrReplaceTempView(<span class="string">&quot;v_phone1&quot;</span>);<span class="comment">//将之前查询出来的表内容放到缓存中，便于二次操作</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        添加表,缓存中的表存到新表中去</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        spark.sql(<span class="string">&quot;create table phone2 as select * from v_phone1&quot;</span>);</span><br><span class="line">        spark.sql(<span class="string">&quot;select * from v_phone1&quot;</span>);</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        使用api来查询</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        sql.select(<span class="string">&quot;price&quot;</span>).show();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>若是要去除控制台的警告，只需将spark的包里conf下log4j.properties.template改名为log4j.properties，放入resources文件夹中，并将红色框出改为ERROR即可</p>
<p><img src="http://img.mxranger.cn/Spark/1554894545660.png" alt="1554894545660"></p>
<h3 id="3-常见问题"><a href="#3-常见问题" class="headerlink" title="3.常见问题"></a>3.常见问题</h3><pre><code>4.1)关闭版本验证
[hive-site.xml]
    &lt;property&gt;
    &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
    &lt;/property&gt;
    
4.2)idea下编程时出现无法instantiate SparkSession with Hive support 
maven中缺少以下依赖：
    &lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt;
    &lt;version&gt;2.4.0&lt;/version&gt;
    &lt;/dependency&gt;

4.3)mysql授权问题
mysql不允许远程连接.
mysql&gt;grant all privileges on *.* to &#39;root&#39;@&#39;192.168.231.1&#39; identified by &#39;root&#39; ;
mysql&gt;flush privileges ;

4.4)hdfs /tmp没有访问权
$&gt;hdfs dfs -chmod -R 777 /tmp
</code></pre>
<h2 id="4、hdfs文件中的WordCount"><a href="#4、hdfs文件中的WordCount" class="headerlink" title="4、hdfs文件中的WordCount"></a>4、hdfs文件中的WordCount</h2><h3 id="1、scala版-1"><a href="#1、scala版-1" class="headerlink" title="1、scala版"></a>1、scala版</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * ClassName SparkSQLDemo1</span></span><br><span class="line"><span class="comment">  * Author    MxRanger</span></span><br><span class="line"><span class="comment">  * Date      2019/4/5</span></span><br><span class="line"><span class="comment">  * Time      9:40</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  **/</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkSQLDemo2</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">&quot;sparkSQL&quot;</span>)</span><br><span class="line">                                      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">                                      .enableHiveSupport()</span><br><span class="line">                                      .getOrCreate()</span><br><span class="line">    <span class="keyword">val</span> rdd1 = spark.sparkContext.textFile(<span class="string">&quot;/spark/tutorial/hello.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd2 = rdd1.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">    <span class="comment">//导入sparksession的隐式转换</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">//将rdd转换成数据框</span></span><br><span class="line">    <span class="keyword">val</span> df = rdd2.toDF(<span class="string">&quot;word&quot;</span>)</span><br><span class="line">    <span class="comment">//将数据框做成临时视图</span></span><br><span class="line">    df.createOrReplaceTempView(<span class="string">&quot;_doc&quot;</span>)</span><br><span class="line"></span><br><span class="line">    spark.sql(<span class="string">&quot;select word,count(*) from _doc group by word&quot;</span>).show(<span class="number">1000</span>,<span class="literal">false</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="http://img.mxranger.cn/Spark/1554432459997.png" alt="1554432459997"></p>
<h3 id="2、java版-1"><a href="#2、java版-1" class="headerlink" title="2、java版"></a>2、java版</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.RowFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * ClassName SparkSQLDemo1</span></span><br><span class="line"><span class="comment"> * Author    MxRanger</span></span><br><span class="line"><span class="comment"> * Date      2019/4/5</span></span><br><span class="line"><span class="comment"> * Time      9:54</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 使用java实现spark sql访问</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkSQLJavaDemo2</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        SparkSession spark = SparkSession.builder().appName(<span class="string">&quot;sparkSQL&quot;</span>)</span><br><span class="line">                .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">                .enableHiveSupport()</span><br><span class="line">                .getOrCreate();</span><br><span class="line">        <span class="comment">//创建javaspark上下文</span></span><br><span class="line">        JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(spark.sparkContext());</span><br><span class="line">        <span class="comment">//加载文件</span></span><br><span class="line">        JavaRDD&lt;String&gt; rdd1 = sc.textFile(<span class="string">&quot;/spark/tutorial/hello.txt&quot;</span>);</span><br><span class="line">        JavaRDD&lt;String&gt; rdd2 = rdd1.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Iterator&lt;String&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Arrays.asList(s.split(<span class="string">&quot; &quot;</span>)).iterator();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">//将字符串变换成row</span></span><br><span class="line">        JavaRDD&lt;Row&gt; rdd3 = rdd2.map(<span class="keyword">new</span> Function&lt;String, Row&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Row <span class="title">call</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> RowFactory.create(word);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//构造表结构</span></span><br><span class="line">        StructField[] fields = <span class="keyword">new</span> StructField[<span class="number">1</span>];</span><br><span class="line">        fields[<span class="number">0</span>] = <span class="keyword">new</span> StructField(<span class="string">&quot;word&quot;</span>, DataTypes.StringType,<span class="keyword">true</span>, Metadata.empty());</span><br><span class="line">        <span class="comment">//表结构类型</span></span><br><span class="line">        StructType type = <span class="keyword">new</span> StructType(fields);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//将rdd变换成数据框</span></span><br><span class="line">        Dataset&lt;Row&gt; df = spark.createDataFrame(rdd3, type);</span><br><span class="line">        <span class="comment">//注册临时视图</span></span><br><span class="line">        df.createOrReplaceTempView(<span class="string">&quot;_doc&quot;</span>);</span><br><span class="line"></span><br><span class="line">        spark.sql(<span class="string">&quot;select word,count(*) from _doc group by word&quot;</span>).show();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="http://img.mxranger.cn/Spark/1554433262586.png" alt="1554433262586"></p>
<h2 id="5、客户订单左外连接查询"><a href="#5、客户订单左外连接查询" class="headerlink" title="5、客户订单左外连接查询"></a>5、客户订单左外连接查询</h2><p>使用sql完成cust和订单的数据查询，查询所有客户的定单总额。<br>|id | name  | total——price|<br>| —- | —- | —- |<br>|1  | smith | 200 |<br>注意：数据为custs.txt和orders.txt,文件的前两行是字段描述信息，使用时将前两行删除。</p>
<figure class="half">
    <img src="index_files/1554447042136.png">
    <img src="index_files/1554447059721.png">
</figure>

<h3 id="1、java版"><a href="#1、java版" class="headerlink" title="1、java版"></a>1、java版</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.RowFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DataTypes;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.Metadata;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructField;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructType;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * ClassName SparkSQLDemo1</span></span><br><span class="line"><span class="comment"> * Author    MxRanger</span></span><br><span class="line"><span class="comment"> * Date      2019/4/5</span></span><br><span class="line"><span class="comment"> * Time      9:54</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 使用java实现spark sql访问</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkSQLJavaDemo3</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        SparkSession spark = SparkSession.builder().appName(<span class="string">&quot;sparkSQL&quot;</span>)</span><br><span class="line">                .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">                .enableHiveSupport()</span><br><span class="line">                .getOrCreate();</span><br><span class="line">        <span class="comment">//[0]创建javaspark上下文</span></span><br><span class="line">        JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(spark.sparkContext());</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * ===========================custs 用户============================================</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="comment">//[1.1]加载文件</span></span><br><span class="line">        JavaRDD&lt;String&gt; c_rdd1 = sc.textFile(<span class="string">&quot;/spark/tutorial/custs.txt&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//[1.2]将字符串变换成row</span></span><br><span class="line">        JavaRDD&lt;Row&gt; c_rdd2 = c_rdd1.map(<span class="keyword">new</span> Function&lt;String, Row&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Row <span class="title">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] arr = line.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                Integer id = Integer.parseInt(arr[<span class="number">0</span>]);</span><br><span class="line">                String name = arr[<span class="number">1</span>];</span><br><span class="line">                Integer age = Integer.parseInt(arr[<span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> RowFactory.create(id,name,age);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//[1.3]构造表结构</span></span><br><span class="line">        StructField[] c_fields = <span class="keyword">new</span> StructField[<span class="number">3</span>];</span><br><span class="line">        c_fields[<span class="number">0</span>] = <span class="keyword">new</span> StructField(<span class="string">&quot;id&quot;</span>, DataTypes.IntegerType,<span class="keyword">false</span>, Metadata.empty());</span><br><span class="line">        c_fields[<span class="number">1</span>] = <span class="keyword">new</span> StructField(<span class="string">&quot;name&quot;</span>, DataTypes.StringType,<span class="keyword">true</span>, Metadata.empty());</span><br><span class="line">        c_fields[<span class="number">2</span>] = <span class="keyword">new</span> StructField(<span class="string">&quot;age&quot;</span>, DataTypes.IntegerType,<span class="keyword">true</span>, Metadata.empty());</span><br><span class="line">        <span class="comment">//表结构类型</span></span><br><span class="line">        StructType c_type = <span class="keyword">new</span> StructType(c_fields);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//[1.4]将rdd变换成数据框</span></span><br><span class="line">        <span class="keyword">final</span> Dataset&lt;Row&gt; c_df = spark.createDataFrame(c_rdd2, c_type);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//[1.5]注册临时视图</span></span><br><span class="line">        c_df.createOrReplaceTempView(<span class="string">&quot;_custs&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * ===========================orders 用户============================================</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="comment">//[2.1]加载文件</span></span><br><span class="line">        JavaRDD&lt;String&gt; o_rdd1 = sc.textFile(<span class="string">&quot;/spark/tutorial/orders.txt&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//[2.2]将字符串变换成row</span></span><br><span class="line">        JavaRDD&lt;Row&gt; o_rdd2 = o_rdd1.map(<span class="keyword">new</span> Function&lt;String, Row&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Row <span class="title">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] arr = line.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                Integer id = Integer.parseInt(arr[<span class="number">0</span>]);</span><br><span class="line">                String orderno = arr[<span class="number">1</span>];</span><br><span class="line">                Float price = Float.parseFloat(arr[<span class="number">2</span>]);</span><br><span class="line">                Integer cid = Integer.parseInt(arr[<span class="number">3</span>]);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> RowFactory.create(id,orderno,price,cid);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//[2.3]构造表结构</span></span><br><span class="line">        StructField[] o_fields = <span class="keyword">new</span> StructField[<span class="number">4</span>];</span><br><span class="line">        o_fields[<span class="number">0</span>] = <span class="keyword">new</span> StructField(<span class="string">&quot;id&quot;</span>, DataTypes.IntegerType,<span class="keyword">false</span>, Metadata.empty());</span><br><span class="line">        o_fields[<span class="number">1</span>] = <span class="keyword">new</span> StructField(<span class="string">&quot;orderno&quot;</span>, DataTypes.StringType,<span class="keyword">true</span>, Metadata.empty());</span><br><span class="line">        o_fields[<span class="number">2</span>] = <span class="keyword">new</span> StructField(<span class="string">&quot;price&quot;</span>, DataTypes.FloatType,<span class="keyword">true</span>, Metadata.empty());</span><br><span class="line">        o_fields[<span class="number">3</span>] = <span class="keyword">new</span> StructField(<span class="string">&quot;cid&quot;</span>, DataTypes.IntegerType,<span class="keyword">true</span>, Metadata.empty());</span><br><span class="line">        <span class="comment">//[2.4]表结构类型</span></span><br><span class="line">        StructType o_type = <span class="keyword">new</span> StructType(o_fields);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//[2.5]将rdd变换成数据框</span></span><br><span class="line">        Dataset&lt;Row&gt; o_df = spark.createDataFrame(o_rdd2, o_type);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//[2.6]注册临时视图</span></span><br><span class="line">        o_df.createOrReplaceTempView(<span class="string">&quot;_orders&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//[3]执行查询</span></span><br><span class="line">        spark.sql(<span class="string">&quot;select * from _custs&quot;</span>).show(<span class="number">100</span>,<span class="keyword">false</span>);</span><br><span class="line">        spark.sql(<span class="string">&quot;select * from _orders&quot;</span>).show(<span class="number">100</span>,<span class="keyword">false</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//[4]拼接sql语句</span></span><br><span class="line">        <span class="comment">//相同cid的求price总和</span></span><br><span class="line">        String sql1 = <span class="string">&quot;( select cid , sum(price) _sum from _orders group by cid )&quot;</span>;</span><br><span class="line">        <span class="comment">//左外连接</span></span><br><span class="line">        String sql = <span class="string">&quot;select c.id , c.name , ifnull(o._sum,0) total_price from _custs c left outer join &quot;</span></span><br><span class="line">                + sql1 + <span class="string">&quot; o &quot;</span> + <span class="string">&quot;on c.id = o.cid&quot;</span>;</span><br><span class="line">        <span class="comment">//[5]展示</span></span><br><span class="line">        spark.sql(sql).show(<span class="number">1000</span>,<span class="keyword">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<figure class="third">
    <img src="index_files/1554447264873.png" width="210">
    <img src="index_files/1554447272568.png" width="210">
    <img src="index_files/1554447288135.png" width="210">
</figure>
## 6、spark sql读写json

<p>对数据框进行读写json</p>
<h3 id="1、写入"><a href="#1、写入" class="headerlink" title="1、写入"></a>1、写入</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * ClassName SparkSQLDemo1</span></span><br><span class="line"><span class="comment">  * Author    MxRanger</span></span><br><span class="line"><span class="comment">  * Date      2019/4/5</span></span><br><span class="line"><span class="comment">  * Time      9:40</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * 将数据框保存到json数据</span></span><br><span class="line"><span class="comment">  * 加载json数据成DataFrame</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  **/</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkSQLJson</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">&quot;sparkSQL&quot;</span>)</span><br><span class="line">                                      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">                                      .enableHiveSupport()</span><br><span class="line">                                      .getOrCreate()</span><br><span class="line">    <span class="keyword">val</span> rdd1 = spark.sparkContext.textFile(<span class="string">&quot;/spark/tutorial/custs.txt&quot;</span>)</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> df1 = rdd1.map(line=&gt;&#123;</span><br><span class="line">      <span class="keyword">val</span> arr = line.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">      (arr(<span class="number">0</span>).toInt,arr(<span class="number">1</span>),arr(<span class="number">2</span>).toInt)</span><br><span class="line">    &#125;).toDF(<span class="string">&quot;id&quot;</span>,<span class="string">&quot;name&quot;</span>,<span class="string">&quot;age&quot;</span>)</span><br><span class="line"></span><br><span class="line">    df1.show(<span class="number">1000</span>,<span class="literal">false</span>)</span><br><span class="line">    df1.where(<span class="string">&quot;id&gt;2&quot;</span>).write.json(<span class="string">&quot;file:///f:/sparktest/custs.json&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="half">
    <img src="index_files/1554448371066.png" width="300">
    <img src="index_files/1554448378016.png" width="300">
</figure>
### 2、读取json

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 读取</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">val</span> r_df1 = spark.read.json(<span class="string">&quot;file:///f:/sparktest/custs.json&quot;</span>)</span><br><span class="line">r_df1.show(<span class="number">1000</span>,<span class="literal">false</span>)</span><br></pre></td></tr></table></figure>

<p><img src="http://img.mxranger.cn/Spark/1554448495071.png" alt="1554448495071"></p>
<h2 id="7、spark-sql读写parquet"><a href="#7、spark-sql读写parquet" class="headerlink" title="7、spark sql读写parquet"></a>7、spark sql读写parquet</h2><p>和读写json类似，对数据框进行读写parquet</p>
<p>1、写入</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = spark.sparkContext.textFile(<span class="string">&quot;/spark/tutorial/custs.txt&quot;</span>)</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> df1 = rdd1.map(line=&gt;&#123;</span><br><span class="line">      <span class="keyword">val</span> arr = line.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">      (arr(<span class="number">0</span>).toInt,arr(<span class="number">1</span>),arr(<span class="number">2</span>).toInt)</span><br><span class="line">    &#125;).toDF(<span class="string">&quot;id&quot;</span>,<span class="string">&quot;name&quot;</span>,<span class="string">&quot;age&quot;</span>)</span><br><span class="line"></span><br><span class="line">    df1.show(<span class="number">1000</span>,<span class="literal">false</span>)</span><br><span class="line">	<span class="comment">//写入</span></span><br><span class="line">    df1.where(<span class="string">&quot;id&gt;2&quot;</span>).write.parquet(<span class="string">&quot;file:///f:/sparktest/par&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>2、读取</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 读取</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">val</span> r_df1 = spark.read.parquet(<span class="string">&quot;file:///f:/sparktest/par&quot;</span>)</span><br><span class="line">r_df1.show(<span class="number">1000</span>,<span class="literal">false</span>)</span><br></pre></td></tr></table></figure>

<h2 id="8、spark-sql读写mysql数据库"><a href="#8、spark-sql读写mysql数据库" class="headerlink" title="8、spark sql读写mysql数据库"></a>8、spark sql读写mysql数据库</h2><p>对数据框进行读写mysql</p>
<p>1、保存</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">&quot;sparkSQL&quot;</span>)</span><br><span class="line">                                      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">                                      .enableHiveSupport()</span><br><span class="line">                                      .getOrCreate()</span><br><span class="line"><span class="keyword">val</span> rdd1 = spark.sparkContext.textFile(<span class="string">&quot;/spark/tutorial/custs.txt&quot;</span>)</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> df1 = rdd1.map(line=&gt;&#123;</span><br><span class="line">      <span class="keyword">val</span> arr = line.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">      (arr(<span class="number">0</span>).toInt,arr(<span class="number">1</span>),arr(<span class="number">2</span>).toInt)</span><br><span class="line">    &#125;).toDF(<span class="string">&quot;id&quot;</span>,<span class="string">&quot;name&quot;</span>,<span class="string">&quot;age&quot;</span>)</span><br><span class="line"></span><br><span class="line">    df1.show(<span class="number">1000</span>,<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> url = <span class="string">&quot;jdbc:mysql://192.168.159.100:3306/spark&quot;</span></span><br><span class="line">    <span class="keyword">val</span> table = <span class="string">&quot;custs&quot;</span></span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    prop.put(<span class="string">&quot;driver&quot;</span>, <span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)</span><br><span class="line">    prop.put(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">    prop.put(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;1234&quot;</span>)</span><br><span class="line">    <span class="comment">//表不能存在，自动创建</span></span><br><span class="line">    df1.where(<span class="string">&quot;id &gt; 2&quot;</span>).write.jdbc(url,table,prop)</span><br></pre></td></tr></table></figure>



<p>2、读取</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">&quot;sparkSQL&quot;</span>)</span><br><span class="line">                                      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">                                      .enableHiveSupport()</span><br><span class="line">                                      .getOrCreate()</span><br><span class="line"><span class="keyword">val</span> url = <span class="string">&quot;jdbc:mysql://192.168.159.100:3306/spark&quot;</span></span><br><span class="line"><span class="keyword">val</span> table = <span class="string">&quot;custs&quot;</span></span><br><span class="line"><span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">prop.put(<span class="string">&quot;driver&quot;</span>, <span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)</span><br><span class="line">prop.put(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">prop.put(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;1234&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> df1 = spark.read.jdbc(url,table,prop)</span><br><span class="line">df1.show(<span class="number">1000</span>,<span class="literal">false</span>)</span><br></pre></td></tr></table></figure>

<h2 id="9、Spark-SQL作为分布式查询引擎"><a href="#9、Spark-SQL作为分布式查询引擎" class="headerlink" title="9、Spark SQL作为分布式查询引擎"></a>9、Spark SQL作为分布式查询引擎</h2><p><img src="http://img.mxranger.cn/Spark/1554452584773.png" alt="1554452584773"></p>
<p>1.启动spark集群</p>
<p>2.启动spark thriftserver<br>服务器，接受客户端jdbc的请求。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$&gt;/soft/spark/sbin/start-thriftserver.sh --master spark://s101:7077</span><br></pre></td></tr></table></figure>



<p>3.验证是否启动成功或者webui</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$&gt;netstat -anop | grep 10000</span><br></pre></td></tr></table></figure>



<p>4.连接到thriftserver<br>4.1)beeline</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$&gt;/soft/spark/bin/beeline</span><br><span class="line">        $beeline&gt;!conn jdbc:hive2://localhost:10000/big12 ;</span><br><span class="line">        $beeline&gt;select * from orders ;</span><br></pre></td></tr></table></figure>

<p>​    </p>
<p>4.2)编程API访问thriftserver<br>    a)pom.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0&quot;</span></span></span><br><span class="line"><span class="tag">						 <span class="attr">xmlns:xsi</span>=<span class="string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span></span><br><span class="line"><span class="tag">						 <span class="attr">xsi:schemaLocation</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span></span><br><span class="line">					<span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line">	</span><br><span class="line">					<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>big13<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">					<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>myspark-sql<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">					<span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">	</span><br><span class="line">					<span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">						<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">							<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">							<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">							<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">						<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">						<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">							<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">							<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">							<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">						<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">						<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">							<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">							<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">							<span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.17<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">						<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">						<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">							<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">							<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">							<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">						<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">						<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">							<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">							<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">							<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">						<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">					<span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line">	</span><br><span class="line">				<span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>​    b) java类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> big13.spark.sql;</span><br><span class="line">	</span><br><span class="line">				<span class="keyword">import</span> java.sql.Connection;</span><br><span class="line">				<span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line">				<span class="keyword">import</span> java.sql.PreparedStatement;</span><br><span class="line">				<span class="keyword">import</span> java.sql.ResultSet;</span><br><span class="line">	</span><br><span class="line">				<span class="comment">/**</span></span><br><span class="line"><span class="comment">				 * 使用Spar SQL分布式查询引擎</span></span><br><span class="line"><span class="comment">				 */</span></span><br><span class="line">				<span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkSQLThriftServerDemo1</span> </span>&#123;</span><br><span class="line">					<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">						String driver = <span class="string">&quot;org.apache.hive.jdbc.HiveDriver&quot;</span> ;</span><br><span class="line">						Class.forName(driver);</span><br><span class="line">						String url = <span class="string">&quot;jdbc:hive2://s101:10000&quot;</span> ;</span><br><span class="line">	</span><br><span class="line">						Connection conn = DriverManager.getConnection(url) ;</span><br><span class="line">						PreparedStatement ppst = conn.prepareStatement(<span class="string">&quot;select * from big12.orders&quot;</span>) ;</span><br><span class="line">						ResultSet rs = ppst.executeQuery() ;</span><br><span class="line">						<span class="keyword">while</span>(rs.next())&#123;</span><br><span class="line">							<span class="keyword">int</span> id = rs.getInt(<span class="string">&quot;id&quot;</span>) ;</span><br><span class="line">							String orderno = rs.getString(<span class="string">&quot;orderno&quot;</span>) ;</span><br><span class="line">							<span class="keyword">float</span> price = rs.getFloat(<span class="string">&quot;price&quot;</span>) ;</span><br><span class="line">							<span class="keyword">int</span> cid = rs.getInt(<span class="string">&quot;cid&quot;</span>) ;</span><br><span class="line">							System.out.printf(<span class="string">&quot;%d/%s/%f/%d\r\n&quot;</span> , id , orderno , price ,cid);</span><br><span class="line">						&#125;</span><br><span class="line">						rs.close();</span><br><span class="line">						ppst.close();</span><br><span class="line">						conn.close();</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;</span><br></pre></td></tr></table></figure>

<h1 id="八、Spark-Streaming"><a href="#八、Spark-Streaming" class="headerlink" title="八、Spark Streaming"></a>八、Spark Streaming</h1><ul>
<li><p>之前的计算属于批量计算（Spark batch），静态数据，终会停止</p>
</li>
<li><p>storm：apache流计算框架，实时性高，吞吐量小</p>
</li>
<li><p>Spark streaming：流计算模块，动态数据，不会停止。<strong>对应实时计算</strong></p>
</li>
</ul>
<blockquote>
<p>spark streaming是准实时计算。</p>
<p>内部原理通过小批次计算，按照时间片切割</p>
</blockquote>
<h2 id="1、介绍-1"><a href="#1、介绍-1" class="headerlink" title="1、介绍"></a>1、介绍</h2><p>​    Spark Streaming是Spark core API的扩展，针对实时数据流计算，具有可伸缩性、高吞吐量、自动容错机制的特 点。数据源可以来自于多种方式，例如kafka、flume等等。使用类似于RDD的高级算子进行复杂计算，像map、 reduce、join和window等等。最后，处理的数据推送到数据库、文件系统或者仪表盘等。也可以对流计算应用机 器学习和图计算。</p>
<p><img src="http://img.mxranger.cn/Spark/streaming-arch.png" alt="Spark Streaming"></p>
<p>在内部，spark streaming接收实时数据流，然后切割成一个个批次，然后通过spark引擎生成result的数据流。</p>
<p><img src="http://img.mxranger.cn/Spark/streaming-flow.png" alt="Spark Streaming"></p>
<p>Spark Streaming提供了称为离散流（DStream-discretized stream）的高级抽象，代表了连续的数据流。离散流 通过kafka、flume等源创建，也可以通过高级操作像map、filter等变换得到，类似于RDD的行为。内部，离散流 表现为连续的RDD。</p>
<h2 id="2、实现WordCount"><a href="#2、实现WordCount" class="headerlink" title="2、实现WordCount"></a>2、实现WordCount</h2><h3 id="1、使用nc作为生产数据"><a href="#1、使用nc作为生产数据" class="headerlink" title="1、使用nc作为生产数据"></a>1、使用nc作为生产数据</h3><p><img src="http://img.mxranger.cn/Spark/1554466999323.png" alt="1554466999323"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">centos中安装nc</span><br><span class="line">&gt; yum install -y nc</span><br><span class="line"></span><br><span class="line">启动nc</span><br><span class="line">&gt; nc -lk 8888</span><br></pre></td></tr></table></figure>



<h3 id="2、添加依赖包"><a href="#2、添加依赖包" class="headerlink" title="2、添加依赖包"></a>2、添加依赖包</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>groupId<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.scala-lang/scala-library --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-lang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-library<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.11.8<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>fastjson<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.15<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.17<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="3、编写程序"><a href="#3、编写程序" class="headerlink" title="3、编写程序"></a>3、编写程序</h3><h4 id="1、scala版-2"><a href="#1、scala版-2" class="headerlink" title="1、scala版"></a>1、scala版</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * ClassName SparkStreamingWorldCountScala</span></span><br><span class="line"><span class="comment">  * Author    MxRanger</span></span><br><span class="line"><span class="comment">  * Date      2019/4/5</span></span><br><span class="line"><span class="comment">  * Time      19:52</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * scala 实现spark streaming</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  **/</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreamingWorldCountScala</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">&quot;StreamingWorldCount&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local[*]&quot;</span>) <span class="comment">//local模式，&gt;1 *：动态提取cpu核数，有多少开多少</span></span><br><span class="line">    <span class="comment">//流上下文</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf,<span class="type">Seconds</span>(<span class="number">2</span>))</span><br><span class="line">    <span class="comment">//返回类型ReceiverInputDStream就是rdd流</span></span><br><span class="line">    <span class="comment">//创建套接字文本流</span></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">&quot;master&quot;</span>,<span class="number">8888</span>)</span><br><span class="line">    <span class="comment">//单词序列</span></span><br><span class="line">    <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">    <span class="comment">//标一成对</span></span><br><span class="line">    <span class="keyword">val</span> pair = words.map((_,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">//统计单词数</span></span><br><span class="line">    <span class="keyword">val</span> result = pair.reduceByKey(_+_)</span><br><span class="line">    result.print()</span><br><span class="line">    <span class="comment">//启动上下文</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h4 id="2、java版-2"><a href="#2、java版-2" class="headerlink" title="2、java版"></a>2、java版</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Durations;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaPairDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * ClassName SparkStreamingWorldCount</span></span><br><span class="line"><span class="comment"> * Author    MxRanger</span></span><br><span class="line"><span class="comment"> * Date      2019/4/5</span></span><br><span class="line"><span class="comment"> * Time      20:11</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * java 实现spark streaming</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkStreamingWorldCount</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line"></span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">        conf.setAppName(<span class="string">&quot;StreamingWorldCount&quot;</span>)</span><br><span class="line">                .setMaster(<span class="string">&quot;local[*]&quot;</span>); <span class="comment">//local模式，&gt;1 *：动态提取cpu核数，有多少开多少</span></span><br><span class="line">        <span class="comment">//创建java流上下文</span></span><br><span class="line">        JavaStreamingContext ssc = <span class="keyword">new</span> JavaStreamingContext(conf, Durations.seconds(<span class="number">2</span>));</span><br><span class="line">        <span class="comment">//创建socket文件流</span></span><br><span class="line">        JavaDStream&lt;String&gt; lines = ssc.socketTextStream(<span class="string">&quot;master&quot;</span>, <span class="number">8888</span>);</span><br><span class="line">        <span class="comment">//压扁成单词流</span></span><br><span class="line">        JavaDStream&lt;String&gt; words = lines.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line"></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Iterator&lt;String&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Arrays.asList(s.split(<span class="string">&quot; &quot;</span>)).iterator();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; pairs = words.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(s, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">//聚合</span></span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; result = pairs.reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer v1, Integer v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        result.print();</span><br><span class="line">        ssc.start();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            ssc.awaitTermination();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<p><img src="http://img.mxranger.cn/Spark/sparkstreaming.gif" alt="sparkstreaming"></p>
<h2 id="3、基本概念"><a href="#3、基本概念" class="headerlink" title="3、基本概念"></a>3、基本概念</h2><h3 id="3-1-StreamingContext初始化"><a href="#3-1-StreamingContext初始化" class="headerlink" title="3.1 StreamingContext初始化"></a>3.1 StreamingContext初始化</h3><p>​    appName是应用程序名称，master是Spark,、Mesos或YARN，也可以是local，local是本地模式运行。实际应 用不需要指定master值，通过spark-submit提交命令中获取该参数，定义完上下文后，必须要完成如下工作：</p>
<ul>
<li><p>通过创建离散流定义数据源 </p>
</li>
<li><p>为流定义变换等计算工作 </p>
</li>
<li><p>streamingContext.start() 开始接受数据并进行处理 </p>
</li>
<li><p>使用 streamingContext.awaitTermination() 函数停止应用 </p>
</li>
<li><p>手动调用 streamingContext.stop() 方法停止应用</p>
</li>
</ul>
<p>切记：</p>
<ul>
<li><p>上下文启动后，不能设置新的计算方法 </p>
</li>
<li><p>上下文停止后，不能重启 </p>
</li>
<li><p>流上下文停止后，还会停止SparkContext，如果不希望停止SparkContext，可以通过stop(false)。 </p>
</li>
<li><p>SparkContext可以重用来创建多个流上下文，新的流上下文创建前需要停止上一个流上下文。</p>
</li>
</ul>
<h3 id="3-2-离散流-DStream"><a href="#3-2-离散流-DStream" class="headerlink" title="3.2 离散流(DStream)"></a>3.2 离散流(DStream)</h3><p>离散流是Spark流应用的抽象，表示的是连续的数据流，数据流要么从数据源而来，或者通过变换生成。在内部， 离散流表示连续的RDD。</p>
<p><img src="http://img.mxranger.cn/Spark/streaming-dstream.png" alt="Spark Streaming"></p>
<p>对离散流的任何应用，都会转换为操纵底层的RDD：</p>
<p><img src="http://img.mxranger.cn/Spark/streaming-dstream-ops.png" alt="Spark Streaming"></p>
<p>底层的RDD变换工作，Spark引擎进行计算。</p>
<h3 id="3-3-Input-DStream和Receiver"><a href="#3-3-Input-DStream和Receiver" class="headerlink" title="3.3 Input DStream和Receiver"></a>3.3 Input DStream和Receiver</h3><p>InputStream Dstream也是一种DStream，从数据源接受的数据流，每个Input DStream都和一个Receiver关 联，Receiver是接受数据并存储在Spark内存中。</p>
<p>Spark内部提供了两种类型的源：</p>
<ul>
<li><p>基本源 </p>
<p>Spark API直接能够使用，比如FileSystem或Socket连接。 </p>
</li>
<li><p>高级源 </p>
<p>像kafka、flume等源需要借助于第三方工具类进行连接。</p>
</li>
</ul>
<p>Spark可以在一个流上下文中创建多个InputStream，就可以进行并行计算，这些创建的多个Input DStream具有 相同时间切片，不可能给不同的Input DStream分别设置时间切片，因为时间切片设置在StreamContext中完成， 同时也会创建多个Receiver。接受器单独占用一个CPU内核，即在一个单独的线程中死循环方式读取数据，需要 分配足够的cpu内核来处理数据。保证CPU内核数据大于Receiver个数。</p>
<p><strong>注意事项：</strong><br><span style="color:red"><strong>本地模式下，不能使用local或者local[1]，Receiver占据唯一的线程，没有线程执行计算工作。</strong></span></p>
<p><span style="color:red"> <strong>扩展到集群，内核数大于Receiver个数。</strong></span></p>
<h2 id="4、Receiver"><a href="#4、Receiver" class="headerlink" title="4、Receiver"></a>4、Receiver</h2><h3 id="4-1-内部结构"><a href="#4-1-内部结构" class="headerlink" title="4.1 内部结构"></a>4.1 内部结构</h3><p>Receiver内部维护了队列，放置的是Block对象，Block包含blockId的ArrayBuffer两个属性。每个Block对应一个 分区，默认每200ms（可通过spark.streaming.blockInterval修改）生成一个Block对象并推送到队列中，在 StreamingContext中指定的时间片就是一个RDD的时长，因此每个RDD含有多少分区，只要计算一下是200ms的 多少倍，然后就可以确定RDD内含有多少个分区了，但如果没有产生数据，就就不会生成分区，因此分区数不会超 过这个倍数。内部结果如图所示：</p>
<p><img src="http://img.mxranger.cn/Spark/1554513185060.png" alt="1554513185060"></p>
<h3 id="4-2-分区数控制"><a href="#4-2-分区数控制" class="headerlink" title="4.2 分区数控制"></a>4.2 分区数控制</h3><p>修改块生成间隔即可改变分区数，代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//时间片，StreamingContext的时间长度。多长时间一个rdd.</span></span><br><span class="line"><span class="comment">//rdd内分区，分区的数量的控制通过</span></span><br><span class="line"><span class="comment">//spark.streaming.blockInverval=200m</span></span><br><span class="line">conf.set(<span class="string">&quot;spark.streaming.blockInverval&quot;</span> , <span class="string">&quot;200ms&quot;</span>)</span><br></pre></td></tr></table></figure>



<h3 id="4-3-限速处理"><a href="#4-3-限速处理" class="headerlink" title="4.3 限速处理"></a>4.3 限速处理</h3><h4 id="1、每秒接收记录数"><a href="#1、每秒接收记录数" class="headerlink" title="1、每秒接收记录数"></a>1、每秒接收记录数</h4><p>  Spark Streaming可以控制每个Receiver每秒接收消息条数的上限，默认没有设置，就没有上限。可以手动设 置接受速率的上限，该种方式缺点可能对集群处理能力估计不足，导致计算资源浪费。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//限速 , 每秒钟最多20条记录</span></span><br><span class="line">conf.set(<span class="string">&quot;spark.streaming.receiver.maxRate&quot;</span> , <span class="string">&quot;20&quot;</span>)</span><br></pre></td></tr></table></figure>



<h4 id="2、压后-backpress-处理"><a href="#2、压后-backpress-处理" class="headerlink" title="2、压后(backpress)处理"></a>2、压后(backpress)处理</h4><p>  可以上spark Streaming基于当前batch的调度延迟与处理时间来控制接收速率，以备让系统只接受系统能够 处理的速率。可以通过spark.streaming.backpressure.enabled属性开启，默认该属性是禁用的。对于 Receiver的第一个批次的速率限制通过spark.streaming.backpressure.initialRate进行设置。启用压后处理 属性后，在Spark Streaming内部，会动态设置Receiver接收速率的最大值。如果设置了 spark.streaming.receiver.maxRate和spark.streaming.kafka.maxRatePerP-artition属性，则速率不会超 过这一设置。具体配置方式如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//反压处理</span></span><br><span class="line"><span class="comment">//启用压后控制</span></span><br><span class="line">conf.set(<span class="string">&quot;spark.streaming.backpressure.enabled&quot;</span> , <span class="string">&quot;true&quot;</span>)</span><br><span class="line"><span class="comment">//设置第一个batch的接收速率</span></span><br><span class="line">conf.set(<span class="string">&quot;spark.streaming.backpressure.initialRate&quot;</span> , <span class="string">&quot;10000&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="5、Window窗口操作"><a href="#5、Window窗口操作" class="headerlink" title="5、Window窗口操作"></a>5、Window窗口操作</h2><h3 id="5-1-介绍"><a href="#5-1-介绍" class="headerlink" title="5.1 介绍"></a>5.1 介绍</h3><p>窗口是若干RDD的集合，窗口的长度必须是批次的整倍数，窗口的滑动间隔也必须是批次整倍数。</p>
<p><span style="color:red"><strong>比如每分钟查询 最近一小时内的百度热词，一分钟就是窗口的滑动间隔，一小时就是窗口长度。</strong></span></p>
<p><img src="http://img.mxranger.cn/Spark/streaming-dstream-window.png" alt="窗口操作"></p>
<p><img src="http://img.mxranger.cn/Spark/1554516208399.png" alt="1554516208399"></p>
<h3 id="5-2、编写程序"><a href="#5-2、编写程序" class="headerlink" title="5.2、编写程序"></a>5.2、编写程序</h3><p><strong>案例：每隔4秒计算一次前10秒内的wordcount</strong></p>
<p>1、使用nc -lk 8888开启socket服务</p>
<p>2、编写窗口程序</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * ClassName SparkStreamingWorldCountScala</span></span><br><span class="line"><span class="comment">  * Author    MxRanger</span></span><br><span class="line"><span class="comment">  * Date      2019/4/5</span></span><br><span class="line"><span class="comment">  * Time      19:52</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * spark streaming 的窗口统计</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  **/</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreamingWorldCountWindowScala</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">&quot;StreamingWorldCount&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local[*]&quot;</span>) <span class="comment">//local模式，&gt;1 *：动态提取cpu核数，有多少开多少</span></span><br><span class="line">    <span class="comment">//流上下文</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf,<span class="type">Seconds</span>(<span class="number">2</span>))</span><br><span class="line">    <span class="comment">//返回类型ReceiverInputDStream就是rdd流</span></span><br><span class="line">    <span class="comment">//创建套接字文本流</span></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">&quot;master&quot;</span>,<span class="number">8888</span>)</span><br><span class="line">    <span class="comment">//单词序列</span></span><br><span class="line">    <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">    <span class="comment">//标一成对</span></span><br><span class="line">    <span class="keyword">val</span> pair = words.map((_,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">//统计单词数</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * reduceFunc: (V, V) =&gt; V,   累加</span></span><br><span class="line"><span class="comment">      windowDuration: Duration,  窗口长度</span></span><br><span class="line"><span class="comment">      slideDuration: Duration    滑动间隔 ，若之前定义了时间间隔，以这个时间为准</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line">    <span class="keyword">val</span> result = pair.reduceByKeyAndWindow((a:<span class="type">Int</span>,b:<span class="type">Int</span>)=&gt;(a+b),<span class="type">Seconds</span>(<span class="number">10</span>),<span class="type">Seconds</span>(<span class="number">4</span>))</span><br><span class="line">    result.print()</span><br><span class="line">    <span class="comment">//启动上下文</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<p><img src="http://img.mxranger.cn/Spark/1554535851737.png" alt="1554535851737"></p>
<h2 id="6、updateStateByKey"><a href="#6、updateStateByKey" class="headerlink" title="6、updateStateByKey"></a>6、updateStateByKey</h2><p><strong>按key更新状态</strong>是spark streaming对k-v类型的DStream提供的操作，是对每个K关联一个状态对象，可以是任何 对象，该状态对象会随着DStream的流动，从上一个的RDD流向到下一个RDD，工作流程如下图所示：</p>
<p><span style="color:red"><strong>从spark streaming开启的那一刻对key进行累加统计，而不是window那样只对某一块进行统计</strong></span></p>
<h3 id="程序"><a href="#程序" class="headerlink" title="程序"></a>程序</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * ClassName SparkStreamingWorldCountScala</span></span><br><span class="line"><span class="comment">  * Author    MxRanger</span></span><br><span class="line"><span class="comment">  * Date      2019/4/5</span></span><br><span class="line"><span class="comment">  * Time      19:52</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * spark streaming按照key更新状态</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  **/</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreamingWorldCountUpdateByKeyScala</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">&quot;StreamingWorldCount&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local[*]&quot;</span>) <span class="comment">//local模式，&gt;1 *：动态提取cpu核数，有多少开多少</span></span><br><span class="line">    <span class="comment">//流上下文,每个2秒进行一次统计</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf,<span class="type">Seconds</span>(<span class="number">2</span>))</span><br><span class="line">    <span class="comment">//需要设置检查点，存放之前的记录</span></span><br><span class="line">    ssc.checkpoint(<span class="string">&quot;file:///f:sparktest/streamingcheckpoint&quot;</span>)</span><br><span class="line">    <span class="comment">//返回类型ReceiverInputDStream就是rdd流</span></span><br><span class="line">    <span class="comment">//创建套接字文本流</span></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">&quot;master&quot;</span>,<span class="number">8888</span>)</span><br><span class="line">    <span class="comment">//单词序列</span></span><br><span class="line">    <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">    <span class="comment">//标一成对</span></span><br><span class="line">    <span class="keyword">val</span> pair = words.map((_,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">//统计单词数</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * reduceFunc: (V, V) =&gt; V,   累加</span></span><br><span class="line"><span class="comment">      windowDuration: Duration,  窗口长度</span></span><br><span class="line"><span class="comment">      slideDuration: Duration    滑动间隔</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line">    <span class="keyword">val</span> result = pair.reduceByKey((a:<span class="type">Int</span>,b:<span class="type">Int</span>)=&gt;&#123;a+b&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">updateFunc</span></span>(vs:<span class="type">Seq</span>[<span class="type">Int</span>] , st:<span class="type">Option</span>[<span class="type">Int</span>]):<span class="type">Option</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">      <span class="keyword">var</span> currCount = <span class="number">0</span></span><br><span class="line">      <span class="keyword">if</span>(vs != <span class="literal">null</span> &amp;&amp; !vs.isEmpty)&#123;</span><br><span class="line">        currCount = vs.sum</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">var</span> newSt = currCount</span><br><span class="line">      <span class="keyword">if</span> (!st.isEmpty)&#123;</span><br><span class="line">        newSt = newSt + st.get</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="type">Some</span>(newSt)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    result.updateStateByKey(updateFunc).print()</span><br><span class="line">    <span class="comment">//启动上下文</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<p><img src="http://img.mxranger.cn/Spark/1554538310790.png" alt="1554538310790"></p>
<h2 id="使用updateStateByKey模拟窗口"><a href="#使用updateStateByKey模拟窗口" class="headerlink" title="使用updateStateByKey模拟窗口"></a>使用updateStateByKey模拟窗口</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * ClassName SparkStreamingWorldCountScala</span></span><br><span class="line"><span class="comment">  * Author    MxRanger</span></span><br><span class="line"><span class="comment">  * Date      2019/4/5</span></span><br><span class="line"><span class="comment">  * Time      19:52</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * spark streaming按照key更新状态</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  **/</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreamingWorldCountUpdateByKeyScala2</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">&quot;StreamingWorldCount&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local[*]&quot;</span>) <span class="comment">//local模式，&gt;1 *：动态提取cpu核数，有多少开多少</span></span><br><span class="line">    <span class="comment">//流上下文</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">2</span>))</span><br><span class="line">    ssc.checkpoint(<span class="string">&quot;file:///f:sparktest/streamingcheckpoint&quot;</span>)</span><br><span class="line">    <span class="comment">//返回类型ReceiverInputDStream就是rdd流</span></span><br><span class="line">    <span class="comment">//创建套接字文本流</span></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">&quot;master&quot;</span>, <span class="number">8888</span>)</span><br><span class="line">    <span class="comment">//单词序列</span></span><br><span class="line">    <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">    <span class="comment">//标一成对</span></span><br><span class="line">    <span class="keyword">val</span> pair = words.map((_, <span class="number">1</span>))</span><br><span class="line">    <span class="comment">//统计单词数</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * reduceFunc: (V, V) =&gt; V,   累加</span></span><br><span class="line"><span class="comment">      windowDuration: Duration,  窗口长度</span></span><br><span class="line"><span class="comment">      slideDuration: Duration    滑动间隔</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line">    <span class="keyword">val</span> result = pair.reduceByKey((a: <span class="type">Int</span>, b: <span class="type">Int</span>) =&gt; &#123;</span><br><span class="line">      a + b</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">updateFunc</span></span>(vs: <span class="type">Seq</span>[<span class="type">Int</span>], st: <span class="type">Option</span>[<span class="type">ArrayBuffer</span>[(<span class="type">Long</span>, <span class="type">Int</span>)]]): <span class="type">Option</span>[<span class="type">ArrayBuffer</span>[(<span class="type">Long</span>, <span class="type">Int</span>)]] = &#123;</span><br><span class="line">      <span class="comment">//新状态</span></span><br><span class="line">      <span class="keyword">val</span> buf = <span class="type">ArrayBuffer</span>[(<span class="type">Long</span>,<span class="type">Int</span>)]()</span><br><span class="line"></span><br><span class="line">      <span class="comment">//提取系统时间当前毫秒数</span></span><br><span class="line">      <span class="keyword">val</span> ms = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line"></span><br><span class="line">      <span class="comment">//当前v的数量</span></span><br><span class="line">      <span class="keyword">var</span> currCount = <span class="number">0</span></span><br><span class="line">      <span class="keyword">if</span>(vs != <span class="literal">null</span> &amp;&amp; !vs.isEmpty)&#123;</span><br><span class="line">        currCount = vs.sum</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span>(!st.isEmpty)&#123;</span><br><span class="line">        <span class="comment">//取出旧状态</span></span><br><span class="line">        <span class="keyword">val</span> oldBuf = st.get</span><br><span class="line">        <span class="keyword">for</span>(t &lt;- oldBuf)&#123;</span><br><span class="line">          <span class="keyword">if</span>((ms - t._1) &lt; <span class="number">10000</span>)&#123; <span class="comment">//判断是否超过10秒</span></span><br><span class="line">            buf.+=(t)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span>(currCount != <span class="number">0</span>)&#123;</span><br><span class="line">        buf.+=((ms , currCount))</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span>(buf.isEmpty)&#123;</span><br><span class="line">        <span class="type">None</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="type">Some</span>(buf)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">      <span class="keyword">val</span> result2 = result.updateStateByKey(updateFunc);</span><br><span class="line">      result2.print()</span><br><span class="line">      <span class="comment">//启动上下文</span></span><br><span class="line">      ssc.start()</span><br><span class="line">      <span class="comment">//等待停止</span></span><br><span class="line">      ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<p><img src="http://img.mxranger.cn/Spark/window.gif" alt="window"></p>
<h2 id="7、避免大量小文件"><a href="#7、避免大量小文件" class="headerlink" title="7、避免大量小文件"></a>7、避免大量小文件</h2><p>spark Streaming提供的saveAsTextFile方法是将每个RDD的每个分区输出到一个文件中，由于时间片通常是几 秒，因此导致产生大量的小文件，进而影响Namenode的资源以及计算时导致大量的task出现。解决办法就是使 用DStream的foreachRDD手动遍历每个分区，按照自定义法则将多个分区数据写入一个文件中，以下就是将多个 RDD中相同分区索引的数据写入一个文件中，文件以主机名-精确化分的时间串-分区索引格式进行命令，代码实现 如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.&#123;<span class="type">File</span>, <span class="type">FileOutputStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Date</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * ClassName SparkStreamingWorldCountScala</span></span><br><span class="line"><span class="comment">  * Author    MxRanger</span></span><br><span class="line"><span class="comment">  * Date      2019/4/5</span></span><br><span class="line"><span class="comment">  * Time      19:52</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * scala 实现spark streaming</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  **/</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreamingWorldCountSaveFileScala</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">&quot;StreamingWorldCount&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local[*]&quot;</span>) <span class="comment">//local模式，&gt;1 *：动态提取cpu核数，有多少开多少</span></span><br><span class="line">    <span class="comment">//流上下文</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf,<span class="type">Seconds</span>(<span class="number">2</span>))</span><br><span class="line">    <span class="comment">//返回类型ReceiverInputDStream就是rdd流</span></span><br><span class="line">    <span class="comment">//创建套接字文本流</span></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">&quot;master&quot;</span>,<span class="number">8888</span>)</span><br><span class="line">    <span class="comment">//单词序列</span></span><br><span class="line">    <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">    <span class="comment">//标一成对</span></span><br><span class="line">    <span class="keyword">val</span> pair = words.map((_,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">//统计单词数</span></span><br><span class="line">    <span class="keyword">val</span> result = pair.reduceByKey(_+_)</span><br><span class="line">    <span class="comment">//result.saveAsTextFiles(&quot;file:///f:/sparktest/streaming&quot;,&quot;dat&quot;)</span></span><br><span class="line">    result.foreachRDD(rdd=&gt;&#123;</span><br><span class="line">      <span class="keyword">val</span> rdd2 = rdd.mapPartitionsWithIndex((idx,it)=&gt;&#123;</span><br><span class="line">        <span class="keyword">val</span> buf: <span class="type">ArrayBuffer</span>[(<span class="type">Int</span>, (<span class="type">String</span>, <span class="type">Int</span>))] = <span class="type">ArrayBuffer</span>[(<span class="type">Int</span>,(<span class="type">String</span>,<span class="type">Int</span>))]()</span><br><span class="line">        <span class="keyword">for</span>(t &lt;- it)&#123;</span><br><span class="line">          buf.+=((idx, t))</span><br><span class="line">        &#125;</span><br><span class="line">        buf.iterator</span><br><span class="line">      &#125;)</span><br><span class="line">      rdd2.foreachPartition(it=&gt;&#123;</span><br><span class="line">        <span class="keyword">val</span> now = <span class="keyword">new</span> <span class="type">Date</span>()</span><br><span class="line">        <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">&quot;yyyy-MM-dd-HH&quot;</span>)</span><br><span class="line">        <span class="comment">//格式化时间串</span></span><br><span class="line">        <span class="keyword">val</span> strDate = sdf.format(now)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> itt = it.take(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">if</span>(!itt.isEmpty)&#123;</span><br><span class="line">          <span class="keyword">var</span> par = itt.next()._1</span><br><span class="line">          <span class="keyword">val</span> file = strDate + <span class="string">&quot;-&quot;</span> + par + <span class="string">&quot;.dat&quot;</span></span><br><span class="line">          <span class="keyword">val</span> fout = <span class="keyword">new</span> <span class="type">FileOutputStream</span>(<span class="keyword">new</span> <span class="type">File</span>(<span class="string">&quot;f:/sparktest/stream&quot;</span>, file),<span class="literal">true</span>)</span><br><span class="line">          <span class="keyword">for</span> (t &lt;- it) &#123;</span><br><span class="line">            <span class="keyword">val</span> word = t._2._1</span><br><span class="line">            <span class="keyword">val</span> cnt = t._2._2</span><br><span class="line">            fout.write((word + <span class="string">&quot;\t&quot;</span> + cnt + <span class="string">&quot;\r\n&quot;</span>).getBytes())</span><br><span class="line">            fout.flush()</span><br><span class="line">          &#125;</span><br><span class="line">          fout.close()</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">//启动上下文</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="http://img.mxranger.cn/Spark/1554553441638.png" alt="1554553441638"></p>
<h2 id="8、Spark-Streaming同Spark-SQL集成"><a href="#8、Spark-Streaming同Spark-SQL集成" class="headerlink" title="8、Spark Streaming同Spark SQL集成"></a>8、Spark Streaming同Spark SQL集成</h2><p>将socket传来处理后的文本流，遍历每一个rdd将其变换成数据框</p>
<p>添加依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-sql --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-hive --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>自制一个socket服务器</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.OutputStream;</span><br><span class="line"><span class="keyword">import</span> java.net.ServerSocket;</span><br><span class="line"><span class="keyword">import</span> java.net.Socket;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.TimeUnit;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * ClassName MyServerSocket</span></span><br><span class="line"><span class="comment"> * Author    MxRanger</span></span><br><span class="line"><span class="comment"> * Date      2019/4/6</span></span><br><span class="line"><span class="comment"> * Time      9:37</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyServerSocket</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            ServerSocket ss = <span class="keyword">new</span> ServerSocket(<span class="number">8888</span>);</span><br><span class="line">            <span class="keyword">while</span> (<span class="keyword">true</span>)&#123;</span><br><span class="line">                Socket socket = ss.accept();</span><br><span class="line">                System.out.println(<span class="string">&quot;有人连接了&quot;</span>);</span><br><span class="line">                OutputStream oos = socket.getOutputStream();</span><br><span class="line">                <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">for</span> (;;)&#123;</span><br><span class="line">                    oos.write((<span class="string">&quot;hello world tom&quot;</span>+i+<span class="string">&quot;\r\n&quot;</span>).getBytes());</span><br><span class="line">                    oos.flush();</span><br><span class="line">                    TimeUnit.MILLISECONDS.sleep(<span class="number">200</span>);</span><br><span class="line">                    i++;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="1、scala版-3"><a href="#1、scala版-3" class="headerlink" title="1、scala版"></a>1、scala版</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * ClassName SparkStreamingWorldCountScala</span></span><br><span class="line"><span class="comment">  * Author    MxRanger</span></span><br><span class="line"><span class="comment">  * Date      2019/4/5</span></span><br><span class="line"><span class="comment">  * Time      19:52</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * Spark Streaming + SQL实现word count</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  **/</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreamingSQLWorldCountScala</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">&quot;StreamingWorldCount&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local[*]&quot;</span>) <span class="comment">//local模式，&gt;1 *：动态提取cpu核数，有多少开多少</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建SparkSession</span></span><br><span class="line">    <span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder()</span><br><span class="line">                                    .config(conf)</span><br><span class="line">                                    <span class="comment">//.enableHiveSupport()</span></span><br><span class="line">                                    .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//流上下文</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkSession.sparkContext,<span class="type">Seconds</span>(<span class="number">2</span>))</span><br><span class="line">    <span class="comment">//返回类型ReceiverInputDStream就是rdd流</span></span><br><span class="line">    <span class="comment">//创建套接字文本流</span></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">&quot;localhost&quot;</span>,<span class="number">8888</span>)</span><br><span class="line">    <span class="comment">//压扁成单词流</span></span><br><span class="line">    <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"></span><br><span class="line">    words.foreachRDD(rdd=&gt;&#123;</span><br><span class="line">      <span class="keyword">import</span> sparkSession.implicits._</span><br><span class="line">      <span class="keyword">val</span> df = rdd.toDF(<span class="string">&quot;word&quot;</span>)<span class="comment">//数据框的列</span></span><br><span class="line">      df.createOrReplaceTempView(<span class="string">&quot;_doc&quot;</span>)</span><br><span class="line">      sparkSession.sql(<span class="string">&quot;select word,count(*) from _doc group by word&quot;</span>).show(<span class="number">1000</span>,<span class="literal">false</span>)</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">//启动上下文</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>结果如下：每隔两秒输出一次数据框</p>
<p><img src="http://img.mxranger.cn/Spark/1554599509386.png" alt="1554599509386"></p>
<h3 id="2、java版-3"><a href="#2、java版-3" class="headerlink" title="2、java版"></a>2、java版</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> big13;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.RowFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DataTypes;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.Metadata;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructField;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructType;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Durations;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaPairDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Spark Streaming + SQL实现word count</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkStreamingSQLWordcountJava</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">		SparkConf conf = <span class="keyword">new</span> SparkConf() ;</span><br><span class="line">		conf.setAppName(<span class="string">&quot;streaming&quot;</span>) ;</span><br><span class="line">		conf.setMaster(<span class="string">&quot;local[2]&quot;</span>) ;</span><br><span class="line"></span><br><span class="line">		<span class="comment">//创建sparksession对象</span></span><br><span class="line">		<span class="keyword">final</span> SparkSession spark = SparkSession.builder().config(conf).getOrCreate() ;</span><br><span class="line"></span><br><span class="line">		<span class="comment">//创建java流上下文</span></span><br><span class="line">		JavaStreamingContext ssc = <span class="keyword">new</span> JavaStreamingContext(<span class="keyword">new</span> JavaSparkContext(spark.sparkContext()) , Durations.seconds(<span class="number">2</span>)) ;</span><br><span class="line"></span><br><span class="line">		<span class="comment">//创建socket文本流</span></span><br><span class="line">		JavaDStream&lt;String&gt; lines = ssc.socketTextStream(<span class="string">&quot;localhost&quot;</span> , <span class="number">8888</span>) ;</span><br><span class="line"></span><br><span class="line">		<span class="comment">//压扁成对</span></span><br><span class="line">		JavaDStream&lt;String&gt; words= lines.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">			<span class="function"><span class="keyword">public</span> Iterator&lt;String&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">				<span class="keyword">return</span> Arrays.asList(s.split(<span class="string">&quot; &quot;</span>)).iterator();</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;) ;</span><br><span class="line"></span><br><span class="line">		words.foreachRDD(<span class="keyword">new</span> VoidFunction&lt;JavaRDD&lt;String&gt;&gt;() &#123;</span><br><span class="line">			<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(JavaRDD&lt;String&gt; rdd)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">				<span class="comment">//</span></span><br><span class="line">				JavaRDD&lt;Row&gt; rdd2 = rdd.map(<span class="keyword">new</span> Function&lt;String, Row&gt;() &#123;</span><br><span class="line">					<span class="function"><span class="keyword">public</span> Row <span class="title">call</span><span class="params">(String v1)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">						<span class="keyword">return</span> RowFactory.create(v1);</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;) ;</span><br><span class="line">				<span class="comment">//转换rdd到DataFrame</span></span><br><span class="line">				StructField[] fields = <span class="keyword">new</span> StructField[<span class="number">1</span>] ;</span><br><span class="line">				fields[<span class="number">0</span>] = <span class="keyword">new</span> StructField(<span class="string">&quot;word&quot;</span> , DataTypes.StringType , <span class="keyword">true</span> , Metadata.empty()) ;</span><br><span class="line">				StructType type = <span class="keyword">new</span> StructType(fields) ;</span><br><span class="line">				Dataset&lt;Row&gt; df = spark.createDataFrame(rdd2 , type) ;</span><br><span class="line"></span><br><span class="line">				<span class="comment">//注册临时视图</span></span><br><span class="line">				df.createOrReplaceTempView(<span class="string">&quot;_doc&quot;</span>);</span><br><span class="line">				spark.sql(<span class="string">&quot;select word , count(*) from _doc group by word&quot;</span>).show(<span class="number">100</span>,<span class="keyword">false</span>) ;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;);</span><br><span class="line">		ssc.start();</span><br><span class="line">		ssc.awaitTermination();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h1 id="九、Spark-Streaming与Kafka集成"><a href="#九、Spark-Streaming与Kafka集成" class="headerlink" title="九、Spark Streaming与Kafka集成"></a>九、Spark Streaming与Kafka集成</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">JMS</span><br><span class="line">-------------------</span><br><span class="line">	java message service,java消息服务.</span><br><span class="line">	消息中间件(middle ware)，独立程序。</span><br><span class="line">	0.destination</span><br><span class="line">		目的地。</span><br><span class="line">		1)队列(queue)</span><br><span class="line">			P2P.</span><br><span class="line">		2)主题(topic)</span><br><span class="line">			pubsub</span><br><span class="line">	1.P2P</span><br><span class="line">		point to point,点对点模型。</span><br><span class="line">		</span><br><span class="line">	2.PubSub模型</span><br><span class="line">		publish subscribe ,发布订阅模型。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="1、介绍-2"><a href="#1、介绍-2" class="headerlink" title="1、介绍"></a>1、介绍</h2><p>kafka是一个发布订阅消息系统，具有分布式、分区化、多副本提交日志特点。kafka项目在0.8和0.10之间引入了 一种新型消费者API，注意选择正确的包以获得相应的特性。每个版本都是向后兼容的，因此0.8可以兼容0.9和 0.10，但是0.10不能兼容早期版本。0.8支持python、Receiver流和Direct流，不支持偏移量提交API以及动态分 区订阅，0.10不支持python和Receiver流，支持Direct流、偏移量提交API和动态分区订阅。具体见表格：</p>
<p>|spark-streaming-kafka-0-8|spark-streaming-kafka-0-10|<br>| —- | —- | —- |<br>|Broker Version      | 0.8.2.1 or higher     |  0.10.0 or higher    |<br>|  API Maturity    |  过时    |   稳定   |<br>|  支持语言    |scala、java、python| scala、java|<br>|   Receiver流    |支持| 不支持|<br>| Direct流      |支持 |支持|<br>|  SSL/TLS    |不支持| 支持|<br>|  偏移量提交API     |不支持 |支持|<br>|  动态分区订阅    |不支持 |支持|</p>
<h3 id="spark-streaming-kafka"><a href="#spark-streaming-kafka" class="headerlink" title="spark streaming + kafka"></a>spark streaming + kafka</h3><p>kafka是消息中间件，消息系统。可以当做很大的缓存.<br>kafka消费者组，组内只有一个消费者消息一条消息。<br>topic可以有多个分区，每个主题有多个副本(3)。</p>
<h2 id="2、使用"><a href="#2、使用" class="headerlink" title="2、使用"></a>2、使用</h2><h3 id="1、测试"><a href="#1、测试" class="headerlink" title="1、测试"></a>1、测试</h3><p>【1】启动zookeeper集群和kafka集群配置见<a href="E://md/Hadoop.md">Hadoop</a>，jps查看</p>
<p><img src="http://img.mxranger.cn/Spark/1554606249381.png" alt="1554606249381"></p>
<p>【2】注册topic</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic mxranger</span><br></pre></td></tr></table></figure>

<p>【3】命令开启生产者、消费者</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">开启生产者</span><br><span class="line">&gt; bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test</span><br><span class="line">开启消费者</span><br><span class="line">&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning</span><br></pre></td></tr></table></figure>

<p>【4】使用scala对接kafka</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.<span class="type">KafkaUtils</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.<span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.<span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * ClassName SparkStreamingWorldCountScala</span></span><br><span class="line"><span class="comment">  * Author    MxRanger</span></span><br><span class="line"><span class="comment">  * Date      2019/4/5</span></span><br><span class="line"><span class="comment">  * Time      19:52</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * scala 实现spark streaming</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  **/</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreamingKafkaScala</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">&quot;StreamingWorldCount&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local[*]&quot;</span>) <span class="comment">//local模式，&gt;1 *：动态提取cpu核数，有多少开多少</span></span><br><span class="line">    <span class="comment">//流上下文</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf,<span class="type">Seconds</span>(<span class="number">2</span>))</span><br><span class="line">    <span class="comment">//kafka参数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">      <span class="string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="string">&quot;192.168.159.200:9092&quot;</span>,</span><br><span class="line">      <span class="string">&quot;key.deserializer&quot;</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="string">&quot;value.deserializer&quot;</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="string">&quot;group.id&quot;</span> -&gt; <span class="string">&quot;g1&quot;</span>,</span><br><span class="line">      <span class="string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="string">&quot;latest&quot;</span>,</span><br><span class="line">      <span class="string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> topics = <span class="type">Array</span>(<span class="string">&quot;mxranger&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> stream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> kv = stream.map(r=&gt;(r.key() , r.value()))</span><br><span class="line">    kv.print()</span><br><span class="line">    <span class="comment">//启动上下文</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>【5】测试结果：</p>
<p><img src="http://img.mxranger.cn/Spark/kafka.gif" alt="kafka"></p>
<h2 id="3、kafka直接流"><a href="#3、kafka直接流" class="headerlink" title="3、kafka直接流"></a>3、kafka直接流</h2><p>每个kafka的主题分区对应一个rdd的分区。<br>    通过spark.streaming.kafka.maxRatePerPartition对每个分区每秒接受的记录数进行限制(限速)</p>
<ul>
<li>[LocationStrategy]<pre><code>位置策略，位置策略的本意就是控制消费者在哪些节点上开启。
</code></pre>
<ul>
<li>LocationStrategies.PreferConsistent<br>大多数情况下，选择使用该方式，将在所有executors上均衡分布分区进行调度。（负载均衡）</li>
<li>LocationStrategies.PreferBrokers<br>如果executor和kafka broker位于同一主机，则可以使用该方式，这将优先调度那些分区为leader的分区。</li>
<li>LocationStrategies.PreferFixed<br>如果在分区间有严重的数据倾斜，可以使用该方式，允许为分区指定特定的位置进 行调度。    </li>
</ul>
</li>
<li>[ConsumerStrategy]<pre><code>消费者策略，控制消费哪些主题，主题下的哪些分区，分区的哪个范围。
</code></pre>
<ul>
<li>ConsumerStrategies.Subscribe<br>允许订阅固定的主题集合。</li>
<li>ConsumerStrategies.SubscribePattern<br>   可以使用正则表达式指定主题。</li>
<li>ConsumerStrategies.Assign<br>允许指定固定的分区集合。</li>
</ul>
</li>
</ul>
<p>待续………</p>
<h1 id="十、Spark的HA模式"><a href="#十、Spark的HA模式" class="headerlink" title="十、Spark的HA模式"></a>十、Spark的HA模式</h1><p>所谓的ha模式，无非就是多几个master，以防只有一个master宕机的问题</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1. 修改所有节点的spark-evn.sh文件</span><br><span class="line">		[spark-env.sh]</span><br><span class="line">		...</span><br><span class="line">		export SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=master:2181,slave1:2181,slave2:2181 -Dspark.deploy.zookeeper.dir=/spark&quot;  </span><br><span class="line">	2.启动spark集群</span><br><span class="line">		$&gt;/soft/spark/sbin/start-all.sh</span><br><span class="line">	3.再启动一个master进程</span><br><span class="line">		[s102]</span><br><span class="line">		$&gt;/soft/spark/sbin/start-master.sh</span><br><span class="line">	4.启动spark-shell,指定多个master地址</span><br><span class="line">		$&gt;spark-shell --master spark://s101:7077,s102:7077</span><br></pre></td></tr></table></figure>



<h1 id="十一、机器学习（spark-mllib）"><a href="#十一、机器学习（spark-mllib）" class="headerlink" title="十一、机器学习（spark mllib）"></a>十一、机器学习（spark mllib）</h1><p><a href="E://md/Spark机器学习.md">机器学习</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">慕·歌</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2019/04/30/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/">http://example.com/2019/04/30/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">MxRanger's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/spark/">spark</a></div><div class="post_share"><div class="social-share" data-image="http://img.mxranger.cn/Spark/spark-stack.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2019/04/30/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/"><img class="prev-cover" src="/images/hadoop.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Hadoop安装教程</div></div></a></div><div class="next-post pull-right"><a href="/2019/04/30/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/"><img class="next-cover" src="/images/hadoop.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">周阳七天Hadoop</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/images/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">慕·歌</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">55</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">27</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">27</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/GavinGrayer"><i class="fas fa-bookmark"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="mailto:mxranger@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">守的云开见明月, 做时间的朋友</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81Spark"><span class="toc-text">一、Spark</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%BB%8B%E7%BB%8D"><span class="toc-text">1、大数据介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81spark%E4%BB%8B%E7%BB%8D"><span class="toc-text">2、spark介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E5%AE%89%E8%A3%85"><span class="toc-text">3、安装</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81%E7%BB%83%E4%B9%A0"><span class="toc-text">4、练习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81wordcount%E5%AE%9E%E7%8E%B0"><span class="toc-text">5、wordcount实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81scala%E7%89%88"><span class="toc-text">1、scala版</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81java%E7%89%88"><span class="toc-text">2、java版</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81%E6%A1%88%E4%BE%8B"><span class="toc-text">6、案例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E6%9C%80%E9%AB%98%E6%B0%94%E6%B8%A9%E6%A1%88%E4%BE%8B"><span class="toc-text">1、最高气温案例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Scala%E7%89%88"><span class="toc-text">Scala版</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#java%E7%89%88"><span class="toc-text">java版</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E6%B0%94%E6%B8%A9%E6%95%B0%E6%8D%AE%E5%A4%9A%E6%8C%87%E6%A0%87%E8%81%9A%E5%90%88"><span class="toc-text">2、气温数据多指标聚合</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Scala%E7%89%88-1"><span class="toc-text">Scala版</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#java%E7%89%88-%E4%BD%9C%E4%B8%9A"><span class="toc-text">java版(作业)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7%E3%80%81%E6%90%AD%E5%BB%BA%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F"><span class="toc-text">7、搭建集群模式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA"><span class="toc-text">1、集群搭建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81spark-shell-%E5%90%AF%E5%8A%A8%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F"><span class="toc-text">2、spark-shell 启动集群模式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E3%80%81%E4%BD%BF%E7%94%A8%E9%9D%99%E6%80%81%E6%95%B0%E6%8D%AE"><span class="toc-text">1、使用静态数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E3%80%81spark%E9%9B%86%E7%BE%A4%E4%B8%8Ehdfs%E9%9B%86%E6%88%90"><span class="toc-text">2、spark集群与hdfs集成</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E5%B0%86%E7%A8%8B%E5%BA%8F%E9%83%A8%E7%BD%B2%E5%88%B0spark%E9%9B%86%E7%BE%A4%E6%89%A7%E8%A1%8C"><span class="toc-text">3、将程序部署到spark集群执行</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E3%80%81%E4%BB%A3%E7%A0%81-%EF%BC%88%E6%B2%A1%E6%9C%89local%EF%BC%89"><span class="toc-text">1、代码   （没有local）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E3%80%81%E6%A8%A1%E5%9D%97%E6%89%93%E5%8C%85"><span class="toc-text">2、模块打包</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E6%A0%87%E7%AD%BE%E7%94%9F%E6%88%90%E6%A1%88%E4%BE%8B"><span class="toc-text">二、标签生成案例</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E8%A7%A3%E6%9E%90json%E5%AD%97%E7%AC%A6%E4%B8%B2"><span class="toc-text">1、解析json字符串</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81spark-API%EF%BC%88rdd%EF%BC%89%E7%9A%84sacla%E7%89%88"><span class="toc-text">2、spark API（rdd）的sacla版</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81Java%E7%89%88"><span class="toc-text">3、Java版</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81RDD%E5%B8%B8%E8%A7%81%E6%93%8D%E4%BD%9C"><span class="toc-text">三、RDD常见操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81transform%E6%96%B9%E6%B3%95"><span class="toc-text">1、transform方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81mapPartitions%E5%92%8CmapPartitionsWithIndex"><span class="toc-text">1、mapPartitions和mapPartitionsWithIndex</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mapPartitions%E6%9F%A5%E7%9C%8B%E6%89%80%E6%9C%89%E5%88%86%E5%8C%BA%E9%87%8C%E7%9A%84%E4%BF%A1%E6%81%AF"><span class="toc-text">mapPartitions查看所有分区里的信息</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mapPartitionsWithIndex%E5%8F%AF%E4%BB%A5%E6%9F%A5%E7%9C%8B%E6%9F%90%E4%B8%AA%E5%88%86%E5%8C%BA%E9%87%8C%E7%9A%84%E4%BF%A1%E6%81%AF"><span class="toc-text">mapPartitionsWithIndex可以查看某个分区里的信息</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81sample%E9%87%87%E6%A0%B7"><span class="toc-text">2、sample采样</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81union"><span class="toc-text">3、union</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E3%80%81intersection"><span class="toc-text">4、intersection</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5%E3%80%81distinct"><span class="toc-text">5、distinct</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6%E3%80%81groupByKey%E5%92%8CreduceByKey"><span class="toc-text">6、groupByKey和reduceByKey</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#groupByKey"><span class="toc-text">groupByKey</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7%E3%80%81join"><span class="toc-text">7、join</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8%E3%80%81fullOuterJoin%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%92%8CleftOuterJoin%E5%B7%A6%E5%A4%96%E8%BF%9E%E6%8E%A5"><span class="toc-text">8、fullOuterJoin全连接和leftOuterJoin左外连接</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9%E3%80%81cogroup"><span class="toc-text">9、cogroup</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10%E3%80%81coalesce%E5%92%8Crepartition"><span class="toc-text">10、coalesce和repartition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11%E3%80%81aggregateByKey"><span class="toc-text">11、aggregateByKey</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0"><span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81action%E6%96%B9%E6%B3%95"><span class="toc-text">2、action方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#saveAsTextFile"><span class="toc-text">saveAsTextFile</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81Spark%E6%A0%B8%E5%BF%83API"><span class="toc-text">四、Spark核心API</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-SparkConf"><span class="toc-text">1.SparkConf</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-SparkContext"><span class="toc-text">2.SparkContext</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-RDD"><span class="toc-text">3.RDD</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Dependency"><span class="toc-text">4.Dependency</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Stage"><span class="toc-text">5.Stage</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Task"><span class="toc-text">6.Task</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-job"><span class="toc-text">7.job</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-Application"><span class="toc-text">8.Application</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-DAGScheduler"><span class="toc-text">9.DAGScheduler</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-%E5%86%85%E6%A0%B8"><span class="toc-text">10. 内核</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E2%80%94%E2%80%94%E5%90%AF%E5%8A%A8%E6%89%A7%E8%A1%8C%E5%99%A8"><span class="toc-text">11.资源分配——启动执行器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%9C%E4%B8%9A%EF%BC%9A"><span class="toc-text">作业：</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C"><span class="toc-text">五、数据倾斜</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AD%E3%80%81job%E8%B0%83%E5%BA%A6"><span class="toc-text">六、job调度</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F"><span class="toc-text">1.集群模式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-job%E7%9A%84%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F"><span class="toc-text">2.job的部署模式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-client"><span class="toc-text">1.client</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-cluster-%E7%A6%BB%E7%BA%BF"><span class="toc-text">2.cluster(离线)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-job%E6%89%A7%E8%A1%8C%E6%A8%A1%E5%BC%8F"><span class="toc-text">3.job执行模式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E4%BB%A5cluster%E6%96%B9%E5%BC%8F%E8%BF%90%E8%A1%8Cjob"><span class="toc-text">1.以cluster方式运行job</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81yarn%E6%A8%A1%E5%BC%8F"><span class="toc-text">4、yarn模式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%83%E3%80%81Spark-SQL"><span class="toc-text">七、Spark SQL</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E4%BB%8B%E7%BB%8D"><span class="toc-text">1、介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81spark%E5%92%8Chive%E9%9B%86%E6%88%90"><span class="toc-text">2、spark和hive集成</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E5%AE%89%E8%A3%85"><span class="toc-text">1、安装</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81idea%E7%BC%96%E5%86%99spark-sql%E5%BC%80%E5%8F%91"><span class="toc-text">3、idea编写spark sql开发</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E4%BD%BF%E7%94%A8scala"><span class="toc-text">1、使用scala</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E4%BD%BF%E7%94%A8java"><span class="toc-text">2、使用java</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#CRUD"><span class="toc-text">CRUD</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98"><span class="toc-text">3.常见问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81hdfs%E6%96%87%E4%BB%B6%E4%B8%AD%E7%9A%84WordCount"><span class="toc-text">4、hdfs文件中的WordCount</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81scala%E7%89%88-1"><span class="toc-text">1、scala版</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81java%E7%89%88-1"><span class="toc-text">2、java版</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E5%AE%A2%E6%88%B7%E8%AE%A2%E5%8D%95%E5%B7%A6%E5%A4%96%E8%BF%9E%E6%8E%A5%E6%9F%A5%E8%AF%A2"><span class="toc-text">5、客户订单左外连接查询</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81java%E7%89%88"><span class="toc-text">1、java版</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E5%86%99%E5%85%A5"><span class="toc-text">1、写入</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7%E3%80%81spark-sql%E8%AF%BB%E5%86%99parquet"><span class="toc-text">7、spark sql读写parquet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8%E3%80%81spark-sql%E8%AF%BB%E5%86%99mysql%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-text">8、spark sql读写mysql数据库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9%E3%80%81Spark-SQL%E4%BD%9C%E4%B8%BA%E5%88%86%E5%B8%83%E5%BC%8F%E6%9F%A5%E8%AF%A2%E5%BC%95%E6%93%8E"><span class="toc-text">9、Spark SQL作为分布式查询引擎</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AB%E3%80%81Spark-Streaming"><span class="toc-text">八、Spark Streaming</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E4%BB%8B%E7%BB%8D-1"><span class="toc-text">1、介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E5%AE%9E%E7%8E%B0WordCount"><span class="toc-text">2、实现WordCount</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E4%BD%BF%E7%94%A8nc%E4%BD%9C%E4%B8%BA%E7%94%9F%E4%BA%A7%E6%95%B0%E6%8D%AE"><span class="toc-text">1、使用nc作为生产数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E6%B7%BB%E5%8A%A0%E4%BE%9D%E8%B5%96%E5%8C%85"><span class="toc-text">2、添加依赖包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E7%BC%96%E5%86%99%E7%A8%8B%E5%BA%8F"><span class="toc-text">3、编写程序</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E3%80%81scala%E7%89%88-2"><span class="toc-text">1、scala版</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E3%80%81java%E7%89%88-2"><span class="toc-text">2、java版</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-text">3、基本概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-StreamingContext%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">3.1 StreamingContext初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E7%A6%BB%E6%95%A3%E6%B5%81-DStream"><span class="toc-text">3.2 离散流(DStream)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-Input-DStream%E5%92%8CReceiver"><span class="toc-text">3.3 Input DStream和Receiver</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81Receiver"><span class="toc-text">4、Receiver</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%86%85%E9%83%A8%E7%BB%93%E6%9E%84"><span class="toc-text">4.1 内部结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E5%88%86%E5%8C%BA%E6%95%B0%E6%8E%A7%E5%88%B6"><span class="toc-text">4.2 分区数控制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E9%99%90%E9%80%9F%E5%A4%84%E7%90%86"><span class="toc-text">4.3 限速处理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E3%80%81%E6%AF%8F%E7%A7%92%E6%8E%A5%E6%94%B6%E8%AE%B0%E5%BD%95%E6%95%B0"><span class="toc-text">1、每秒接收记录数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E3%80%81%E5%8E%8B%E5%90%8E-backpress-%E5%A4%84%E7%90%86"><span class="toc-text">2、压后(backpress)处理</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81Window%E7%AA%97%E5%8F%A3%E6%93%8D%E4%BD%9C"><span class="toc-text">5、Window窗口操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E4%BB%8B%E7%BB%8D"><span class="toc-text">5.1 介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2%E3%80%81%E7%BC%96%E5%86%99%E7%A8%8B%E5%BA%8F"><span class="toc-text">5.2、编写程序</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81updateStateByKey"><span class="toc-text">6、updateStateByKey</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A8%8B%E5%BA%8F"><span class="toc-text">程序</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8updateStateByKey%E6%A8%A1%E6%8B%9F%E7%AA%97%E5%8F%A3"><span class="toc-text">使用updateStateByKey模拟窗口</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7%E3%80%81%E9%81%BF%E5%85%8D%E5%A4%A7%E9%87%8F%E5%B0%8F%E6%96%87%E4%BB%B6"><span class="toc-text">7、避免大量小文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8%E3%80%81Spark-Streaming%E5%90%8CSpark-SQL%E9%9B%86%E6%88%90"><span class="toc-text">8、Spark Streaming同Spark SQL集成</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81scala%E7%89%88-3"><span class="toc-text">1、scala版</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81java%E7%89%88-3"><span class="toc-text">2、java版</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B9%9D%E3%80%81Spark-Streaming%E4%B8%8EKafka%E9%9B%86%E6%88%90"><span class="toc-text">九、Spark Streaming与Kafka集成</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E4%BB%8B%E7%BB%8D-2"><span class="toc-text">1、介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#spark-streaming-kafka"><span class="toc-text">spark streaming + kafka</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E4%BD%BF%E7%94%A8"><span class="toc-text">2、使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E6%B5%8B%E8%AF%95"><span class="toc-text">1、测试</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81kafka%E7%9B%B4%E6%8E%A5%E6%B5%81"><span class="toc-text">3、kafka直接流</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%81%E3%80%81Spark%E7%9A%84HA%E6%A8%A1%E5%BC%8F"><span class="toc-text">十、Spark的HA模式</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%81%E4%B8%80%E3%80%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88spark-mllib%EF%BC%89"><span class="toc-text">十一、机器学习（spark mllib）</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/06/19/hello-world/" title="Hello World"><img src="http://img.mxranger.cn/gratisography-370H.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Hello World"/></a><div class="content"><a class="title" href="/2021/06/19/hello-world/" title="Hello World">Hello World</a><time datetime="2021-06-19T00:44:25.616Z" title="发表于 2021-06-19 08:44:25">2021-06-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/02/08/back/Vue%E8%84%9A%E6%89%8B%E6%9E%B6%E5%B5%8C%E5%85%A5SpringBoot/" title="Vue脚手架嵌入SpringBoot"><img src="/images/vue%E5%86%85%E5%B5%8Cspringboot.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Vue脚手架嵌入SpringBoot"/></a><div class="content"><a class="title" href="/2021/02/08/back/Vue%E8%84%9A%E6%89%8B%E6%9E%B6%E5%B5%8C%E5%85%A5SpringBoot/" title="Vue脚手架嵌入SpringBoot">Vue脚手架嵌入SpringBoot</a><time datetime="2021-02-08T08:35:54.000Z" title="发表于 2021-02-08 16:35:54">2021-02-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/01/31/front/vue/6%E3%80%81%E6%A8%A1%E5%9D%97%E5%8C%96%E5%BC%80%E5%8F%91%E4%B8%8EElmentUI%E7%9A%84%E4%BD%BF%E7%94%A8/" title="Vue——模块化开发与ElmentUI的使用"><img src="https://pic1.zhimg.com/v2-a3a350493a1ad6d46a1800ee2aad3fcf_1440w.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Vue——模块化开发与ElmentUI的使用"/></a><div class="content"><a class="title" href="/2021/01/31/front/vue/6%E3%80%81%E6%A8%A1%E5%9D%97%E5%8C%96%E5%BC%80%E5%8F%91%E4%B8%8EElmentUI%E7%9A%84%E4%BD%BF%E7%94%A8/" title="Vue——模块化开发与ElmentUI的使用">Vue——模块化开发与ElmentUI的使用</a><time datetime="2021-01-31T12:27:02.000Z" title="发表于 2021-01-31 20:27:02">2021-01-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/01/31/front/vue/5%E3%80%81%E7%BB%84%E4%BB%B6%E5%8F%8A%E7%BB%84%E4%BB%B6%E9%97%B4%E7%9A%84%E9%80%9A%E4%BF%A1/" title="Vue——组件及组件间的通信"><img src="https://pic1.zhimg.com/v2-a3a350493a1ad6d46a1800ee2aad3fcf_1440w.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Vue——组件及组件间的通信"/></a><div class="content"><a class="title" href="/2021/01/31/front/vue/5%E3%80%81%E7%BB%84%E4%BB%B6%E5%8F%8A%E7%BB%84%E4%BB%B6%E9%97%B4%E7%9A%84%E9%80%9A%E4%BF%A1/" title="Vue——组件及组件间的通信">Vue——组件及组件间的通信</a><time datetime="2021-01-31T12:27:02.000Z" title="发表于 2021-01-31 20:27:02">2021-01-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/01/31/front/vue/3%E3%80%81vue%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E5%8F%8A%E5%AE%9E%E4%BE%8B%E7%9A%84%E5%B1%9E%E6%80%A7%E5%92%8C%E6%96%B9%E6%B3%95/" title="Vue——生命周期及实例的属性和方法"><img src="https://pic1.zhimg.com/v2-a3a350493a1ad6d46a1800ee2aad3fcf_1440w.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Vue——生命周期及实例的属性和方法"/></a><div class="content"><a class="title" href="/2021/01/31/front/vue/3%E3%80%81vue%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E5%8F%8A%E5%AE%9E%E4%BE%8B%E7%9A%84%E5%B1%9E%E6%80%A7%E5%92%8C%E6%96%B9%E6%B3%95/" title="Vue——生命周期及实例的属性和方法">Vue——生命周期及实例的属性和方法</a><time datetime="2021-01-31T12:25:02.000Z" title="发表于 2021-01-31 20:25:02">2021-01-31</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('http://img.mxranger.cn/Spark/spark-stack.png')"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2021 By 慕·歌</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a target="_blank" rel="noopener" href="http://blog.mxranger.cn/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'default',
      })
      true && mermaid.init()
    })
  }
}</script></div><div class="aplayer no-destroy" data-id="6589190051" data-server="netease" data-type="playlist" data-fixed="true" data-mini="true" data-listFolded="false" data-order="random" data-preload="none" data-autoplay="true" muted></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/gh/metowolf/MetingJS@1.2/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = [
  'title',
  '#config-diff',
  '#body-wrap',
  '#rightside-config-hide',
  '#rightside-config-show',
  '.js-pjax'
]

if (false) {
  pjaxSelectors.unshift('meta[property="og:image"]', 'meta[property="og:title"]', 'meta[property="og:url"]')
}

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener toc scroll 
  window.removeEventListener('scroll', window.tocScrollFn)

  typeof preloader === 'object' && preloader.initLoading()
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // Analytics
  if (false) {
    MtaH5.pgv()
  }

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>